[
  {
    "objectID": "w16_Chapter10NonParStats.html",
    "href": "w16_Chapter10NonParStats.html",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "",
    "text": "Parametric statistics makes statements about population parameters, that come from well-specified population distributions, such as \\(\\mu\\), \\(\\pi\\) or \\(\\sigma^2\\)\nThe estimators \\(\\bar{X}\\), \\(P\\), or \\(s^2\\), their associated standard errors, sampling distributions (or asymptotic sampling distributions based on the central limit theorem) are used to develop the critical regions, calculate confidence intervals, \\(p\\)-values (significance or PROB-values) etc.\nNon-parametric and/or distribution free statistical tests do not use as stringent assumptions about the underlying population distribution and therefore ignore population parameters.\n\n\n\n\n\nThe sample sizes are fairly small so that the central limit theorem is not applicable.\nIf the distribution of the population is unknown and/or assumptions about it are not reasonable.\nNote: There are tests based on sample observations to check if the underlying population follows a particular parametric distribution (e.g., Chi-square goodness-of-fit test or the Kolmogorov-Smirnov test).\nIf the random variables are qualitative, i.e., measured on the nominal or ordinal scale.\nException: We know about the exact binomial tests for nominal scaled observations.\n\n\nFor many parametric tests there are equivalent tests in the non-parametric domain (see BBR Chapter 10)\n\n\n\n\n\nIf the underlying parametric assumptions are satisfied and quality data are available, parametric tests are superior.\nThe actual \\(\\alpha\\)-error of non-parametric tests may be smaller by an unknown quantity if the underlying parametric assumptions are satisfied.\nThat is, non-parametric tests may be conservative, because they reject the \\(H_0\\) at a nominal error level substantially less frequently than their parametric counterparts.\nThe likelihood of committing a \\(\\beta\\)-error (not rejecting \\(H_0\\) even though it is incorrect) is smaller for parametric tests if the parametric assumptions are satisfied.\n\n\n\n\n\nDue to the robustness of non-parametric tests they can be applied to a broader range of underlying populations and measurement scales.\nRemember we always can transform variables on a higher-level measurement scale to variables on a lower order (e.g., interval to ordinal scale).\n\n\n\n\n\nFor small sample sizes \\(n\\) we cannot apply the central limit theorem, when the underlying distribution assumptions are violated.\nFor small sample sizes \\(n\\), non-parametric test statistics have the advantage that their sampling distribution under the null hypothesis can be derived using combinatorial arguments. See, for instance, the Mann-Whitney test pp 385-390.\nFor larger sample sizes \\(n\\), non-parametric tests frequently make use of:\n\n[a] an assumed distribution of the observations, which is different from the normal distribution, and/or\n[b] a normal approximation by calculating the expected value \\(E(T|H_0)\\) and standard error \\(\\sqrt{Var(T|H_0)}\\) of the test statistics \\(T\\) assuming the null hypothesis is true\n\nFor non-parametric test statistics with an underlying discrete distribution, we will not be able to exhaust the nominal significance level \\(\\alpha\\) exactly (remember exact binomial test), and we will rather make use of the discrete PROB-values.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#introduction",
    "href": "w16_Chapter10NonParStats.html#introduction",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "",
    "text": "Parametric statistics makes statements about population parameters, that come from well-specified population distributions, such as \\(\\mu\\), \\(\\pi\\) or \\(\\sigma^2\\)\nThe estimators \\(\\bar{X}\\), \\(P\\), or \\(s^2\\), their associated standard errors, sampling distributions (or asymptotic sampling distributions based on the central limit theorem) are used to develop the critical regions, calculate confidence intervals, \\(p\\)-values (significance or PROB-values) etc.\nNon-parametric and/or distribution free statistical tests do not use as stringent assumptions about the underlying population distribution and therefore ignore population parameters.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#when-to-use-non-parametric-methods",
    "href": "w16_Chapter10NonParStats.html#when-to-use-non-parametric-methods",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "",
    "text": "The sample sizes are fairly small so that the central limit theorem is not applicable.\nIf the distribution of the population is unknown and/or assumptions about it are not reasonable.\nNote: There are tests based on sample observations to check if the underlying population follows a particular parametric distribution (e.g., Chi-square goodness-of-fit test or the Kolmogorov-Smirnov test).\nIf the random variables are qualitative, i.e., measured on the nominal or ordinal scale.\nException: We know about the exact binomial tests for nominal scaled observations.\n\n\nFor many parametric tests there are equivalent tests in the non-parametric domain (see BBR Chapter 10)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#comparison-of-parametric-and-non-parametric-tests",
    "href": "w16_Chapter10NonParStats.html#comparison-of-parametric-and-non-parametric-tests",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "",
    "text": "If the underlying parametric assumptions are satisfied and quality data are available, parametric tests are superior.\nThe actual \\(\\alpha\\)-error of non-parametric tests may be smaller by an unknown quantity if the underlying parametric assumptions are satisfied.\nThat is, non-parametric tests may be conservative, because they reject the \\(H_0\\) at a nominal error level substantially less frequently than their parametric counterparts.\nThe likelihood of committing a \\(\\beta\\)-error (not rejecting \\(H_0\\) even though it is incorrect) is smaller for parametric tests if the parametric assumptions are satisfied.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#scope-of-application",
    "href": "w16_Chapter10NonParStats.html#scope-of-application",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "",
    "text": "Due to the robustness of non-parametric tests they can be applied to a broader range of underlying populations and measurement scales.\nRemember we always can transform variables on a higher-level measurement scale to variables on a lower order (e.g., interval to ordinal scale).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#sample-size-considerations",
    "href": "w16_Chapter10NonParStats.html#sample-size-considerations",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "",
    "text": "For small sample sizes \\(n\\) we cannot apply the central limit theorem, when the underlying distribution assumptions are violated.\nFor small sample sizes \\(n\\), non-parametric test statistics have the advantage that their sampling distribution under the null hypothesis can be derived using combinatorial arguments. See, for instance, the Mann-Whitney test pp 385-390.\nFor larger sample sizes \\(n\\), non-parametric tests frequently make use of:\n\n[a] an assumed distribution of the observations, which is different from the normal distribution, and/or\n[b] a normal approximation by calculating the expected value \\(E(T|H_0)\\) and standard error \\(\\sqrt{Var(T|H_0)}\\) of the test statistics \\(T\\) assuming the null hypothesis is true\n\nFor non-parametric test statistics with an underlying discrete distribution, we will not be able to exhaust the nominal significance level \\(\\alpha\\) exactly (remember exact binomial test), and we will rather make use of the discrete PROB-values.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#overview",
    "href": "w16_Chapter10NonParStats.html#overview",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "2.1 Overview",
    "text": "2.1 Overview\n\nStatistical tests often assume that the underlying population follows a particular distribution, like the normal distribution for most parametric test.\nStatistical Goodness-of-Fit tests check whether the sample observations have been sampled from a hypothetical population.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#the-chi-square-test",
    "href": "w16_Chapter10NonParStats.html#the-chi-square-test",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "2.2 The Chi-Square Test",
    "text": "2.2 The Chi-Square Test\n\n2.2.1 Data Transformation\nDepending on the measurement scale of the random variable a transformation needs to be applied:\n\nIf the underlying distribution is continuous, then it needs to be re-scaled into the ordinal scale by building a mutually exclusive and disjunctive partition (class intervals) of the underlying support of the random variable \\(X\\).\nIf the underlying distribution is discrete one can work with the classified data immediately.\n\n\n\n2.2.2 Basic Concept\n\nThe underlying idea is to count the number of sample observations \\(f_i\\) (observed frequencies) of the random variable \\(X\\) within the class intervals \\(i \\in \\{1, \\ldots, k\\}\\).\nNote: At least two classes are needed to perform the test.\n\n\n\n2.2.3 Expected Frequencies\nAssuming that the hypothetical population follows a particular distribution, the expected counts within each class are calculated using the cumulative distribution function:\n\\[F_i = n \\cdot (F(\\text{upper bound of class } i) - F(\\text{lower bound of class } i))\\]\nImportant: For the \\(\\chi^2\\)-test the expected count has to be \\(F_i \\geq 5\\). Should the expected count be below 5, then consecutive classes need to be pooled together.\nBoth sets of counts are then compared within the classes.\n\n\n2.2.4 Structure of the Goodness-of-Fit Test\n\n\n\nTABLE 11-13: Structure of a χ² Goodness-of-Fit Test\n\n\n\n\n2.2.5 The Test Statistic\nThe test statistic is:\n\\[\\chi^2 = \\sum_{i=1}^{k} z_i^2 \\quad \\text{with} \\quad z_i^2 = \\frac{(f_i - F_i)^2}{F_i}\\]\n\n\n2.2.6 Poisson Distribution Foundation\n\nThe individual counts follow under \\(H_0\\) a Poisson distribution:\n\n\\[f_i \\sim Poisson(F_i)\\]\nThe Poisson distribution has the expectation \\(E(f_i) = F_i\\) and the variance \\(Var(f_i) = F_i\\).\nTherefore, \\(z_i = \\frac{f_i - E(f_i)}{\\sqrt{Var(f_i)}}\\) follows approximately a standard normal distribution.\n\nOn average this approximation works well for a Poisson distribution if \\(E(f_i) \\geq 5\\).\nSquaring a standard normal distributed variable \\(z_i\\) makes it a \\(\\chi^2\\)-distributed variable with one degree of freedom, that is, \\(z_i^2 \\sim \\chi^2_{df=1}\\).\nThe sum of \\(\\chi^2\\)-distributed random variables is again \\(\\chi^2\\)-distributed. The sum’s degrees of freedom become the sum of the individual degrees of freedom.\n\n\n\n2.2.7 Degrees of Freedom\nThe total degrees of freedom over all class intervals are equal to:\n\\[df = \\underbrace{k}_{\\text{# of classes}} - \\underbrace{\\text{# of estimated parameters}}_{\\text{2 for } \\bar{X} \\text{ and } s^2 \\text{ of the normal distribution}} - \\underbrace{1}_{\\text{for the constraint } n = \\sum_{i=1}^{k} f_i = \\sum_{i=1}^{k} F_i}\\]\nSeveral degrees of freedom are lost due to: 1. The use of estimated parameters based on the sample observations to calibrate the hypothetical distribution 2. The constraint that the overall sample size \\(N\\) and expected counts need to be equal\n\n\n2.2.8 Formal Definition\n\n\n\nTABLE 10-11: Chi-Square Goodness-of-Fit Test\n\n\nBackground:\nThe \\(\\chi^2\\) test is used to check if a sample distribution agrees with a theoretical probability distribution. The underlying random variable can be measured at the nominal level or higher. The sample consists of \\(n\\) observations distributed over \\(k\\) categories. Each category has \\(O_j\\) observations, thus \\(\\sum O_j = n\\). The theoretical distribution is used to generate expected frequencies for each category in the sample. For example, in testing a sample against the uniform distribution, each sample category would have the same expected frequency, equal to the sample size divided by the number of categories. The expected count for each category is denoted \\(E_j\\). The test is based on the differences between \\(O_j\\) and \\(E_j\\). For this test to be reliable, all \\(E_j\\) should exceed 2, and 80% of the \\(E_j\\) should exceed 5.\nHypotheses:\nLetting \\(Y\\) be the random variable, and \\(f(Y)\\) be the theoretical probability distribution of \\(Y\\), the null and alternative hypotheses are:\n\n\\(H_0\\): The sample was drawn from a population \\(f(Y)\\)\n\\(H_A\\): The sample was drawn from some distribution other than \\(f(Y)\\)\n\nTest statistic:\nUnder \\(H_0\\) the following has an approximate \\(\\chi^2\\) distribution:\n\\[X^2 = \\sum_{j=1}^{k} \\frac{(O_j - E_j)^2}{E_j}\\]\nwith degrees of freedom \\(df = k - m - 1\\) where \\(m\\) is the number of parameters in \\(f(Y)\\) estimated from the sample.\nDecision rule:\n\n\\(H_0\\) is rejected when \\(PV &lt; \\alpha\\), and we conclude that the sample did not arise from the distribution \\(f(Y)\\).\nOtherwise (if \\(PV \\geq \\alpha\\)), we fail to reject \\(H_0\\) and draw no conclusion about the underlying population.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#example-in-r",
    "href": "w16_Chapter10NonParStats.html#example-in-r",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "2.3 Example in R",
    "text": "2.3 Example in R\nThe test can be set up in R by using the probabilities of each class using the reference distribution.\nFor instance, if we have 4 classes and assume a uniform distribution, then the theoretical probabilities become \\(\\pi_1 = 0.25\\), \\(\\pi_2 = 0.25\\), \\(\\pi_3 = 0.25\\) and \\(\\pi_4 = 0.25\\).\n\nchi.results &lt;- chisq.test(c(20, 29, 35, 16), p = c(0.25, 0.25, 0.25, 0.25))\nchi.results\n\nOutput:\nChi-squared test for given probabilities\n\ndata:  c(20, 29, 35, 16)\nX-squared = 8.88, df = 3, p-value = 0.03093\n\nchi.results$expected\n\nOutput:\n[1] 25 25 25 25\n\n2.3.1 Important Notes\n\nThe expected frequencies need to be checked for the low count condition.\nIn this case the degrees of freedom are correct but in other situations they need to be reduced by the number of parameters which were estimated from the data to obtain the theoretical distribution.\nThe theoretical probabilities need to sum to one.\nExplore the online help for the function chisq.test().",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#overview-1",
    "href": "w16_Chapter10NonParStats.html#overview-1",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "3.1 Overview",
    "text": "3.1 Overview\n\nContingency tables are a cross-tabulation of two (or more) nominal or ordinal scaled variables.\nRow and column sums represent the univariate sample frequency distributions of the variables \\(X_1\\) and \\(X_2\\),\nwhereas the cell counts \\(X_{ij}\\) denote the sample frequency of the joint occurrences of the events \\(X_1 = i \\cap X_2 = j\\)\n\n\n3.1.1 Example: Household Size vs Car Ownership\n\n\n\nContingency Table: Household Size vs Car Ownership",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#testing-for-independence",
    "href": "w16_Chapter10NonParStats.html#testing-for-independence",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "3.2 Testing for Independence",
    "text": "3.2 Testing for Independence\n\nContingency tables allow testing the null hypothesis whether two nominal or ordinal scaled variables are jointly statistically independent of each other\nagainst the alternative hypothesis that they are statistically related with each other.\n\n\n3.2.1 Statistical Independence\nRecall the concept of statistical independence:\n\\[\\Pr(X_1 = i \\cap X_2 = j) = \\Pr(X_1 = i) \\cdot \\Pr(X_2 = j)\\]\n\n\n3.2.2 Hypotheses\nTherefore, the null and alternative hypotheses become:\n\\[H_0: \\pi_{ij} = \\pi_i^r \\cdot \\pi_j^c\\]\nagainst\n\\[H_A: \\pi_{ij} \\neq \\pi_i^r \\cdot \\pi_j^c \\quad \\text{for at least one pair } (i, j)\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#construction-of-the-test-statistic",
    "href": "w16_Chapter10NonParStats.html#construction-of-the-test-statistic",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "3.3 Construction of the Test Statistic",
    "text": "3.3 Construction of the Test Statistic\n\n3.3.1 Initial Approach\nAt first glance the underlying test statistic could center around the absolute difference between the observed proportion and the expected probabilities: \\(|p_{ij} - \\pi_{ij}|\\).\nJust small difference − due to sampling variations − would support the null hypothesis.\nHowever, two issues need to be considered:\n\nThe expected probabilities need to be calculated from estimates of the marginal relative row and column frequencies:\n\\[\\hat{\\pi}_{ij} = \\hat{\\pi}_i^r \\cdot \\hat{\\pi}_j^c \\quad \\text{with} \\quad \\hat{\\pi}_i^r = p_i^r = x_{i+}/n \\quad \\text{and} \\quad \\hat{\\pi}_j^c = p_j^c = x_{+j}/n\\]\nJust using the expected and observed probabilities ignores the sampling variability which depends on the sample size \\(n\\).\nThe distribution of \\(|p_{ij} - \\pi_{ij}|\\) is unknown and cannot be approximated by the standard normal distribution.\n\n\n\n3.3.2 Using Counts Instead of Probabilities\nFor these reasons the test statistics are expressed in terms of observed and expected counts (i.e., frequencies) rather than probabilities:\n\\[E(X_{ij} | H_0) = n \\cdot \\hat{\\pi}_{ij} = n \\cdot p_i^r \\cdot p_j^c\\]\n\\[= n \\cdot \\frac{\\sum_{j=1}^{J} x_{ij}}{n} \\cdot \\frac{\\sum_{i=1}^{I} x_{ij}}{n} = \\frac{\\sum_{j=1}^{J} x_{ij} \\cdot \\sum_{i=1}^{I} x_{ij}}{n}\\]\n\\[= \\frac{x_{i+} \\cdot x_{+j}}{n}\\]\n\n\n3.3.3 Poisson Distribution for Cell Counts\n\\(X_{ij}\\) are counts and thus again can be assumed to follow a Poisson distribution:\n\\[X_{ij} \\sim Poisson(n \\cdot \\pi_{ij})\\]\nwith \\(n \\cdot \\pi_{ij} = E(X_{ij} | H_0) = Var(X_{ij} | H_0)\\) (because the expectation and variance of a Poisson distribution are identical)\n\n\n3.3.4 The Chi-Square Statistic\nSumming the squared and standardized differences between the observed and expected frequencies (i.e., squared z-transformed variables) leads to the \\(\\chi^2\\)-statistic:\n\\[\\chi^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\frac{\\left[X_{ij} - E(X_{ij} | H_0)\\right]^2}{Var(X_{ij} | H_0)} = \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\frac{\\left[X_{ij} - E(X_{ij} | H_0)\\right]^2}{E(X_{ij} | H_0)}\\]\n\n\n3.3.5 Degrees of Freedom\nIn total there are \\(I \\times J\\) cells in a contingency table. However, since the expected cell frequencies are estimated from the row and column sums minus the double counting of the sample size \\(n\\), in total \\((I + J - 1)\\) degrees of freedom are lost.\n\\[df = I \\times J - (I + J - 1) = (I - 1) \\times (J - 1)\\]\nConsequently, once we know the \\((I - 1)\\) row sums and \\((J - 1)\\) column sums as well as the total sum \\(n\\), only \\((I - 1) \\times (J - 1)\\) expected cell counts can vary freely.\n\n\n3.3.6 Minimum Expected Cell Counts\nApproximating summands \\(\\frac{\\left[X_{ij} - E(X_{ij} | H_0)\\right]^2}{Var(X_{ij} | H_0)}\\) by the \\(\\chi^2\\)-distribution is only feasible if \\(E(X_{ij} | H_0) \\geq 5\\) for almost all cells.\nJust a few cells are allowed to have an expectation \\(E(X_{ij} | H_0) \\geq 2\\).\nNote: Other books quote different minimum expected counts.\n\n\n3.3.7 When Minimum Count Rules Are Not Satisfied\nShould the minimum count rules not be satisfied then:\n\nOne could switch to an exact test based on the multi-nominal distribution (similar to the exact binomial test).\nAggregate classes along rows or columns if this is meaningful from a contextual perspective. This will increase the expected count in the aggregated classes but reduce the degree of freedom.\nDrop either a class that has a low row or column counts (thus leading to low expected counts)\n\n\n\n3.3.8 Power Concerns\nAs the sample size increases − while keeping all table rates constant − it will become more likely to reject the null hypothesis because the \\(\\chi^2\\) statistic will increase while the degrees of freedom stay the same.\nFor instance, doubling the cell counts (i.e., \\(2 \\cdot n\\)) doubles the \\(\\chi^2\\) statistic, while leaving its degrees of freedom unchanged. \\(\\Rightarrow\\) It becomes more significant.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#formal-test-procedure",
    "href": "w16_Chapter10NonParStats.html#formal-test-procedure",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "3.4 Formal Test Procedure",
    "text": "3.4 Formal Test Procedure\n\n\n\nTABLE 11-22: Chi-Square Test for Independence in Contingency Tables\n\n\nSpecification:\nA total of \\(n\\) observations of two nominal/ordinal variables are cross-classified in a contingency table of dimensions \\(r \\times c\\). The observed frequency in the \\(ij\\)th cell is denoted \\(f_{ij}\\). Expected frequencies for these cells are calculated from:\n\\[F_{ij} = \\frac{R_i C_j}{n}\\]\nHere \\(R_i\\) is the observed frequency count of the \\(i\\)th row, and \\(C_j\\) is the observed frequency count of the \\(j\\)th column.\nHypotheses:\n\n\\(H_0\\): \\(\\pi_{ij} = \\pi_i^r \\pi_j^c\\) for \\(i = 1, 2, \\ldots, r\\); \\(j = 1, 2, \\ldots, c\\)\nThe variables are statistically independent.\n\\(H_A\\): \\(\\pi_{ij} \\neq \\pi_i^r \\pi_j^c\\) for at least one \\(ij\\) pair\nThe variables are not statistically independent.\n\nTest statistic:\n\\[X^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(f_{ij} - F_{ij})^2}{F_{ij}}\\]\nwith \\((r - 1)(c - 1)\\) degrees of freedom\nDecision rule:\nReject \\(H_0\\) if \\(X^2 &gt; A\\), where \\(A = \\chi^2[1 - \\alpha, (r-1)(c-1)]\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#chi-square-goodness-of-fit-test",
    "href": "w16_Chapter10NonParStats.html#chi-square-goodness-of-fit-test",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "4.1 Chi-Square Goodness-of-Fit Test",
    "text": "4.1 Chi-Square Goodness-of-Fit Test\n\n# Test if observed frequencies match expected uniform distribution\nobserved &lt;- c(20, 29, 35, 16)\nn &lt;- sum(observed)\nk &lt;- length(observed)\n\n# Uniform distribution - equal probabilities\nexpected_prob &lt;- rep(1/k, k)\nexpected_freq &lt;- n * expected_prob\n\n# Chi-square test\nchi_result &lt;- chisq.test(observed, p = expected_prob)\nprint(chi_result)\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 8.88, df = 3, p-value = 0.03093\n\n# Examine expected frequencies\ncat(\"\\nExpected frequencies:\", chi_result$expected, \"\\n\")\n\n\nExpected frequencies: 25 25 25 25 \n\n# Manual calculation\nchi_sq_manual &lt;- sum((observed - expected_freq)^2 / expected_freq)\ndf &lt;- k - 1\np_value &lt;- 1 - pchisq(chi_sq_manual, df)\n\ncat(\"\\nManual calculation:\\n\")\n\n\nManual calculation:\n\ncat(\"Chi-squared:\", round(chi_sq_manual, 3), \"\\n\")\n\nChi-squared: 8.88 \n\ncat(\"df:\", df, \"\\n\")\n\ndf: 3 \n\ncat(\"p-value:\", round(p_value, 4), \"\\n\")\n\np-value: 0.0309",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#chi-square-test-for-independence",
    "href": "w16_Chapter10NonParStats.html#chi-square-test-for-independence",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "4.2 Chi-Square Test for Independence",
    "text": "4.2 Chi-Square Test for Independence\n\n# Create a contingency table: Household Size vs Car Ownership\nhousehold_car &lt;- matrix(c(10, 8, 3, 2,\n                          7, 10, 6, 3,\n                          4, 5, 12, 6,\n                          1, 2, 6, 15), \n                        nrow = 4, byrow = TRUE)\nrownames(household_car) &lt;- c(\"HH Size 2\", \"HH Size 3\", \"HH Size 4\", \"HH Size 5\")\ncolnames(household_car) &lt;- c(\"0 Cars\", \"1 Car\", \"2 Cars\", \"3 Cars\")\n\nprint(\"Contingency Table:\")\n\n[1] \"Contingency Table:\"\n\nprint(household_car)\n\n          0 Cars 1 Car 2 Cars 3 Cars\nHH Size 2     10     8      3      2\nHH Size 3      7    10      6      3\nHH Size 4      4     5     12      6\nHH Size 5      1     2      6     15\n\n# Perform chi-square test for independence\nchi_indep &lt;- chisq.test(household_car)\nprint(chi_indep)\n\n\n    Pearson's Chi-squared test\n\ndata:  household_car\nX-squared = 37.17, df = 9, p-value = 2.454e-05\n\n# Expected frequencies under independence\ncat(\"\\nExpected frequencies under H0:\\n\")\n\n\nExpected frequencies under H0:\n\nprint(round(chi_indep$expected, 2))\n\n          0 Cars 1 Car 2 Cars 3 Cars\nHH Size 2   5.06  5.75   6.21   5.98\nHH Size 3   5.72  6.50   7.02   6.76\nHH Size 4   5.94  6.75   7.29   7.02\nHH Size 5   5.28  6.00   6.48   6.24\n\n# Residuals (standardized)\ncat(\"\\nPearson residuals:\\n\")\n\n\nPearson residuals:\n\nprint(round(chi_indep$residuals, 2))\n\n          0 Cars 1 Car 2 Cars 3 Cars\nHH Size 2   2.20  0.94  -1.29  -1.63\nHH Size 3   0.54  1.37  -0.38  -1.45\nHH Size 4  -0.80 -0.67   1.74  -0.38\nHH Size 5  -1.86 -1.63  -0.19   3.51",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#titanic-example",
    "href": "w16_Chapter10NonParStats.html#titanic-example",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "4.3 Titanic Example",
    "text": "4.3 Titanic Example\n\n# Load Titanic data\ndata(Titanic)\n\n# Convert to individual records\nTitanicIndividual &lt;- as.data.frame(lapply(as.data.frame(Titanic), \n                                          function(x) rep(x, Titanic)))\n\n# Test: Is survival independent of sex?\nsurvival_sex &lt;- table(TitanicIndividual$Survived, TitanicIndividual$Sex)\ncat(\"Survival by Sex:\\n\")\n\nSurvival by Sex:\n\nprint(survival_sex)\n\n     \n      Male Female\n  No  1364    126\n  Yes  367    344\n\nchisq.test(survival_sex)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  survival_sex\nX-squared = 454.5, df = 1, p-value &lt; 2.2e-16\n\n# Test: Is survival independent of class?\nsurvival_class &lt;- table(TitanicIndividual$Survived, TitanicIndividual$Class)\ncat(\"\\nSurvival by Class:\\n\")\n\n\nSurvival by Class:\n\nprint(survival_class)\n\n     \n      1st 2nd 3rd Crew\n  No  122 167 528  673\n  Yes 203 118 178  212\n\nchisq.test(survival_class)\n\n\n    Pearson's Chi-squared test\n\ndata:  survival_class\nX-squared = 190.4, df = 3, p-value &lt; 2.2e-16",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#visualization",
    "href": "w16_Chapter10NonParStats.html#visualization",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "4.4 Visualization",
    "text": "4.4 Visualization\n\nlibrary(ggplot2)\n\n# Visualize the chi-square distribution\ndf_values &lt;- c(1, 3, 5, 10)\nx &lt;- seq(0, 25, length.out = 500)\n\ndf_plot &lt;- data.frame()\nfor (d in df_values) {\n  df_plot &lt;- rbind(df_plot, \n                   data.frame(x = x, \n                              y = dchisq(x, df = d), \n                              df = paste0(\"df = \", d)))\n}\n\nggplot(df_plot, aes(x = x, y = y, color = df)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Chi-Square Distribution for Various Degrees of Freedom\",\n       x = expression(chi^2),\n       y = \"Density\",\n       color = \"Degrees of\\nFreedom\") +\n  theme_minimal() +\n  xlim(0, 20) +\n  ylim(0, 0.5)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w16_Chapter10NonParStats.html#effect-of-sample-size-on-chi-square-test",
    "href": "w16_Chapter10NonParStats.html#effect-of-sample-size-on-chi-square-test",
    "title": "Chapter 10: Non-Parametric Statistics and Chi-Square Tests",
    "section": "4.5 Effect of Sample Size on Chi-Square Test",
    "text": "4.5 Effect of Sample Size on Chi-Square Test\n\n# Demonstrate how sample size affects chi-square statistic\n# while keeping proportions constant\n\n# Original table\noriginal &lt;- matrix(c(10, 20, 30, 40), nrow = 2)\ncat(\"Original table (n = 100):\\n\")\n\nOriginal table (n = 100):\n\nprint(original)\n\n     [,1] [,2]\n[1,]   10   30\n[2,]   20   40\n\nchi_original &lt;- chisq.test(original, correct = FALSE)\ncat(\"Chi-squared:\", round(chi_original$statistic, 3), \n    \"p-value:\", round(chi_original$p.value, 4), \"\\n\\n\")\n\nChi-squared: 0.794 p-value: 0.373 \n\n# Double the sample size (same proportions)\ndoubled &lt;- original * 2\ncat(\"Doubled table (n = 200):\\n\")\n\nDoubled table (n = 200):\n\nprint(doubled)\n\n     [,1] [,2]\n[1,]   20   60\n[2,]   40   80\n\nchi_doubled &lt;- chisq.test(doubled, correct = FALSE)\ncat(\"Chi-squared:\", round(chi_doubled$statistic, 3), \n    \"p-value:\", round(chi_doubled$p.value, 4), \"\\n\\n\")\n\nChi-squared: 1.587 p-value: 0.2077 \n\n# Quadruple the sample size\nquadrupled &lt;- original * 4\ncat(\"Quadrupled table (n = 400):\\n\")\n\nQuadrupled table (n = 400):\n\nprint(quadrupled)\n\n     [,1] [,2]\n[1,]   40  120\n[2,]   80  160\n\nchi_quad &lt;- chisq.test(quadrupled, correct = FALSE)\ncat(\"Chi-squared:\", round(chi_quad$statistic, 3), \n    \"p-value:\", round(chi_quad$p.value, 4), \"\\n\")\n\nChi-squared: 3.175 p-value: 0.0748 \n\ncat(\"\\nNote: Chi-squared statistic increases proportionally with sample size!\\n\")\n\n\nNote: Chi-squared statistic increases proportionally with sample size!\n\n\n\nNote: This document was converted from lecture slides for GISC6301 Geo-spatial Data Fundamentals (Fall 2025) by Tiefelsdorf.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk16-ch10.Non-Parametric Statistics and Chi-Square Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html",
    "href": "w14_Chapter08SingleTests.html",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "",
    "text": "The concepts of randomness of a sample and the sampling distribution provide the link between a sample statistic and the underlying parameter of its parent population.\nThe sample gives us a glance at the otherwise invisible and unknown parent population.\nInferential statistics allows us to assess the likelihood whether a sample statistic deviates\n[a] due to random sampling chance from an assumed value of a hypothetical reference population parameter, or\n[b] more due to some substantial differences from the assumed value of the hypothetical reference population parameter.\n\n\n\n\nThe flow chart below displays the general components of hypothesis testing. Each block starts with a particular, perhaps hypothetical, underlying population:\n\nWe assume a hypothetical reference distribution for the unknown underlying parent population, i.e., given the null hypothesis would be true the hypothetical reference distribution is equal to the parent distribution.\nWe would like to make a statement about a parameter of the unknown parent population based on the drawn sample.\nA set of hypothetical populations assume a set of possible population parameters and allow us to make what-if statements.\n\n\n\n\nConceptional components of statistical hypothesis testing\n\n\n\n\n\n\nWe develop the sampling distribution of the test statistics assuming the sample was drawn from the hypothetical reference distribution.\nWe draw a sample from the unknown parent population and calculate its associate test statistic.\nWe evaluate how likely this test statistic comes from the assumed reference distribution.\n\\(\\Rightarrow\\) Extreme test statistics (in either tail of the hypothetical reference distribution) are less likely to have been drawn from the hypothetical reference distribution. Thus they lead with error probability \\(\\alpha\\) to a rejection of the null hypothesis \\(H_0\\).\n\n\n\n\n\n\nα and β Error Animation\n\n\n\nExample: The blue sampling distribution assumes the \\(H_0: \\mu \\leq 74.6\\) is true. A test statistic beyond the critical value is less likely to have been drawn for this hypothetical reference population.\nIf the true parent population has an expectation of \\(\\mu_{H_1} = 77.0\\) then the distribution of the test statistic would be the one shown in red. (try the Shiny script HypothesesApp.R on your computer)\nOnly the \\(\\alpha\\)-error can be controlled because it depends on the hypothetical reference distribution.\nHowever, the \\(\\beta\\)-error cannot be controlled because it depends on the status of an unknown population distributions, which is unknown to us.\n\n\n\n\n\nPossible outcomes of the decision-making process about the true state of the parent population, given the observed sample is drawn from it, are:\n\n\n\n\n\n\n\n\nDecision\n\\(H_0\\) is true (unknown)\n\\(H_0\\) is false (unknown)\n\n\n\n\nReject \\(H_0\\) (based on sample)\nType I error: Small error with probability \\(\\alpha\\) assumes that the hypothetical reference distribution is true but we got an extreme sample. This leads to a rejection.\nCorrect Decision: With probability \\(1 - \\beta\\) depending on the state of the unknown parent population. This is also known as the Power at a given effect size \\(|\\mu_{H_0} - \\mu_{H_1}|\\).\n\n\nFail to reject \\(H_0\\) (based on sample)\nCorrect Decision: Tentatively \\(H_0\\) cannot be rejected. However, there is always the possibility that \\(H_1\\) is true, and we do not reject \\(H_0\\) due to sampling variation.\nType II error: Probability \\(\\beta\\) depending on a given state \\(\\mu_{H_1}\\) of the unknown population, the sample statistic falls into in non-rejection region due to sampling variation from the parent population.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#overview-hypothesis-testing",
    "href": "w14_Chapter08SingleTests.html#overview-hypothesis-testing",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "",
    "text": "The concepts of randomness of a sample and the sampling distribution provide the link between a sample statistic and the underlying parameter of its parent population.\nThe sample gives us a glance at the otherwise invisible and unknown parent population.\nInferential statistics allows us to assess the likelihood whether a sample statistic deviates\n[a] due to random sampling chance from an assumed value of a hypothetical reference population parameter, or\n[b] more due to some substantial differences from the assumed value of the hypothetical reference population parameter.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#conceptional-components-of-statistical-hypothesis-testing",
    "href": "w14_Chapter08SingleTests.html#conceptional-components-of-statistical-hypothesis-testing",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "",
    "text": "The flow chart below displays the general components of hypothesis testing. Each block starts with a particular, perhaps hypothetical, underlying population:\n\nWe assume a hypothetical reference distribution for the unknown underlying parent population, i.e., given the null hypothesis would be true the hypothetical reference distribution is equal to the parent distribution.\nWe would like to make a statement about a parameter of the unknown parent population based on the drawn sample.\nA set of hypothetical populations assume a set of possible population parameters and allow us to make what-if statements.\n\n\n\n\nConceptional components of statistical hypothesis testing",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#steps-of-hypothesis-testing",
    "href": "w14_Chapter08SingleTests.html#steps-of-hypothesis-testing",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "",
    "text": "We develop the sampling distribution of the test statistics assuming the sample was drawn from the hypothetical reference distribution.\nWe draw a sample from the unknown parent population and calculate its associate test statistic.\nWe evaluate how likely this test statistic comes from the assumed reference distribution.\n\\(\\Rightarrow\\) Extreme test statistics (in either tail of the hypothetical reference distribution) are less likely to have been drawn from the hypothetical reference distribution. Thus they lead with error probability \\(\\alpha\\) to a rejection of the null hypothesis \\(H_0\\).\n\n\n\n\n\n\nα and β Error Animation\n\n\n\nExample: The blue sampling distribution assumes the \\(H_0: \\mu \\leq 74.6\\) is true. A test statistic beyond the critical value is less likely to have been drawn for this hypothetical reference population.\nIf the true parent population has an expectation of \\(\\mu_{H_1} = 77.0\\) then the distribution of the test statistic would be the one shown in red. (try the Shiny script HypothesesApp.R on your computer)\nOnly the \\(\\alpha\\)-error can be controlled because it depends on the hypothetical reference distribution.\nHowever, the \\(\\beta\\)-error cannot be controlled because it depends on the status of an unknown population distributions, which is unknown to us.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#general-decision-errors",
    "href": "w14_Chapter08SingleTests.html#general-decision-errors",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "",
    "text": "Possible outcomes of the decision-making process about the true state of the parent population, given the observed sample is drawn from it, are:\n\n\n\n\n\n\n\n\nDecision\n\\(H_0\\) is true (unknown)\n\\(H_0\\) is false (unknown)\n\n\n\n\nReject \\(H_0\\) (based on sample)\nType I error: Small error with probability \\(\\alpha\\) assumes that the hypothetical reference distribution is true but we got an extreme sample. This leads to a rejection.\nCorrect Decision: With probability \\(1 - \\beta\\) depending on the state of the unknown parent population. This is also known as the Power at a given effect size \\(|\\mu_{H_0} - \\mu_{H_1}|\\).\n\n\nFail to reject \\(H_0\\) (based on sample)\nCorrect Decision: Tentatively \\(H_0\\) cannot be rejected. However, there is always the possibility that \\(H_1\\) is true, and we do not reject \\(H_0\\) due to sampling variation.\nType II error: Probability \\(\\beta\\) depending on a given state \\(\\mu_{H_1}\\) of the unknown population, the sample statistic falls into in non-rejection region due to sampling variation from the parent population.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#general-steps-of-classical-hypothesis-testing",
    "href": "w14_Chapter08SingleTests.html#general-steps-of-classical-hypothesis-testing",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "2.1 General Steps of Classical Hypothesis Testing",
    "text": "2.1 General Steps of Classical Hypothesis Testing\nThe steps are:\n\nFormulation of the null and the alternative hypotheses and other assumptions.\nSpecification of the test statistic and its statistical distribution under the assumed reference distribution.\nSelection of a level of significance (willingness to reject the null hypothesis even though it is correct due to having obtained an extreme sample).\nConstruction of a decision rule (i.e., critical values).\nCollection of a sample and computation of the value of the test statistic.\nDeciding in favor or against the null hypothesis based on the value of the test statistic.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#two-sided-test",
    "href": "w14_Chapter08SingleTests.html#two-sided-test",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "3.1 Two-sided Test",
    "text": "3.1 Two-sided Test\n\nTwo-sided test for the point value of the hypothetical reference parameter \\(\\theta_0\\):\n\n\\[H_0: \\theta = \\theta_0 \\quad \\text{against} \\quad H_A: \\theta \\neq \\theta_0\\]\nor equivalently \\(H_A: \\begin{cases} \\text{either } \\theta &gt; \\theta_0 \\\\ \\text{or } \\theta &lt; \\theta_0 \\end{cases}\\)\n(two-sided because the true parameter can be left or right from \\(\\theta_0\\))\nThis implies that we have two critical values, one on the left and one on the right of the sample distribution of the test statistic under the null hypothesis.\n\n\n\nTwo-sided Hypothesis Testing",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#one-sided-tests",
    "href": "w14_Chapter08SingleTests.html#one-sided-tests",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "3.2 One-sided Tests",
    "text": "3.2 One-sided Tests\n\nOne sided for an interval of parameter values:\n\n\\[H_0: \\theta \\leq \\theta_0 \\quad \\text{against} \\quad H_A: \\theta &gt; \\theta_0\\]\nor\n\\[H_0: \\theta \\geq \\theta_0 \\quad \\text{against} \\quad H_A: \\theta &lt; \\theta_0\\]\n\n\n\nOne-sided Hypothesis Testing (left-tailed)\n\n\n\n\n\nOne-sided Hypothesis Testing (right-tailed)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#important-notes-on-hypothesis-formulation",
    "href": "w14_Chapter08SingleTests.html#important-notes-on-hypothesis-formulation",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "3.3 Important Notes on Hypothesis Formulation",
    "text": "3.3 Important Notes on Hypothesis Formulation\n\nIn one-sided testing the null hypothesis is very specific by choosing a particular direction of deviation from the null hypothesis in favor of an alternative hypothesis.\nThe choice of the one- or two-sided specification depends on the problem under investigation. A one-sided hypothesis assumes that the analyst is more knowledgeable about the underlying populations.\nIn a one-sided test the critical value moves closer towards the center of the test-statistics’ distribution because all the error probability \\(\\alpha\\) is concentrated in one tail: Therefore, if the direction of the alternative hypothesis has been assumed properly, the null hypothesis will more likely be rejected.\nPlace the issue that you aim to confirm into the alternative hypothesis, because we only can control the \\(\\alpha\\)-error:\n\nI.e., rejection of the null hypothesis leads to acceptance of alternative hypothesis with a small error probability.\nThe probability \\(\\alpha\\) of erroneously rejecting the null hypothesis should be small.\nThe alternative hypothesis is usually less specific (broad range of possible parameters).\n\nAs a rule of thumb, choose the largest possible \\(\\alpha\\)-error that you can live with.\nWhen \\(H_0\\) is not rejected we need to say “we fail to reject \\(H_0\\) at the error level \\(\\alpha\\)” rather than saying “we accept \\(H_0\\) at the error level \\(\\alpha\\)” because there is always the possibility of committing a \\(\\beta\\) error (i.e., \\(H_0\\) is false but the observed sample is extreme with regards to the unknown parent population, which leads to a failure of rejecting \\(H_0\\)).\nRemember:\n\n[a] under the assumption that the null hypothesis is correct, then the variability of the test statistic only comes from sampling error,\n[b] if the alternative hypothesis is correct, then the deviation of test statistic stems from [i] a true difference to the expected value (the effect) from the value under the null hypothesis plus [ii] a general sampling error.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#a-selection-of-a-sample-statistic-and-its-sampling-distribution",
    "href": "w14_Chapter08SingleTests.html#a-selection-of-a-sample-statistic-and-its-sampling-distribution",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "4.1 [a] Selection of a Sample Statistic and Its Sampling Distribution",
    "text": "4.1 [a] Selection of a Sample Statistic and Its Sampling Distribution\n\nBased on the point estimator, e.g., \\(P = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} X_i\\) and its variances, we can design a test statistics. Note that the variance must be evaluated assuming that null hypothesis correct, that is \\(H_0: \\pi = \\pi_0\\).\nThus, the standard error for a binomial test scenario becomes:\n\n\\[\\sigma_P = \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\]\n\nWe also can approximate the distributions under the null hypothesis of this test statistic for a sufficiently large sample size \\(n &gt; 100\\) by the normal distribution and \\(\\pi_0\\) is not to close at either end of its feasible range \\([0,1]\\):\n\n\\[z(P) = \\frac{P - \\pi_0}{\\sigma_P} \\sim N(0,1)\\]\n\n4.1.1 Excursion: Exact Binomial Test\n\n\n\nBinomial Distribution\n\n\n\nFor smaller sample sizes or extreme hypothetical parameters \\(\\pi_0\\) (leading to a skewed distribution of the test statistic) the exact binomial distribution should be used.\nHowever, one most likely will not be able to work with the exact Type I error probability \\(\\alpha\\), due to the discrete nature of the binomial distribution. See: ExactBinomialTest.Rmd\n\nNotes:\n\nRather than using the distribution of the rate estimator \\(\\hat{\\pi} = \\frac{X}{n}\\), the hypothetical distribution \\(X \\sim Binomial(\\pi_0 = 0.2, n = 100)\\) of the equivalent count estimator \\(X = \\sum_{i=1}^{n} X_i\\) (number of successes) is used because it is binomial distributed.\nThe expected value under the null hypothesis is \\(E(X) = n \\cdot \\pi_0 = 20\\)\nThe critical values are \\(X_{lower} = 12\\) and \\(X_{upper} = 28\\) with their associated error probabilities \\(\\alpha_{lower} = 0.02522\\) and \\(\\alpha_{upper} = 0.02002\\), respectively.\nNote: Because the distribution is discrete the total error probability of \\(\\alpha = 0.05\\) cannot be exhausted.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#b-selection-of-the-level-of-significance",
    "href": "w14_Chapter08SingleTests.html#b-selection-of-the-level-of-significance",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "4.2 [b] Selection of the Level of Significance",
    "text": "4.2 [b] Selection of the Level of Significance\n\nSelect the significance level \\(\\alpha\\) according to the amount of risk you are willing to take to reject the correct null hypothesis by making an erroneous decision due to sampling variations.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#c-construct-the-decision-rule",
    "href": "w14_Chapter08SingleTests.html#c-construct-the-decision-rule",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "4.3 [c] Construct the Decision Rule",
    "text": "4.3 [c] Construct the Decision Rule\n\nSplit the parameter space of \\(\\pi\\) in dependence of the significance level into an area in accordance with the null hypothesis and tail areas (two-sided tests) or one tail area (one-sided test) indicating the alternative hypothesis is more valid because of a wrongly assumed null hypothesis.\n\n\n\n\nFIGURE 9-2. Sampling distribution of P, centered on hypothesized value π = .2.\n\n\n\nThe selection of the acceptance and rejection areas depends on the specification of the null hypothesis as one- or two-sided.\nDef. Critical region and critical values: The critical region corresponds to those values of the test statistics, for which the null hypothesis is rejected at a given error probability \\(\\alpha\\). The limit(s) of the critical region are (is) the critical values.\nThe two-sided null and the alternative hypotheses can be translated into:\n\n\\(H_0: T \\in ]A_1, A_2[\\) under the null hypothesis, and\n\\(H_1: T \\leq A_1 \\cup T \\geq A_2\\) under the alternative hypothesis: (these are the critical regions for a two-sided test)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#d-example-residential-mobility",
    "href": "w14_Chapter08SingleTests.html#d-example-residential-mobility",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "4.4 [d] Example: Residential Mobility",
    "text": "4.4 [d] Example: Residential Mobility\n\n\n\nTABLE 9-4. Summary of Test of Hypotheses for Residential Mobility Example\n\n\n\n\n\nFIGURE 9-3. Determining critical region limits for residential mobility example, α = .05.\n\n\n\nAssume that the observed value of for the sample proportion is \\(P = 0.26\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#the-t-distribution",
    "href": "w14_Chapter08SingleTests.html#the-t-distribution",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "6.1 The t-Distribution",
    "text": "6.1 The t-Distribution\n\n\n\nFigure 12.5. \\(t\\) distribution for 1, 30, and \\(\\infty\\) degrees of freedom\n\n\n\nThe symmetric \\(t\\)-distribution corrects this inflation of the \\(t\\)-statistic by being heavier in its tails.\nThe \\(t\\)-distribution in dependence of the sample size through the degrees of freedom \\(df = n - 1\\) is shown above.\nOne degree of freedom is lost because after the mean has been estimated only \\(n - 1\\) sample observation can vary freely.\nThe test logic remains the same as for the \\(z\\)-values.\nJust the \\(t\\)-distribution is used to calculate the critical values and evaluate the \\(t\\)-statistic:\n\n\\(t_{\\alpha/2, df}^{critical}\\) and \\(t_{1-\\alpha/2, df}^{critical}\\) for the two-sided hypothesis,\nand \\(t_{\\alpha, df}^{critical}\\) or \\(t_{1-\\alpha, df}^{critical}\\) for one-sided hypotheses testing, respectively.\n\nAs the sample size increases, the sampling distribution of the variance will become more symmetric, thus there will be an equal number of estimated variances below and above the expected value.\n\nConsequently, sample sizes larger than \\(n &gt; 30\\) the \\(t\\)-distribution can be approximated by the standard normal distribution.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#practical-relevance",
    "href": "w14_Chapter08SingleTests.html#practical-relevance",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "8.1 Practical Relevance",
    "text": "8.1 Practical Relevance\n\nA statistically significant result does not mean that it is relevant for practical purposes. Vice versa, a small but insignificant difference may be practically relevant.\nStatistical significance is based on the distribution of the test statistic under the null hypothesis, whereas practical significance is based on the absolute difference of the test statistic from the value assumed under the null hypothesis.\nThis difference needs to be larger than a given threshold value to induce action.\nExample: a homeowner has a mortgage at 5.5% interest on his/her home. Another lender offers an interest rate of 5.0% to refinance the mortgage. Will the homeowner take this offer because he/she saves substantial money after refinancing expenses?",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#deductive-reasoning",
    "href": "w14_Chapter08SingleTests.html#deductive-reasoning",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "8.2 Deductive Reasoning",
    "text": "8.2 Deductive Reasoning\n\nA sample must always be drawn after we have formalized the hypotheses.\nThis leads to problems for prior exploratory data analyses, which may bias the process of hypothesis formulation.\nThe hypothesis would become dependent on a sample, if we use the same sample to formulate the hypothesis and therefore, predetermine the outcome.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#large-sample-problem",
    "href": "w14_Chapter08SingleTests.html#large-sample-problem",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "8.3 Large Sample Problem",
    "text": "8.3 Large Sample Problem\n\nAs the sample size \\(n\\) becomes excessively large, virtually non-existing differences (i.e., effect sizes) of the test statistic from the hypothetical parameter become statistically significant because the standard error of the test statistics shrinks toward zero (remember the central limit theorem).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#multiple-testing",
    "href": "w14_Chapter08SingleTests.html#multiple-testing",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "8.4 Multiple Testing",
    "text": "8.4 Multiple Testing\n\nThe error probability increases if we perform multiple tests on the same dataset:\nAssume we perform two independent tests on the same data, the probability of not rejecting [NR] both null hypotheses correctly becomes:\n\n\\[\\Pr(NR \\cap NR) = (1 - \\alpha) \\cdot (1 - \\alpha) = (1 - \\alpha)^2 &lt; (1 - \\alpha)\\]\nTherefore, the complement error probability of rejecting [R] at least one test incorrectly becomes:\n\\[\\Pr(R \\cup R) = 1 - (1 - \\alpha)^2 = \\alpha_{new} \\quad \\text{with} \\quad \\alpha_{new} &gt; \\alpha\\]\nConsequently, the error probability \\(\\alpha_{new}\\) for each individual test must be set to something smaller, e.g., Bonferroni adjustment:\n\\[\\alpha_{new} = \\frac{\\alpha}{K}\\]\nwhere \\(K\\) is the number of tests performed on the same data set.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#test-under-the-null-hypothesis-of-independence-see-bbr-p486-487",
    "href": "w14_Chapter08SingleTests.html#test-under-the-null-hypothesis-of-independence-see-bbr-p486-487",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "9.1 Test Under the Null Hypothesis of Independence (see BBR p486-487)",
    "text": "9.1 Test Under the Null Hypothesis of Independence (see BBR p486-487)\n\nAssumption: The bivariate reference population consists of two uncorrelated variables.\nThis leads to the null hypothesis \\(H_0: \\rho_0 = 0.0\\) and the alternative hypothesis \\(H_1: \\rho_0 \\neq 0.0\\).\nAssuming a correlation of zero under the null hypothesis is sensible because it is least specific and neutral with regards to a potential dependency between two variables.\nThe population correlation parameter is denoted by \\(\\rho\\) whereas the sample statistic is denoted by \\(r\\).\nIf we can assume that both variables \\(X_1\\) and \\(X_2\\) are approximately jointly normal distributed and uncorrelated, then we do not need to evaluate the sampling distribution with a simulation experiment. Its distribution is known:\n\nThe sample correlation coefficient \\(r\\) follows under the null hypothesis \\(H_0: \\rho = 0\\) a t-distribution with \\(df = n - 2\\) degrees of freedom where \\(n\\) is the number of observations.\nIt has an expected value of \\(E(r) = 0\\) and a standard error of \\(\\sqrt{Var(r)} = \\sqrt{\\frac{1 - r^2}{n - 2}}\\), that is:\n\n\n\\[t = \\frac{r - E(r)}{\\sqrt{Var(r)}} \\sim t_{df=n-2}\\]\n\nWe are losing two degrees of freedom for the correlation coefficient because for each variable first their means need to be estimated.\n\n\n\n\nSampling Distribution of \\(\\hat{\\rho}\\) given population \\(\\rho = 0\\) and \\(n = 64\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#test-statistic-for-correlation",
    "href": "w14_Chapter08SingleTests.html#test-statistic-for-correlation",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "9.2 Test Statistic for Correlation",
    "text": "9.2 Test Statistic for Correlation\n\nThe test statistic becomes:\n\n\\[t = \\frac{r - E(r)}{\\sqrt{Var(r)}} = \\frac{r \\cdot \\sqrt{n - 2}}{\\sqrt{1 - r^2}}\\]\n\nExtreme values \\(t\\) indicate that the pairs of sample observations are most likely not originating from an uncorrelated bivariate parent population.\nFor a sample sizes of more than \\(n &gt; 30\\) observation pairs the \\(t\\)-distribution can be approximated by the standard normal distribution.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#fisher-z-transformation",
    "href": "w14_Chapter08SingleTests.html#fisher-z-transformation",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "10.1 Fisher z-Transformation",
    "text": "10.1 Fisher z-Transformation\n\nHowever, the so-called Fisher z-transformation can be used to transform the correlation coefficient \\(r\\) to be approximately normal distributed:\n\n\\[z(r) = \\frac{1}{2} \\cdot \\ln\\left(\\frac{1 + r}{1 - r}\\right)\\]\nwith\n\\[E[z(r)] = \\frac{1}{2} \\cdot \\ln\\left(\\frac{1 + \\rho_0}{1 - \\rho_0}\\right)\\]\nand\n\\[Var[z(r)] = \\frac{1}{N - 3}\\]\nTherefore,\n\\[\\frac{z(r) - E[z(r)]}{\\sqrt{Var[z(r)]}} \\sim N(0,1)\\]\nis approximately standard normal distributed and can be used to test:\n\\[H_0: \\rho = \\rho_0 \\quad \\text{against} \\quad H_1: \\rho \\neq \\rho_0\\]\n\n\n\nFisher z-transformed Distribution of \\(\\hat{\\rho}\\) given population \\(\\rho = 0.8\\) and \\(n = 64\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#one-sample-t-test",
    "href": "w14_Chapter08SingleTests.html#one-sample-t-test",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "11.1 One-Sample t-Test",
    "text": "11.1 One-Sample t-Test\n\n# Example: Testing if population mean equals a hypothesized value\nset.seed(123)\nsample_data &lt;- rnorm(25, mean = 52, sd = 10)\n\n# Hypotheses: H0: mu = 50 vs H1: mu != 50\nmu_0 &lt;- 50\n\n# Perform t-test\nt_result &lt;- t.test(sample_data, mu = mu_0)\nprint(t_result)\n\n\n    One Sample t-test\n\ndata:  sample_data\nt = 0.88024, df = 24, p-value = 0.3875\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 47.75878 55.57462\nsample estimates:\nmean of x \n  51.6667 \n\n# Manual calculation\nx_bar &lt;- mean(sample_data)\ns &lt;- sd(sample_data)\nn &lt;- length(sample_data)\nt_stat &lt;- (x_bar - mu_0) / (s / sqrt(n))\ndf &lt;- n - 1\np_value &lt;- 2 * pt(-abs(t_stat), df = df)\n\ncat(\"\\nManual Calculation:\\n\")\n\n\nManual Calculation:\n\ncat(\"Sample mean:\", round(x_bar, 3), \"\\n\")\n\nSample mean: 51.667 \n\ncat(\"Sample SD:\", round(s, 3), \"\\n\")\n\nSample SD: 9.467 \n\ncat(\"t-statistic:\", round(t_stat, 3), \"\\n\")\n\nt-statistic: 0.88 \n\ncat(\"Degrees of freedom:\", df, \"\\n\")\n\nDegrees of freedom: 24 \n\ncat(\"p-value (two-sided):\", round(p_value, 4), \"\\n\")\n\np-value (two-sided): 0.3875",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#one-sample-proportion-test",
    "href": "w14_Chapter08SingleTests.html#one-sample-proportion-test",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "11.2 One-Sample Proportion Test",
    "text": "11.2 One-Sample Proportion Test\n\n# Example: Testing population proportion\n# H0: pi = 0.20 vs H1: pi != 0.20\nsuccesses &lt;- 26\nn &lt;- 100\npi_0 &lt;- 0.20\n\n# Using prop.test (with continuity correction)\nprop_result &lt;- prop.test(successes, n, p = pi_0)\nprint(prop_result)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  successes out of n, null probability pi_0\nX-squared = 1.8906, df = 1, p-value = 0.1691\nalternative hypothesis: true p is not equal to 0.2\n95 percent confidence interval:\n 0.1797427 0.3590222\nsample estimates:\n   p \n0.26 \n\n# Manual z-test calculation\np_hat &lt;- successes / n\nse &lt;- sqrt(pi_0 * (1 - pi_0) / n)\nz_stat &lt;- (p_hat - pi_0) / se\np_value_z &lt;- 2 * pnorm(-abs(z_stat))\n\ncat(\"\\nManual z-test:\\n\")\n\n\nManual z-test:\n\ncat(\"Sample proportion:\", p_hat, \"\\n\")\n\nSample proportion: 0.26 \n\ncat(\"Standard error:\", round(se, 4), \"\\n\")\n\nStandard error: 0.04 \n\ncat(\"z-statistic:\", round(z_stat, 3), \"\\n\")\n\nz-statistic: 1.5 \n\ncat(\"p-value (two-sided):\", round(p_value_z, 4), \"\\n\")\n\np-value (two-sided): 0.1336",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#correlation-test",
    "href": "w14_Chapter08SingleTests.html#correlation-test",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "11.3 Correlation Test",
    "text": "11.3 Correlation Test\n\n# Example: Testing correlation coefficient\nset.seed(456)\nn &lt;- 30\nx &lt;- rnorm(n)\ny &lt;- 0.6 * x + rnorm(n, sd = 0.8)  # Correlated with rho ~ 0.6\n\n# Test H0: rho = 0\ncor_result &lt;- cor.test(x, y)\nprint(cor_result)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 4.1711, df = 28, p-value = 0.0002652\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3330771 0.8007416\nsample estimates:\n      cor \n0.6190606 \n\n# Manual calculation\nr &lt;- cor(x, y)\nt_stat_cor &lt;- r * sqrt(n - 2) / sqrt(1 - r^2)\ndf_cor &lt;- n - 2\np_value_cor &lt;- 2 * pt(-abs(t_stat_cor), df = df_cor)\n\ncat(\"\\nManual Calculation:\\n\")\n\n\nManual Calculation:\n\ncat(\"Sample correlation:\", round(r, 4), \"\\n\")\n\nSample correlation: 0.6191 \n\ncat(\"t-statistic:\", round(t_stat_cor, 3), \"\\n\")\n\nt-statistic: 4.171 \n\ncat(\"p-value:\", round(p_value_cor, 4), \"\\n\")\n\np-value: 3e-04",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#fisher-z-transformation-1",
    "href": "w14_Chapter08SingleTests.html#fisher-z-transformation-1",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "11.4 Fisher z-Transformation",
    "text": "11.4 Fisher z-Transformation\n\n# Fisher z-transformation for testing rho = rho_0 (non-zero)\nfisher_z &lt;- function(r) {\n  0.5 * log((1 + r) / (1 - r))\n}\n\n# Example: Test H0: rho = 0.5 vs H1: rho != 0.5\nr &lt;- 0.72\nn &lt;- 50\nrho_0 &lt;- 0.5\n\nz_r &lt;- fisher_z(r)\nz_rho &lt;- fisher_z(rho_0)\nse_z &lt;- 1 / sqrt(n - 3)\nz_stat_fisher &lt;- (z_r - z_rho) / se_z\np_value_fisher &lt;- 2 * pnorm(-abs(z_stat_fisher))\n\ncat(\"Testing H0: rho =\", rho_0, \"vs H1: rho !=\", rho_0, \"\\n\")\n\nTesting H0: rho = 0.5 vs H1: rho != 0.5 \n\ncat(\"Sample r:\", r, \"\\n\")\n\nSample r: 0.72 \n\ncat(\"Fisher z(r):\", round(z_r, 4), \"\\n\")\n\nFisher z(r): 0.9076 \n\ncat(\"Fisher z(rho_0):\", round(z_rho, 4), \"\\n\")\n\nFisher z(rho_0): 0.5493 \n\ncat(\"z-statistic:\", round(z_stat_fisher, 3), \"\\n\")\n\nz-statistic: 2.457 \n\ncat(\"p-value:\", round(p_value_fisher, 4), \"\\n\")\n\np-value: 0.014",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#visualizing-hypothesis-testing",
    "href": "w14_Chapter08SingleTests.html#visualizing-hypothesis-testing",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "11.5 Visualizing Hypothesis Testing",
    "text": "11.5 Visualizing Hypothesis Testing\n\nlibrary(ggplot2)\n\n# Two-sided hypothesis test visualization\nalpha &lt;- 0.05\nmu_0 &lt;- 0\nx &lt;- seq(-4, 4, length.out = 1000)\ny &lt;- dnorm(x)\n\n# Critical values\nz_crit &lt;- qnorm(1 - alpha/2)\n\ndf &lt;- data.frame(x = x, y = y)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line(color = \"black\", linewidth = 1) +\n  geom_area(data = subset(df, x &lt;= -z_crit), aes(x = x, y = y), \n            fill = \"red\", alpha = 0.5) +\n  geom_area(data = subset(df, x &gt;= z_crit), aes(x = x, y = y), \n            fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = c(-z_crit, z_crit), linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = -z_crit, y = 0.05, label = paste0(\"-z[α/2] = \", round(-z_crit, 2)), \n           hjust = 1.1, parse = FALSE) +\n  annotate(\"text\", x = z_crit, y = 0.05, label = paste0(\"z[α/2] = \", round(z_crit, 2)), \n           hjust = -0.1, parse = FALSE) +\n  annotate(\"text\", x = -3, y = 0.1, label = paste0(\"α/2 = \", alpha/2), color = \"red\") +\n  annotate(\"text\", x = 3, y = 0.1, label = paste0(\"α/2 = \", alpha/2), color = \"red\") +\n  annotate(\"text\", x = 0, y = 0.2, label = paste0(\"1 - α = \", 1 - alpha)) +\n  labs(title = \"Two-Sided Hypothesis Test (α = 0.05)\",\n       subtitle = \"Reject H₀ if |z| &gt; 1.96\",\n       x = \"z-value\",\n       y = \"Density\") +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w14_Chapter08SingleTests.html#power-analysis",
    "href": "w14_Chapter08SingleTests.html#power-analysis",
    "title": "Chapter 08: One-Sample Hypothesis Tests",
    "section": "11.6 Power Analysis",
    "text": "11.6 Power Analysis\n\n# Visualizing power and Type II error\nlibrary(ggplot2)\n\n# Parameters\nmu_0 &lt;- 0      # Mean under H0\nmu_1 &lt;- 1.5    # Mean under H1 (true mean)\nsigma &lt;- 1     # Standard deviation\nn &lt;- 25        # Sample size\nalpha &lt;- 0.05  # Significance level\n\nse &lt;- sigma / sqrt(n)\nz_crit &lt;- qnorm(1 - alpha/2)\ncrit_value &lt;- mu_0 + z_crit * se\n\nx &lt;- seq(-3, 5, length.out = 1000)\ny_h0 &lt;- dnorm(x, mean = mu_0, sd = se)\ny_h1 &lt;- dnorm(x, mean = mu_1, sd = se)\n\ndf_h0 &lt;- data.frame(x = x, y = y_h0, dist = \"H0\")\ndf_h1 &lt;- data.frame(x = x, y = y_h1, dist = \"H1\")\n\n# Calculate power\npower &lt;- 1 - pnorm(crit_value, mean = mu_1, sd = se)\nbeta &lt;- pnorm(crit_value, mean = mu_1, sd = se)\n\nggplot() +\n  # H0 distribution\n  geom_line(data = df_h0, aes(x = x, y = y), color = \"blue\", linewidth = 1) +\n  geom_area(data = subset(df_h0, x &gt;= crit_value), aes(x = x, y = y), \n            fill = \"blue\", alpha = 0.3) +\n  # H1 distribution\n  geom_line(data = df_h1, aes(x = x, y = y), color = \"red\", linewidth = 1) +\n  geom_area(data = subset(df_h1, x &gt;= crit_value), aes(x = x, y = y), \n            fill = \"green\", alpha = 0.3) +\n  geom_area(data = subset(df_h1, x &lt;= crit_value), aes(x = x, y = y), \n            fill = \"red\", alpha = 0.3) +\n  # Critical value line\n  geom_vline(xintercept = crit_value, linetype = \"dashed\", color = \"black\") +\n  # Annotations\n  annotate(\"text\", x = mu_0, y = max(y_h0) * 1.1, label = \"H₀: μ = 0\", color = \"blue\") +\n  annotate(\"text\", x = mu_1, y = max(y_h1) * 1.1, label = \"H₁: μ = 1.5\", color = \"red\") +\n  annotate(\"text\", x = crit_value + 0.5, y = max(y_h0) * 0.6, \n           label = paste0(\"α = \", round(alpha, 2)), color = \"blue\") +\n  annotate(\"text\", x = mu_1 - 0.8, y = max(y_h1) * 0.4, \n           label = paste0(\"β = \", round(beta, 3)), color = \"red\") +\n  annotate(\"text\", x = mu_1 + 0.8, y = max(y_h1) * 0.4, \n           label = paste0(\"Power = \", round(power, 3)), color = \"darkgreen\") +\n  labs(title = \"Type I Error (α), Type II Error (β), and Power\",\n       subtitle = paste0(\"n = \", n, \", σ = \", sigma, \", α = \", alpha),\n       x = \"Sample Mean\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNote: This document was converted from lecture slides for GISC6301 Geo-spatial Data Fundamentals (Fall 2025) by Tiefelsdorf.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk14-ch8.One-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html",
    "href": "w12_Chapter06Sampling.html",
    "title": "Chapter 06: Sampling",
    "section": "",
    "text": "Sampling ultimately aims to gain information about the underlying population by using a sample of observations from the population. The information could be used to:\n\napproximate the distribution of the underlying population, or\nto estimate some of its distributional parameters (e.g., central tendency, variability, regression coefficients etc.)\n\nGeneral questions to be addressed before proceeding with sampling:\n\nHow do we obtain a representative sample from the underlying population?\nWhich objects and how many objects must be included into the sample to provide on average from sample to sample an accurate snapshot of the total underlying population?\n\nLack of representativeness leads to a biased sample.\n\n\nExample: Industrious students are more likely to be on campus. Random interviews on campus have the tendency to oversample this group.\n\n\nDue to the nature of random sampling (one sample will be different from any other sample), the information obtained from sampled observations always will deviate to some degree from the underlying population → we just should aim at making these deviations small and well balanced.\n\n\n\n\n\n\nThe likelihood of obtaining a representative sample increases as we sample a larger representative cross-section of the population. Again: avoid biases.\nRepresentative cross-section means that no particular sub-group of the population should be favored to be included into or excluded from the sample.\n\n\nCounterexample: Estimation of election outcomes in the “big data” article. The Literary Digest only sampled affluent people (i.e., their subscribers) about their presidential election attitudes.\n\n\nUncertainty is the price we pay for not fully enumerating every object of the underlying population.\n\n\n\n\n\n\n\nDefinition: Sampling error is the uncertainty that arises by working with a (random) sample rather than with the entire population.\n\n\n\nDefinition: Sampling bias occurs when the procedure used to draw sample observations selectively favors the inclusion and/or suppression of specific population members. This sample is not representative of the underlying population.\n\n\n\nSampling bias can be avoided by implementing:\n[a] a general understanding of the underlying population\n[b] an appropriate sampling plan\n[c] check for recording errors of the sampled data",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#sampling-objectives",
    "href": "w12_Chapter06Sampling.html#sampling-objectives",
    "title": "Chapter 06: Sampling",
    "section": "",
    "text": "Sampling ultimately aims to gain information about the underlying population by using a sample of observations from the population. The information could be used to:\n\napproximate the distribution of the underlying population, or\nto estimate some of its distributional parameters (e.g., central tendency, variability, regression coefficients etc.)\n\nGeneral questions to be addressed before proceeding with sampling:\n\nHow do we obtain a representative sample from the underlying population?\nWhich objects and how many objects must be included into the sample to provide on average from sample to sample an accurate snapshot of the total underlying population?\n\nLack of representativeness leads to a biased sample.\n\n\nExample: Industrious students are more likely to be on campus. Random interviews on campus have the tendency to oversample this group.\n\n\nDue to the nature of random sampling (one sample will be different from any other sample), the information obtained from sampled observations always will deviate to some degree from the underlying population → we just should aim at making these deviations small and well balanced.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#impact-of-sample-size",
    "href": "w12_Chapter06Sampling.html#impact-of-sample-size",
    "title": "Chapter 06: Sampling",
    "section": "",
    "text": "The likelihood of obtaining a representative sample increases as we sample a larger representative cross-section of the population. Again: avoid biases.\nRepresentative cross-section means that no particular sub-group of the population should be favored to be included into or excluded from the sample.\n\n\nCounterexample: Estimation of election outcomes in the “big data” article. The Literary Digest only sampled affluent people (i.e., their subscribers) about their presidential election attitudes.\n\n\nUncertainty is the price we pay for not fully enumerating every object of the underlying population.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#key-definitions",
    "href": "w12_Chapter06Sampling.html#key-definitions",
    "title": "Chapter 06: Sampling",
    "section": "",
    "text": "Definition: Sampling error is the uncertainty that arises by working with a (random) sample rather than with the entire population.\n\n\n\nDefinition: Sampling bias occurs when the procedure used to draw sample observations selectively favors the inclusion and/or suppression of specific population members. This sample is not representative of the underlying population.\n\n\n\nSampling bias can be avoided by implementing:\n[a] a general understanding of the underlying population\n[b] an appropriate sampling plan\n[c] check for recording errors of the sampled data",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#probability-sample",
    "href": "w12_Chapter06Sampling.html#probability-sample",
    "title": "Chapter 06: Sampling",
    "section": "4.1 Probability Sample",
    "text": "4.1 Probability Sample\nDefinition: The probability of any individual member of the population being selected into the sample can be determined.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#simple-random-sample-from-a-finite-population",
    "href": "w12_Chapter06Sampling.html#simple-random-sample-from-a-finite-population",
    "title": "Chapter 06: Sampling",
    "section": "4.2 Simple Random Sample from a Finite Population",
    "text": "4.2 Simple Random Sample from a Finite Population\nDefinition: A simple random sample from a finite population of size \\(N\\) is one in which each possible sample object has an equal selection probability.\n\nSimple random sampling does not rule out the chance of obtaining a set of extreme sample observations. However, since we know the selection probability, we can calculate the probability of obtaining such an extreme sample.\nWith increasing unbiased sample size, the probability of obtaining an extreme sample is decreasing.\n\n\n4.2.1 Example: Enumeration of Sample Combinations\nFull enumeration of all combinations of 2 sample objects out of a population of 4 without repetition:\n\\[\\binom{4}{2} = \\frac{4!}{(4-2)! \\cdot 2!} = 6\\]\n\n\n\nFIGURE 6-3: Samples of size n = 2 from Population N = 4\n\n\n\n\n4.2.2 Problem: Dependent Draws\nSampling without replacement leads to statistically dependent draws. The probability of selecting the second observation changes after the first observation has been selected.\n\n\n4.2.3 Independence Approximation\nRather than enumerating all possible combinations as in Figure 6-3:\n\nWe can assume for large enough populations in which each member in the population has an equal probability of being selected into the sample.\nTherefore, the individual draws of sample members become approximately statistically independent among each other.\n\nThus, the probability of any set of sampled members can be approximated by the individual probabilities:\n\\[\\Pr(\\omega_1 \\cap \\omega_2 \\cap \\cdots \\cap \\omega_n) = \\Pr(\\omega_1) \\cdot \\Pr(\\omega_2) \\cdots \\Pr(\\omega_n) = \\pi^n\\]\nwhich assumes independence of each object \\(\\omega_i\\) being drawn into the sample with probability \\(\\pi = \\frac{1}{n}\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#population-parameters-vs-sample-statistics",
    "href": "w12_Chapter06Sampling.html#population-parameters-vs-sample-statistics",
    "title": "Chapter 06: Sampling",
    "section": "5.1 Population Parameters vs Sample Statistics",
    "text": "5.1 Population Parameters vs Sample Statistics\n\nRecall: the population parameters are denoted by Greek characters, e.g., \\(\\mu\\) and \\(\\sigma^2\\), and the sample statistics are denoted by Latin letters or have a hat on top, e.g., means \\(\\bar{x}\\) and \\(s^2\\) or by the population parameter with a hat on top, i.e., \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}^2\\), respectively.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#definition-sample-statistic",
    "href": "w12_Chapter06Sampling.html#definition-sample-statistic",
    "title": "Chapter 06: Sampling",
    "section": "5.2 Definition: Sample Statistic",
    "text": "5.2 Definition: Sample Statistic\nA sample statistic is itself a random variable – because it is based on the random set of variables \\(X_1, X_2, \\ldots, X_n\\) in the sample – that ties these individual random variables together through some functional expression.\nExample: The sample statistic function is:\n\\[\\bar{X} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} X_i\\]\n(notice use of random variables, i.e., large caps letters) and its random outcome for a particular sample becomes:\n\\[\\bar{x} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} x_i\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#definition",
    "href": "w12_Chapter06Sampling.html#definition",
    "title": "Chapter 06: Sampling",
    "section": "6.1 Definition",
    "text": "6.1 Definition\nDefinition: Sampling Distribution of a Statistic: A sampling distribution is a probability distribution of a sample statistic.\nThat is, the sample statistic must have a distribution because it is calculated from a set of random variables.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#developing-the-sampling-distribution",
    "href": "w12_Chapter06Sampling.html#developing-the-sampling-distribution",
    "title": "Chapter 06: Sampling",
    "section": "6.2 Developing the Sampling Distribution",
    "text": "6.2 Developing the Sampling Distribution\nThe sampling distribution of a statistic can be, in theory, developed:\n[1] by taking all possible samples of size \\(n\\) from a population,\n[2] calculating the values of the sample statistic for each of these sampling outcomes, and\n[3] drawing the distribution of the values of the observed sample statistic.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#example-distribution-of-the-sample-mean",
    "href": "w12_Chapter06Sampling.html#example-distribution-of-the-sample-mean",
    "title": "Chapter 06: Sampling",
    "section": "6.3 Example: Distribution of the Sample Mean",
    "text": "6.3 Example: Distribution of the Sample Mean\nEvaluate the distribution of the sample mean based on \\(n = 3\\) random draws without replacement and irrespectively of their order from a population of size \\(N = 5\\).\n\n\n\nPopulation Values Table\n\n\n\n\n\nTABLE 7-2: Possible Samples of Size n = 3 from Population N = 5\n\n\n\n\n\nFIGURE 7-6: Sampling distribution of the sample mean\n\n\n\n6.3.1 Expected Value of the Sampling Distribution\nThis distribution can be characterized by its expected value:\n\\[E(\\bar{X} \\mid n = 3, N = 5) = \\frac{1}{10} \\cdot 4.3 + \\frac{2}{10} \\cdot 4.7 + \\frac{4}{10} \\cdot 5.0 + \\frac{2}{10} \\cdot 5.3 + \\frac{1}{10} \\cdot 5.7 = 5.0\\]\n\n\n6.3.2 Variance of the Sampling Distribution\n\\[Var(\\bar{X} \\mid n = 3, N = 5) = \\frac{1}{10} \\cdot (4.3 - 5.0)^2 + \\frac{2}{10} \\cdot (4.7 - 5.0)^2 + \\frac{4}{10} \\cdot (5.0 - 5.0)^2 + \\frac{2}{10} \\cdot (5.3 - 5.0)^2 + \\frac{1}{10} \\cdot (5.7 - 5.0)^2 = 0.134\\]\nwith \\(\\sqrt{Var(\\bar{X} \\mid n = 3, N = 5)} = 0.366\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#unbiasedness",
    "href": "w12_Chapter06Sampling.html#unbiasedness",
    "title": "Chapter 06: Sampling",
    "section": "6.4 Unbiasedness",
    "text": "6.4 Unbiasedness\nIf the expected value of the sample statistic is equal to the expected value in the population, then the sampling statistic is said to be unbiased.\nWe usually prefer statistical estimation rules that are unbiased.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#standard-error",
    "href": "w12_Chapter06Sampling.html#standard-error",
    "title": "Chapter 06: Sampling",
    "section": "6.5 Standard Error",
    "text": "6.5 Standard Error\nThe standard deviation of the sampling statistics is called the standard error. For the mean statistic its standard error is denoted by \\(s_{\\bar{X}}\\).\nThe standard error measures the degree of uncertainty that the sample statistic will deviate from its expected population value.\nWe prefer statistical estimation rules that lead to small standard errors (i.e., small uncertainty).\n\n\n\nFIGURE 7-7: Central limit theorem and the distribution of sample means",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#definition-1",
    "href": "w12_Chapter06Sampling.html#definition-1",
    "title": "Chapter 06: Sampling",
    "section": "7.1 Definition",
    "text": "7.1 Definition\nDefinition: Central Limit Theorem: Let \\(X_1, X_2, \\ldots, X_n\\) be a random independent sample of size \\(n\\) drawn from an arbitrarily distributed population with expectation \\(\\mu\\) and standard deviation \\(\\sigma\\). Then for large enough sample sizes \\(n\\), the sampling distribution of the mean \\(\\bar{X}\\) is asymptotically (i.e., as \\(n \\to \\infty\\)) normal distributed with:\n\\[\\bar{X} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#two-parts-of-the-theorem",
    "href": "w12_Chapter06Sampling.html#two-parts-of-the-theorem",
    "title": "Chapter 06: Sampling",
    "section": "7.2 Two Parts of the Theorem",
    "text": "7.2 Two Parts of the Theorem\n\n7.2.1 Part 1: Expected Value and Variance\nIrrespectively of the sample size the expected value of the mean \\(\\bar{X}\\) is:\n\\[E(\\bar{X}) = \\mu\\]\nand its variance is:\n\\[Var(\\bar{X}) = \\sigma^2/n\\]\nNote, \\(n\\) in the denominator. Therefore, as the sample size \\(n\\) increases the standard error (or variance) \\(s_{\\bar{X}} = \\sqrt{\\sigma^2/n} = \\frac{\\sigma}{\\sqrt{n}}\\) of the mean will shrink.\n\n\n7.2.2 Part 2: Asymptotic Normality\nAsymptotically the sample mean will follow a normal distribution irrespective of the underlying distribution of the population.\n\n\n7.2.3 Proof for Independent Sample Objects\n\\[Var\\left(\\frac{1}{n} \\cdot \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\cdot \\sum_{i=1}^{n} Var(X_i) = \\frac{1}{n^2} \\cdot \\underbrace{n \\cdot \\sigma^2}_{=\\sigma^2} = \\frac{\\sigma^2}{n}\\]\n\n\n\nFIGURE 2.24: Three samples of five observations drawn randomly from U-shaped distribution. Means of samples are shown by X-bar.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#example-central-limit-theorem-simulation",
    "href": "w12_Chapter06Sampling.html#example-central-limit-theorem-simulation",
    "title": "Chapter 06: Sampling",
    "section": "7.3 Example: Central Limit Theorem Simulation",
    "text": "7.3 Example: Central Limit Theorem Simulation\nExample: Central limit theorem with the R-script CENTRALLIMIT.R:\n\n\n\nCentral Limit Theorem Simulation: Three different population distributions (Population 1, 2, 3) with sampling distributions at n=4, n=16, and n=256",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#objectives",
    "href": "w12_Chapter06Sampling.html#objectives",
    "title": "Chapter 06: Sampling",
    "section": "8.1 Objectives",
    "text": "8.1 Objectives\nThe objective of sampling theory is to develop sampling plans and statistics that lead to the most precise estimators of population properties that we are interested in, i.e., estimators with low uncertainty (standard error) and control for any biases.\nThis is an optimization problem, perhaps, under constraints.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#weighting-to-control-for-biases",
    "href": "w12_Chapter06Sampling.html#weighting-to-control-for-biases",
    "title": "Chapter 06: Sampling",
    "section": "8.2 Weighting to Control for Biases",
    "text": "8.2 Weighting to Control for Biases\nWeighting can control for biases:\n\nThe impact of observations with a higher probability of being selected into the sample need to be weighted down, and\nThe impact of observations with lower selection probability needs to be weighted upwards.\n\nThis may achieve representativeness.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#definition-2",
    "href": "w12_Chapter06Sampling.html#definition-2",
    "title": "Chapter 06: Sampling",
    "section": "9.1 Definition",
    "text": "9.1 Definition\nDefinition: Stratified Random Sampling: A stratified random sample is obtained by:\n[1] splitting the population into \\(k\\) preferably homogeneous groups – also called strata – and\n[2] selecting a simple random samples of a predetermined size \\(n_j\\) from each stratum \\(j\\).\nThe observed sample statistics from these strata-specific samples are then combined into the global sample statistic. Each stratum will have a stratum-specific weight.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#homogeneity-assumption",
    "href": "w12_Chapter06Sampling.html#homogeneity-assumption",
    "title": "Chapter 06: Sampling",
    "section": "9.2 Homogeneity Assumption",
    "text": "9.2 Homogeneity Assumption\n\nExternal knowledge is required to split the population into \\(k\\) preferably homogenous strata (the objects in each strata are similar with respect to desired attributes).\nExternal proxy variables that are closely correlated with the measure attributes can be used as surrogates to define the strata.\nHomogeneity means that the variances of the sub-populations within each stratum are less than the overall population variance.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#advantage",
    "href": "w12_Chapter06Sampling.html#advantage",
    "title": "Chapter 06: Sampling",
    "section": "9.3 Advantage",
    "text": "9.3 Advantage\nThe additional control leads to a reduction in the overall sampling error while maintaining unbiasedness.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#strata-specific-data-structure",
    "href": "w12_Chapter06Sampling.html#strata-specific-data-structure",
    "title": "Chapter 06: Sampling",
    "section": "9.4 Strata-Specific Data Structure",
    "text": "9.4 Strata-Specific Data Structure\nThe underlying strata-specific data structure for \\(k\\) strata becomes:\n\n\n\n\n\n\n\n\n\n\nStrata\n1\n2\n\\(\\cdots\\)\n\\(k\\)\n\n\n\n\nSize\n\\(N_1\\)\n\\(N_2\\)\n\\(\\cdots\\)\n\\(N_k\\)\n\n\nVariance\n\\(\\sigma_1^2\\)\n\\(\\sigma_2^2\\)\n\\(\\cdots\\)\n\\(\\sigma_k^2\\)\n\n\nSampling cost per unit\n\\(c_1\\)\n\\(c_2\\)\n\\(\\cdots\\)\n\\(c_k\\)\n\n\nPopulation\n\\(\\{X_{1,1}, X_{2,1}, \\ldots, X_{N_1,1}\\}\\)\n\\(\\{X_{1,2}, X_{2,2}, \\ldots, X_{N_2,2}\\}\\)\n\\(\\cdots\\)\n\\(\\{X_{1,k}, X_{2,k}, \\ldots, X_{N_k,k}\\}\\)\n\n\nOutcome: Strata-specific sample size\n\\(n_1\\)\n\\(n_2\\)\n\\(\\cdots\\)\n\\(n_k\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#challenges-of-stratification",
    "href": "w12_Chapter06Sampling.html#challenges-of-stratification",
    "title": "Chapter 06: Sampling",
    "section": "9.5 Challenges of Stratification",
    "text": "9.5 Challenges of Stratification\n\nWe must have some external knowledge about the population characteristics to stratify it properly so that the internal strata variances become small.\nThe strata membership for each object in the population must be known.\nThe sub-population size in each stratum must be known.\nWe should have a rough idea of the costs of obtaining a sample observation from each stratum. These costs may vary from stratum to stratum.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#estimates",
    "href": "w12_Chapter06Sampling.html#estimates",
    "title": "Chapter 06: Sampling",
    "section": "9.6 Estimates",
    "text": "9.6 Estimates\n\n9.6.1 Strata Means\n\\[\\bar{x}_j = \\frac{1}{n_j} \\cdot \\sum_{i=1}^{n_j} x_{i,j}\\]\n\n\n9.6.2 Strata Variance\n\\[s_j^2 = \\frac{1}{n_j - 1} \\cdot \\sum_{i=1}^{n_j} \\left(x_{i,j} - \\bar{x}_j\\right)^2\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#overall-estimated-mean-and-variance",
    "href": "w12_Chapter06Sampling.html#overall-estimated-mean-and-variance",
    "title": "Chapter 06: Sampling",
    "section": "9.7 Overall Estimated Mean and Variance",
    "text": "9.7 Overall Estimated Mean and Variance\nThe overall estimated mean and variance become weighted estimates of the strata statistics:\n\n9.7.1 Overall Mean\n\\[\\bar{x}_{overall} = \\frac{1}{N} \\cdot \\sum_{j=1}^{k} N_j \\cdot \\bar{x}_j\\]\n\n\n9.7.2 Overall Variance\n\\[s_{overall}^2 = \\underbrace{\\frac{1}{N} \\cdot \\sum_{j=1}^{k} N_j \\cdot s_j^2}_{\\text{within strata variation}} + \\underbrace{\\frac{1}{N} \\cdot \\sum_{j=1}^{k} N_j \\cdot \\left(\\bar{x}_j - \\bar{x}_{overall}\\right)^2}_{\\text{between strata variation}}\\]\n\nNote: Not weighting the strata statistics leads to biased overall estimates.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#population-and-sample-size-constraints",
    "href": "w12_Chapter06Sampling.html#population-and-sample-size-constraints",
    "title": "Chapter 06: Sampling",
    "section": "9.8 Population and Sample Size Constraints",
    "text": "9.8 Population and Sample Size Constraints\n\nThe total population size is \\(N = N_1 + N_2 + \\cdots + N_k\\) and for each sub-population \\(N_j \\geq 2\\).\nThe stratum-specific sample size \\(n_j\\) needs to satisfy the constraints:\n\n\\(0 \\leq n_j \\leq N_j \\quad \\forall j \\in \\{1, 2, \\ldots, k\\}\\)\n\\(n_j\\) needs to be integer numbers\nFor large \\(N_j\\) usually sampling with replacement is assumed to make calculations easier.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#optimization-problem",
    "href": "w12_Chapter06Sampling.html#optimization-problem",
    "title": "Chapter 06: Sampling",
    "section": "9.9 Optimization Problem",
    "text": "9.9 Optimization Problem\nThe optimization problem to determine the best strata sample sizes \\(\\{n_1^*, n_2^*, \\ldots, n_k^*\\}\\) becomes:\n\n9.9.1 Objective Function\nThe optimal sampling plan \\(\\{n_1^*, n_2^*, \\ldots, n_k^*\\}\\) for \\(\\bar{x}_{overall} = \\frac{1}{N} \\cdot \\sum_{j=1}^{k} N_j \\cdot \\bar{x}_j\\) can be analytically determined by minimizing the standard error of the global estimator (objective function):\n\\[\\min_{n_1, n_2, \\ldots, n_k} Var\\left(\\underbrace{\\frac{1}{N} \\cdot \\sum_{j=1}^{k} N_j \\cdot \\bar{x}_j}_{\\bar{x}_{global}}\\right) = \\sum_{j=1}^{k} \\left(N_j / N\\right)^2 \\cdot \\frac{\\sigma_j^2}{n_j}\\]\n\n\n9.9.2 Cost Constraint\nsubject to the cost constraint:\n\\[c_{total} \\equiv c_0 + \\sum_{j=1}^{k} c_j \\cdot n_j^*\\]\n\n\n9.9.3 Solution: Lagrange Multiplier Optimization\nThe solution to this optimization problem under constraints can be approximated with the Lagrange Multiplier Optimization technique:\n\\[n_j^* = (c_{total} - c_0) \\cdot \\frac{N_j \\cdot \\sigma_j / \\sqrt{c_j}}{\\sum_{j=1}^{k} N_j \\cdot \\sigma_j \\cdot \\sqrt{c_j}}\\]\nHowever, the \\(n_j^*\\) must be rounded to the closest integer value with \\(n_j^* \\geq 2\\).\nAnother optimization technique called Integer Programming would give exact results, but it does not provide an analytical solution.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#general-rules-for-sample-size-selection",
    "href": "w12_Chapter06Sampling.html#general-rules-for-sample-size-selection",
    "title": "Chapter 06: Sampling",
    "section": "9.10 General Rules for Sample Size Selection",
    "text": "9.10 General Rules for Sample Size Selection\nSelect a larger sample size \\(n_j^*\\) in strata \\(j\\):\n\n\\(n_j^* \\uparrow\\) if \\(N_j \\uparrow\\): if strata \\(j\\) consists of a larger proportion \\(N_j / N\\) of the population\n\\(n_j^* \\uparrow\\) if \\(\\sigma_j \\uparrow\\): if strata \\(j\\) is more heterogeneous (larger internal variance \\(\\sigma_j^2\\))\n\\(n_j^* \\uparrow\\) if \\(c_j \\downarrow\\): if it is less expensive to sample in stratum \\(j\\) (small \\(c_j\\))\n\nStratification can also be used to oversample otherwise underrepresented groups.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#definition-3",
    "href": "w12_Chapter06Sampling.html#definition-3",
    "title": "Chapter 06: Sampling",
    "section": "10.1 Definition",
    "text": "10.1 Definition\nDefinition: Cluster Random Sampling: In clustered random sampling the population is divided by convenience into mutually exclusive strata (i.e., clusters) in a two-steps procedure:\n[1] randomly a subset of clusters is picked.\n[2] a specific number of observations are sampled from within the selected clusters.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#heterogeneity-assumption",
    "href": "w12_Chapter06Sampling.html#heterogeneity-assumption",
    "title": "Chapter 06: Sampling",
    "section": "10.2 Heterogeneity Assumption",
    "text": "10.2 Heterogeneity Assumption\nOpposite to stratified sampling, clusters are supposed to be as heterogeneous as possible (strong mixing of attribute values) with regards to the attributes under investigation.\nThis means we expect that each cluster is representative of the whole population.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#overall-mean-and-variance-estimates",
    "href": "w12_Chapter06Sampling.html#overall-mean-and-variance-estimates",
    "title": "Chapter 06: Sampling",
    "section": "10.3 Overall Mean and Variance Estimates",
    "text": "10.3 Overall Mean and Variance Estimates\nThe overall mean and variance estimate in clustered sampling again become:\n\\[\\bar{x}_{overall} = \\frac{1}{N} \\cdot \\sum_{j=1}^{k} N_j \\cdot \\bar{x}_j\\]\n\\[s_{overall}^2 = \\frac{1}{N} \\cdot \\sum_{j=1}^{k} N_j \\cdot s_j^2 + \\frac{1}{N} \\cdot \\sum_{j=1}^{k} N_j \\cdot \\left(\\bar{x}_j - \\bar{x}_{overall}\\right)^2\\]\nTherefore, we need to know at least the cluster sizes \\(N_j\\) of the randomly selected clusters.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#advantages-and-risks",
    "href": "w12_Chapter06Sampling.html#advantages-and-risks",
    "title": "Chapter 06: Sampling",
    "section": "10.4 Advantages and Risks",
    "text": "10.4 Advantages and Risks\nClustered sampling can save sampling costs but bears the potential risks of high sampling error and sampling bias.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#spatial-point-sampling-approaches",
    "href": "w12_Chapter06Sampling.html#spatial-point-sampling-approaches",
    "title": "Chapter 06: Sampling",
    "section": "12.1 Spatial Point Sampling Approaches",
    "text": "12.1 Spatial Point Sampling Approaches\n\nFor a countable number of given spatial objects (such as a finite set of points) standard sampling procedures can be applied by randomly picking objects from an object’s list.\nFor spatially continuous surfaces random locations need to be generated:\n\n\n12.1.1 Generating Random Points from a Square Study Area\nHow to pick a random point from a square study area \\(\\mathbb{R}\\)?\nSelect the x-coordinate from a uniform distribution \\(X_i \\sim \\mathcal{U}(x_{min}, x_{max})\\) and the y-coordinate from \\(Y_i \\sim \\mathcal{U}(y_{min}, y_{max})\\), respectively.\n\nThe resulting local densities of the sample points are approximately uniform (i.e., constant).\nThis sampling procedure leads to complete spatial randomness.\nExample: Use complete spatial randomness to estimate areas (areal integrals) with the script EstimateAreaBySampling.r.\nFor non-rectangular study areas \\(\\mathbb{R}\\), points outside the study area are rejected and the procedure is repeated until the target sample size is obtained.\n\n\n\n12.1.2 Reference Frames for Spatial Sampling\nFor systematic, stratified, or clustered sampling the reference frame can either be:\n\nsquare raster cells, or\na hexagonal grid\n\n\n\n\nFIGURE 7-17: (a) A simple random point sample; (b) a systematic areal sample; (c) an areally stratified random sample; (d) a cluster sample; (e) a stratified, systematic, unaligned areal sample.\n\n\n\n\n\nFig. 6.2: Hexagonal points (left) and polygons (right)\n\n\n\n\n12.1.3 Properties of Hexagons vs Grid Cells\n\nHexagons have the advantage that the nearest neighbor points are all equidistant.\nThis is not the case for grid cells where diagonal cell centers are further apart than horizontal and vertical cell centers.\nProblem with systematic spatial sampling: If the objects under investigation are regularly spaced the systematic sampling procedures may skip over them.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w12_Chapter06Sampling.html#example-estimating-woodland-proportion",
    "href": "w12_Chapter06Sampling.html#example-estimating-woodland-proportion",
    "title": "Chapter 06: Sampling",
    "section": "14.1 Example: Estimating Woodland Proportion",
    "text": "14.1 Example: Estimating Woodland Proportion\n\n\n\nFIGURE 7-16: Estimating the proportion of woodland on a map by using traverses\n\n\n\\[\\text{Proportion of woodland} = \\frac{78}{200} = 0.39\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk12-ch6.Sampling"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html",
    "href": "w11_Chapter05A.html",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "",
    "text": "Part A covers basic probability theory (including appendix 5a) up to the concept of a random variable.\nPart B will review selected discrete and continuous distributions, the concepts of expectation and variance, and a brief overview over bivariate distributions.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definitions-elementary-outcomes-and-sample-space",
    "href": "w11_Chapter05A.html#definitions-elementary-outcomes-and-sample-space",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "2.1 Definitions: Elementary Outcomes and Sample Space",
    "text": "2.1 Definitions: Elementary Outcomes and Sample Space\n\nEach individual possible outcome \\(\\omega_i\\) (small omega) of an experiment is known as an elementary outcome, and the set of all possible elementary outcomes denotes the sample space \\(\\Omega = \\{\\omega_1, \\ldots, \\omega_n\\}\\) (capital Omega).\nElementary outcomes are disjunct, i.e., they do not overlap.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-event",
    "href": "w11_Chapter05A.html#definition-event",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "2.2 Definition: Event",
    "text": "2.2 Definition: Event\n\nAn event is a subset of the sample space, i.e., \\(A \\equiv \\{\\omega_2, \\omega_5\\}\\), with elementary outcomes \\(\\omega_2\\) and \\(\\omega_5\\) constituting the event \\(A\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-empty-set",
    "href": "w11_Chapter05A.html#definition-empty-set",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "2.3 Definition: Empty Set",
    "text": "2.3 Definition: Empty Set\n\nAn empty set does not contain any elementary outcomes and is denoted by \\(\\varnothing = \\{\\}\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-complementary-event-bara",
    "href": "w11_Chapter05A.html#definition-complementary-event-bara",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "2.4 Definition: Complementary Event \\(\\bar{A}\\)",
    "text": "2.4 Definition: Complementary Event \\(\\bar{A}\\)\n\nLet \\(\\Omega \\equiv \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5\\}\\) and event \\(A \\equiv \\{\\omega_2, \\omega_5\\}\\) then \\(\\bar{A} = \\Omega - A = \\{\\omega_1, \\omega_3, \\omega_4\\}\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-subset",
    "href": "w11_Chapter05A.html#definition-subset",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "2.5 Definition: Subset",
    "text": "2.5 Definition: Subset\n\nDefinition: a subset is a set whose members are also elements of another set.\n\nLet \\(A = \\{\\omega_1, \\omega_2\\}\\) and \\(B = \\{\\omega_1, \\omega_2, \\omega_3\\}\\) then \\(A \\subset B\\)\nAny event \\(A\\) is a subset of the sample space: \\(A \\subset \\Omega\\)\nA set \\(A\\) and its complement \\(\\bar{A}\\) can never be subsets \\(A \\not\\subset \\bar{A}\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-event-intersection",
    "href": "w11_Chapter05A.html#definition-event-intersection",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "2.6 Definition: Event Intersection",
    "text": "2.6 Definition: Event Intersection\n\nThe intersection symbol is \\(\\cap\\), which is also the logical AND. It means that both intersecting events must be satisfied simultaneously being members of their parent sets.\n\nLet \\(A = \\{\\omega_1, \\omega_2, \\omega_4\\}\\) and \\(B = \\{\\omega_1, \\omega_2, \\omega_3\\}\\) then \\(A \\cap B = \\{\\omega_1, \\omega_2\\}\\)\nIf \\(A \\cap B \\neq \\varnothing\\) then \\(A \\cap B \\subset A\\) and \\(A \\cap B \\subset B\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-mutually-exclusive-events",
    "href": "w11_Chapter05A.html#definition-mutually-exclusive-events",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "2.7 Definition: Mutually Exclusive Events",
    "text": "2.7 Definition: Mutually Exclusive Events\n\nThe intersection of mutually exclusive events is the empty set: \\(A \\cap B = \\varnothing\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-union",
    "href": "w11_Chapter05A.html#definition-union",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "2.8 Definition: Union",
    "text": "2.8 Definition: Union\n\nThe Union symbol is \\(\\cup\\), which is also the logical Either-OR. Thus, either event \\(A\\) or event \\(B\\) or both events happen together.\nLet \\(A = \\{\\omega_1, \\omega_2, \\omega_4\\}\\) and \\(B = \\{\\omega_1, \\omega_2, \\omega_3\\}\\) then \\(A \\cup B = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4\\}\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#venn-diagrams",
    "href": "w11_Chapter05A.html#venn-diagrams",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "2.9 Venn-Diagrams",
    "text": "2.9 Venn-Diagrams\nSome Venn-Diagrams highlighting the intersection, union and complement:\n\n\n\n\n\nBasic Venn diagrams showing set operations: union, intersection, complement, and De Morgan laws\n\n\n\n\n\n\n\n\n\nHalloween-themed Venn diagrams illustrating logical operations: OR, AND, XOR, NOR, NAND, XNOR",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#postulated-properties-of-probabilities-kolmogorovs-axioms",
    "href": "w11_Chapter05A.html#postulated-properties-of-probabilities-kolmogorovs-axioms",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "3.1 Postulated Properties of Probabilities (Kolmogorov’s axioms)",
    "text": "3.1 Postulated Properties of Probabilities (Kolmogorov’s axioms)\n(pp. 203-204)\n\n\\(0 \\leq \\Pr(\\omega_i) \\leq 1\\) for all \\(\\omega_i \\in \\Omega\\). In a deterministic world (no uncertainty) either \\(\\Pr(\\omega_i) = 0\\) or \\(\\Pr(\\omega_i) = 1\\).\n\\(\\Pr(A) = \\sum_{\\omega_i \\in A} \\Pr(\\omega_i)\\). This property requires that the elementary outcomes, which constitute the event \\(A\\), are mutually exclusive.\n\\(\\Pr(\\Omega) = 1\\) and \\(\\Pr(\\varnothing) = 0\\). This can be derived by deductive logic from the previous properties.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-of-probabilities",
    "href": "w11_Chapter05A.html#definition-of-probabilities",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "3.2 Definition of Probabilities",
    "text": "3.2 Definition of Probabilities\nProbabilities, as measure of uncertainties or likelihood of an experiment with many possible outcomes, can be obtained from several perspectives:\n\n3.2.1 [a] Analytical: a probability model based on counting rules\nThis perspective is usually based on the appealing assumption that all elementary have equal probability \\(\\Pr(\\omega_i) = \\frac{1}{n}\\).\nThe equal probability assumption leads to the classical definition:\n\\[\\Pr(A) = \\frac{|A|}{|\\Omega|}\\]\nwith \\(|\\cdot|\\) denoting the number of elements in a set.\nCriticism: [a] Circular definition because of the use of the definition of the probability in \\(\\Pr(\\omega_i) = \\frac{1}{n}\\); [b] the event and sample spaces need to be countable.\n\n\n3.2.2 [b] Relative Frequency\nProbabilities are obtained by repeating a random experiment under fixed conditions over a very large number of independent trials; e.g.,\n\\[\\Pr(success) = \\lim_{n \\to \\infty} \\left(\\frac{number\\ of\\ successes}{total\\ number\\ of\\ experiments}\\right)\\]\nLarger numbers of repetitions lead to more accurate estimates.\nCriticism: [a] Theoretically requires infinite number of trials. [b] Conditions underlying each trial cannot be held indefinitely constant. [c] Some experiments cannot be repeated indefinitely (e.g., lifespan of light bulbs in a production process)\nSee the script PROBFREQUENT.R:\n\n3.2.2.1 Changho’s experiment: 500 random tosses of a fair coin\n\n\n\n\n\nChangho’s experiment: 500 random tosses of a fair coin showing convergence to 0.5\n\n\n\n\n\n\n3.2.2.2 Instructor’s Experiment: 500 random tosses of a fair coin\n\n\n\n\n\nInstructor’s experiment: 500 random tosses of a fair coin showing convergence to 0.5\n\n\n\n\n\n\n\n3.2.3 [c] Subjective Probabilities\nA person assigns subjectively a probability to a possibly random event.\nThese subjective assessments may originate from personal experiences or “divine intuition”.\nFor experience-based probabilities one assumes, that under similar circumstances one would observe an event occurring with a particular frequency.\nCriticism: [a] The problem with subjective probabilities is that an external observer cannot reproduce the subjective probability (i.e., we cannot read people’s mind); [b] Subjective probabilities may not necessarily satisfy the Kolmogorov’s axioms.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-combination",
    "href": "w11_Chapter05A.html#definition-combination",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "5.1 Definition: Combination",
    "text": "5.1 Definition: Combination\nA set \\(C\\) of distinguishable objects regardless of their order.\nExample: \\(\\{a, b\\} = \\{b, a\\}\\). Both are just one event.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-permutations",
    "href": "w11_Chapter05A.html#definition-permutations",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "5.2 Definition: Permutations",
    "text": "5.2 Definition: Permutations\nA set \\(P\\) of distinguishable objects with distinct ordering (order is relevant here).\nExample: \\(\\{a, b\\} \\neq \\{b, a\\}\\). Both are separate events.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-product-rule-p-247",
    "href": "w11_Chapter05A.html#definition-product-rule-p-247",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "5.3 Definition: Product Rule (p 247)",
    "text": "5.3 Definition: Product Rule (p 247)\nSuppose there are \\(r\\) sets of different objects. Each set \\(i\\) has \\(n_i\\) objects. If we select one object from each set, then there are in total \\(\\prod_{i=1}^{r} n_i = n_1 \\cdot n_2 \\cdots n_r\\) distinct combinations.\nExample: we have \\(n\\) objects (set 1) and we sample one object. From the remaining \\(n-1\\) objects (now set 2) we sample again one object. Then there are \\(n \\cdot (n-1)\\) distinct combinations (i.e., pairs of events).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#sampling-schemes",
    "href": "w11_Chapter05A.html#sampling-schemes",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "5.4 Sampling Schemes",
    "text": "5.4 Sampling Schemes\nEvaluation: Sampling with or without replacement and with (permutation) or without (combination) considering the order of events.\n\n5.4.1 Potential permutations by sampling twice from the set \\(S = \\{A, B, C, D, E\\}\\) with replacement:\n\\[S \\times S = \\begin{pmatrix}\n\\{\\mathbf{A}, \\mathbf{A}\\} & \\{A, B\\} & \\{A, C\\} & \\{A, D\\} & \\{A, E\\} \\\\\n\\{B, A\\} & \\{\\mathbf{B}, \\mathbf{B}\\} & \\{B, C\\} & \\{B, D\\} & \\{B, E\\} \\\\\n\\{C, A\\} & \\{C, B\\} & \\{\\mathbf{C}, \\mathbf{C}\\} & \\{C, D\\} & \\{C, E\\} \\\\\n\\{D, A\\} & \\{D, B\\} & \\{D, C\\} & \\{\\mathbf{D}, \\mathbf{D}\\} & \\{D, E\\} \\\\\n\\{E, A\\} & \\{E, B\\} & \\{E, C\\} & \\{E, D\\} & \\{\\mathbf{E}, \\mathbf{E}\\}\n\\end{pmatrix}\\]\nThe symbol \\(\\times\\) denotes the Cartesian Product of two sets.\n\n\n5.4.2 Recall the definition of the factorial:\n\\[n! = 1 \\cdot 2 \\cdots n\\]\nwith \\(0! = 1\\) by convention.\n\n\n5.4.3 Classification scheme for sampling twice from \\(S = \\{A, B, C, D, E\\}\\):\n\\(n\\) expresses the number of different elements in the sample space and \\(p\\) is the size of sample (repeated samples from the set \\(S\\)).\n\nWith replacement and with considering order (i.e., permutation): \\(5 \\cdot 5 = 25\\)\nGeneral: \\(n^p\\)\nWithout replacement and with considering the order (i.e., permutation): \\(5 \\cdot 4 = 20\\)\nGeneral Permutations: \\[P_r^n = n \\cdot (n-1) \\cdots (n-r+1) = \\frac{n!}{(n-r)!}\\]\nWithout replacement and without considering order (i.e., combination): \\(5 \\cdot 4 / 2 = 10\\)\nGeneral Combinations: \\[C_r^n = P_r^n / r! = \\left[n \\cdot (n-1) \\cdots (n-r+1)\\right] / \\left[1 \\cdot 2 \\cdots r\\right] = \\frac{n!}{(n-r)! \\cdot r!} = \\binom{n}{r}\\]\nWith replacement and without considering order (i.e., combination): \\(6 \\cdot 5 / 2 = 15\\)\nGeneral: \\[\\binom{n+r-1}{r} = \\frac{(n+r-1)!}{(n-1)! \\cdot r!}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#hypergeometric-rule-p.-249",
    "href": "w11_Chapter05A.html#hypergeometric-rule-p.-249",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "5.5 Hypergeometric Rule (p. 249)",
    "text": "5.5 Hypergeometric Rule (p. 249)\nThe Hypergeometric Rule is a combination of the product rule and the combination rule.\nSuppose there are \\(p\\) sets of objects. Each set has \\(n_i\\) objects with the total number of objects being \\(\\sum_{i=1}^{p} n_i = n\\).\nFrom each set we select \\(r_i\\) with \\(r_i \\leq n_i\\) objects. The different number of possible combinations is:\n\\[\\binom{n_1}{r_1} \\cdot \\binom{n_2}{r_2} \\cdots \\binom{n_p}{r_p}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#cross-tabulation-of-events",
    "href": "w11_Chapter05A.html#cross-tabulation-of-events",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "6.1 Cross-tabulation of Events",
    "text": "6.1 Cross-tabulation of Events\nThe sample space \\(\\Omega = A \\times B = \\{A_1 \\cap B_1, A_1 \\cap B_2, A_1 \\cap B_3, A_2 \\cap B_1, A_2 \\cap B_2, A_2 \\cap B_3\\}\\) is derived by evaluating all pairwise combinations of events \\(A = \\{A_1, A_2\\}\\) with events \\(B = \\{B_1, B_2, B_3\\}\\).\nLet us arrange the probabilities of these events and their intersection in a cross-tabulation:\n\n\n\n\n\n\n\n\n\n\n\n\\(B_1\\)\n\\(B_2\\)\n\\(B_3\\)\n\\(\\sum\\)\n\n\n\n\n\\(A_1\\)\n\\(\\Pr(A_1 \\cap B_1)\\)\n\\(\\Pr(A_1 \\cap B_2)\\)\n\\(\\Pr(A_1 \\cap B_3)\\)\n\\(\\Pr(A_1)\\)\n\n\n\\(A_2\\)\n\\(\\Pr(A_2 \\cap B_1)\\)\n\\(\\Pr(A_2 \\cap B_2)\\)\n\\(\\Pr(A_2 \\cap B_3)\\)\n\\(\\Pr(A_2)\\)\n\n\n\\(\\sum\\)\n\\(\\Pr(B_1)\\)\n\\(\\Pr(B_2)\\)\n\\(\\Pr(B_3)\\)\n\\(\\Pr(\\Omega) = 1.0\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#marginal-probabilities",
    "href": "w11_Chapter05A.html#marginal-probabilities",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "6.2 Marginal Probabilities",
    "text": "6.2 Marginal Probabilities\nThe marginal probabilities of individual events \\(A_i\\) or \\(B_j\\) are:\n\\[\\Pr(A_i) = \\Pr(A_i \\cap B_1) + \\Pr(A_i \\cap B_2) + \\Pr(A_i \\cap B_3)\\]\nand\n\\[\\Pr(B_j) = \\Pr(A_1 \\cap B_j) + \\Pr(A_2 \\cap B_j)\\]\nWhy are we allowed to do this?\nAnswer: The intersections \\(\\Pr(A_i \\cap B_j)\\) are based mutually exclusive pairs of events.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-addition-theorem-p-207",
    "href": "w11_Chapter05A.html#definition-addition-theorem-p-207",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "6.3 Definition: Addition Theorem (p 207)",
    "text": "6.3 Definition: Addition Theorem (p 207)\nWarning: Look out for the intersection of events:\n\\[\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\]\nExample: \\(\\Pr(A_i \\cup B_j) = \\Pr(A_i) + \\Pr(B_j) - \\Pr(A_i \\cap B_j)\\)\nSpecial rule for mutually exclusive events, i.e, \\(A_i \\cap B_j = \\varnothing\\):\n\\[\\Pr(A_i \\cup B_j) = \\Pr(A_i) + \\Pr(B_j)\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-complementation-theorem-p.-208",
    "href": "w11_Chapter05A.html#definition-complementation-theorem-p.-208",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "6.4 Definition: Complementation Theorem (p. 208)",
    "text": "6.4 Definition: Complementation Theorem (p. 208)\n\\[\\Pr(\\bar{A}) = \\underbrace{1}_{=\\Pr(\\Omega)} - \\Pr(A)\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-conditional-probability-p-208",
    "href": "w11_Chapter05A.html#definition-conditional-probability-p-208",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "6.5 Definition: Conditional Probability (p 208)",
    "text": "6.5 Definition: Conditional Probability (p 208)\nThe probability of an event \\(A\\) may change once another event \\(B\\) has taken place ahead of it.\nThis allows predicting the probabilities of events with the knowledge of a conditioning event \\(B\\):\nGeneral rule:\n\\[\\Pr(A|B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}\\]\nHow to remember the rule: The event that conditions the other event (written after “|”) is in the denominator.\nNote: Some books use the mathematical notation \\(\\Pr(A, B) \\Leftrightarrow \\Pr(A \\cap B)\\) for the probability of the intersection of events.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-multiplication-theorem-p-209",
    "href": "w11_Chapter05A.html#definition-multiplication-theorem-p-209",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "6.6 Definition: Multiplication Theorem (p 209)",
    "text": "6.6 Definition: Multiplication Theorem (p 209)\nSimple algebraic transformation of the definition of the conditional probabilities gives:\n\\[\\Pr(A \\cap B) = \\Pr(A|B) \\cdot \\Pr(B)\\] \\[= \\Pr(B|A) \\cdot \\Pr(A)\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-statistical-independent-events-p-209",
    "href": "w11_Chapter05A.html#definition-statistical-independent-events-p-209",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "6.7 Definition: Statistical Independent Events (p 209)",
    "text": "6.7 Definition: Statistical Independent Events (p 209)\nUnder statistical independence, the occurrence of a conditioning event \\(B\\) does not change the probability for another event:\n\\[\\Pr(A) = \\Pr(A|B)\\]\nEquivalently using the definition of the conditional probability, we can write using the multiplication theorem of probabilities for independent events:\n\\[\\Pr(A \\cap B) = \\Pr(A) \\cdot \\Pr(B)\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#derivation",
    "href": "w11_Chapter05A.html#derivation",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "7.1 Derivation",
    "text": "7.1 Derivation\nFrom the multiplication theorem we have:\n\\[\\Pr(A \\cap B) = \\Pr(A|B) \\cdot \\Pr(B) \\quad \\text{or} \\quad \\Pr(A \\cap B) = \\Pr(B|A) \\cdot \\Pr(A)\\]\n\\[\\Leftrightarrow \\Pr(A|B) \\cdot \\Pr(B) = \\Pr(B|A) \\cdot \\Pr(A)\\]\n\\[\\Rightarrow \\Pr(A|B) = \\frac{\\Pr(B|A) \\cdot \\Pr(A)}{\\Pr(B)} \\quad \\text{or alternatively,} \\quad \\Pr(B|A) = \\frac{\\Pr(A|B) \\cdot \\Pr(B)}{\\Pr(A)}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#interpretation",
    "href": "w11_Chapter05A.html#interpretation",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "7.2 Interpretation",
    "text": "7.2 Interpretation\nFor the Bayesian equation:\n\\[\\Pr(B|A) = \\frac{\\Pr(A|B) \\cdot \\Pr(B)}{\\Pr(A)}\\]\n\nThe probability \\(\\Pr(B)\\) is called the a priori probability (first believe in the probability) of event \\(B\\)\n\\(\\Pr(B|A)\\) is called the posteriori probability (revised probability) of event \\(B\\) after we have observed event \\(A\\).\nThe probability \\(\\Pr(A|B)\\) is called the likelihood of event \\(A\\) assuming event \\(B\\) has taken place. The likelihood is assumed to be externally known.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#total-probability",
    "href": "w11_Chapter05A.html#total-probability",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "7.3 Total Probability",
    "text": "7.3 Total Probability\nThe total probability \\(\\Pr(A)\\) for event \\(A\\) can be calculated using marginal probability equation:\n\\[\\Pr(A) = \\sum_{j=1}^{r} \\Pr(A \\cap B_j) = \\sum_{j=1}^{r} \\Pr(A|B_j) \\cdot \\Pr(B_j)\\]\nassuming \\(\\Pr(A|B_j)\\) and \\(\\Pr(B_j)\\ \\forall j\\) are externally given.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#general-form-of-the-bayes-theorem",
    "href": "w11_Chapter05A.html#general-form-of-the-bayes-theorem",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "7.4 General Form of the Bayes’ Theorem",
    "text": "7.4 General Form of the Bayes’ Theorem\nThis gives the general form of the Bayes’ theorem:\n\\[\\Pr(B_j|A) = \\frac{\\Pr(A|B_j) \\cdot \\Pr(B_j)}{\\sum_{j=1}^{r} \\Pr(A|B_j) \\cdot \\Pr(B_j)} \\quad \\text{for all events } B_j\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#example-paleontologist-and-mosasaur",
    "href": "w11_Chapter05A.html#example-paleontologist-and-mosasaur",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "7.5 Example: Paleontologist and Mosasaur",
    "text": "7.5 Example: Paleontologist and Mosasaur\n\n\n\n\n\nMosasaur skeleton - the paleontologist wishes to find the complete skeleton\n\n\n\n\nA paleontologist found downstream of a river some Mosasaur fragment.\nHe/she wishes to conduct an expedition into either basin \\(B_1\\) (with the area of 18 acres) or basin \\(B_2\\) (with the area of 10 acres) from where these fragments may have originated.\nResearch question: Which basin maximizes the likelihood of finding the skeleton?\n\n7.5.1 Prior Probabilities\nThe prior probabilities of the basins are proportional to their size:\n\\[\\Pr(B_1) = 18/(18+10) = 0.64 \\quad \\text{and} \\quad \\Pr(B_2) = 10/(18+10) = 0.36\\]\nThe higher prior probabilities direct the paleontologist to basin \\(B_1\\).\n\n\n7.5.2 Likelihood Information\nSkeletons are only found in Cretaceous rock (event \\(A\\)). The proportions of Cretaceous rock in either basin is:\n\\[\\Pr(A|B_1) = 0.35 \\quad \\text{and} \\quad \\Pr(A|B_2) = 0.80\\]\n\n\n7.5.3 Schematic Diagram\n\n\n\n7.5.4 Total Probability\nThe total probability of Cretaceous rock in both basins is:\n\\[\\Pr(A) = \\Pr(A|B_1) \\cdot \\Pr(B_1) + \\Pr(A|B_2) \\cdot \\Pr(B_2) = 0.35 \\cdot 0.64 + 0.80 \\cdot 0.36 = 0.512\\]\n\n\n7.5.5 Posteriori Probabilities\n\\[\\Pr(B_1|A) = \\frac{\\Pr(A|B_1) \\cdot \\Pr(B_1)}{\\Pr(A)}\\] \\[= \\frac{0.35 \\cdot 0.64}{0.512} = 0.4375\\]\n\\[\\Pr(B_2|A) = \\frac{0.80 \\cdot 0.36}{0.512} = 0.5625\\]\n\n\n7.5.6 Conclusion\nThe posteriori probabilities direct the paleontologist towards searching in basin \\(B_2\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05A.html#definition-random-variable-p-210",
    "href": "w11_Chapter05A.html#definition-random-variable-p-210",
    "title": "Chapter 05 (Part A): Probabilities and Random Variables",
    "section": "8.1 Definition: Random Variable (p 210)",
    "text": "8.1 Definition: Random Variable (p 210)\n\n\n\n\n\nFigure 4.11: A random variable is a mapping of events to the real line\n\n\n\n\nThe function \\(X(\\cdot)\\) measures the numerical population characteristic. It is usually denoted in the statistical literature by a capital letter, e.g., \\(X\\).\nWhy are events translated into numerical values?\nAnswer: To perform calculations, it is easier to work with numbers rather than with descriptive characteristics of elements in sets.\nThe probability of an event \\(A\\) and its associated random variable \\(X(A)\\) must be identical:\n\\[\\Pr(A) = \\Pr(X(A))\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5A.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html",
    "href": "w09_Chapt04BivariateRegression.html",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "",
    "text": "Bivariate Regression analysis assumes a clear distinction between a cause and its resulting effect variable:\n\nOne variable is given exogenously. This variable is called the independent variable or cause and denoted by \\(X\\). In theory, the independent variable can be controlled, and its measurements can be repeated.\nThe response, endogenous, effect or dependent variable \\(Y\\) is linked through a linear function to the independent variable:\n\n\n\\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\varepsilon_i\\]\n(note deviation from BBR notation) where \\(\\varepsilon_i\\) is the random disturbance term. It captures the random variation \\(\\varepsilon_i = \\hat{y}_i - y_i\\) of \\(y_i\\) around its predicted value \\(\\hat{y}_i = \\beta_0 + \\beta_1 \\cdot x_i\\).\n\nRegression traces the conditional distribution of \\(Y\\) given any feasible value of \\(X = x_i\\).\n\n\n\nRegression traces the conditional distribution of \\(Y\\) given any feasible value of \\(X = x_i\\).\n\n\n\nHelicopter experiment data and scatterplot",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#data-example-students-helicopter-experiments",
    "href": "w09_Chapt04BivariateRegression.html#data-example-students-helicopter-experiments",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "",
    "text": "Regression traces the conditional distribution of \\(Y\\) given any feasible value of \\(X = x_i\\).\n\n\n\nHelicopter experiment data and scatterplot",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#population-model",
    "href": "w09_Chapt04BivariateRegression.html#population-model",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "2.1 Population Model",
    "text": "2.1 Population Model\nFor the \\(i^{th}\\) observation the population model is:\n\\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\varepsilon_i\\]\n\nNeither the parameters \\(\\beta_0, \\beta_1\\) nor the error term \\(\\varepsilon_i\\) are directly observable.\nThe parameter \\(\\beta_0\\) is called the intercept and the parameter \\(\\beta_1\\) is the slope.\nThe parameters \\(\\beta_0\\) and \\(\\beta_1\\) are constant for all observations. They denote a part of the model structure.\nThe disturbance \\(\\varepsilon_i\\) is directly associated with the \\(i^{th}\\) observation.\nThe disturbances share an underlying random distribution, which is also part of the model structure.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#sample-estimates",
    "href": "w09_Chapt04BivariateRegression.html#sample-estimates",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "2.2 Sample Estimates",
    "text": "2.2 Sample Estimates\nFor the sample estimates the predicted value of the model becomes:\n\\[\\hat{y}_i = b_0 + b_1 \\cdot x_i\\]\nwith the estimated residual:\n\\[e_i = y_i - \\hat{y}_i\\]\n\n2.2.1 Notation Notes\n\nSome books use \\(\\alpha\\) instead of \\(\\beta_0\\) for the population intercept and \\(a\\) instead of \\(b_0\\) for the estimated intercept (for instance, Burt and Barber).\nSome people also write \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the estimated parameters \\(b_0\\) and \\(b_1\\) as well as \\(\\hat{\\varepsilon}_i\\) for the residuals.\nBivariate regression has two estimated parameters \\(K = 2\\): one for the intercept \\(\\beta_0\\) and one for the slope \\(\\beta_1\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#intercept-and-slope",
    "href": "w09_Chapt04BivariateRegression.html#intercept-and-slope",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "3.1 Intercept and Slope",
    "text": "3.1 Intercept and Slope\n \n\nThe estimate \\(b_1\\) slope is interpreted as: If we change \\(X\\) by one unit, \\(Y\\) will change by \\(b_1\\) units.\nIf the estimated slope is virtually zero then the independent variable has no impact on the variability of the dependent variable.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#key-questions",
    "href": "w09_Chapt04BivariateRegression.html#key-questions",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "3.2 Key Questions",
    "text": "3.2 Key Questions\n\nWhat happens to the intercept if the slope equals zero?\nWhat happens to two regression lines if they only differ in their intercepts?\nWhat happens to two regression lines if they only differ in their slopes?\nCan we make certain statements outside the observed support (value range) of the variable \\(X\\)?\n\n Note: We can interpolate within data samples but cant extrapolate outside the samples or learned data.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#important-concepts",
    "href": "w09_Chapt04BivariateRegression.html#important-concepts",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "3.3 Important Concepts",
    "text": "3.3 Important Concepts\n\nThe regression line linearly summarizes the data by a straight line (uses only two parameters for the \\(n\\) observed data pairs \\((X_i, Y_i)\\) of observations \\(i\\)).\nLinearity needs to be approximately satisfied. Advanced topic: Some transformations allow achieving linearity.\nExtrapolations must be made with caution.\nParsimony Principle: reduce the number of data points by replacing them with simple but general rules (summarize the data points).\nLinear regression analysis separates:\n\nThe systematic component which is the conditional expectations of \\(y_i\\) (conditional on the observation \\(x_i\\)): \\(\\hat{y}_i = \\beta_0 + \\beta_1 \\cdot x_i\\)\nThe random component which are the disturbances \\(\\varepsilon_i\\)\n\nThe residuals are unique to each observation. They deviate from the general rule \\(\\beta_0 + \\beta_1 \\cdot x_i\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#notes-on-assumptions",
    "href": "w09_Chapt04BivariateRegression.html#notes-on-assumptions",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "4.1 Notes on Assumptions",
    "text": "4.1 Notes on Assumptions\n\nThe independence and identical distribution assumption of the disturbances is abbreviated by i.i.d. (independently identically distributed)\nOnly the disturbances are required to be normal i.i.d. Neither \\(Y\\) nor \\(X\\) need to follow necessarily a normal distribution.\nHowever, a joint normal distribution of \\(Y\\) and \\(X\\) is highly desirable to approach a linear relationship and a balanced distribution of all data points in the scatterplot. This linear relationship may be achieved by transformations of either the dependent and/or the independent variables.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#important-property-regression-line-through-means",
    "href": "w09_Chapt04BivariateRegression.html#important-property-regression-line-through-means",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "5.1 Important Property: Regression Line Through Means",
    "text": "5.1 Important Property: Regression Line Through Means\nThe first equation shows that the regression line must go through the means of the independent and the dependent variables \\((\\bar{Y}, \\bar{X})\\).\n\\[-\\sum y_i + nb_0 + b_1 \\sum x_{i1} \\equiv 0\\]\n\\[\\Rightarrow \\sum y_i = nb_0 + b_1 \\sum x_{i1} \\quad | \\div n\\]\n\\[\\Rightarrow \\bar{y} = b_0 + b_1 \\cdot \\bar{x}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#comparing-different-regression-lines",
    "href": "w09_Chapt04BivariateRegression.html#comparing-different-regression-lines",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "5.2 Comparing Different Regression Lines",
    "text": "5.2 Comparing Different Regression Lines\n \n \nThe line with the lowest RSS is the optimal regression line.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#questions-for-consideration",
    "href": "w09_Chapt04BivariateRegression.html#questions-for-consideration",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "5.3 Questions for Consideration",
    "text": "5.3 Questions for Consideration\n\nWhich optimality condition did the arithmetic mean satisfy?\nWhy do we focus on the sum of the squared differences \\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) rather than the sum of the simple differences \\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)\\)?\nWhat impact may outliers and extreme cases have on the estimated regression line?\nWhat happens if the best fitting regression line is the horizontal line?\nDo the estimated intercept and the estimated slope parameters covary?",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#important-properties-of-ols",
    "href": "w09_Chapt04BivariateRegression.html#important-properties-of-ols",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "5.4 Important Properties of OLS",
    "text": "5.4 Important Properties of OLS\n\nAs long as we have an intercept \\(b_0\\) in the model, the sum of the residuals is always zero:\n\n\\[\\sum_{i=1}^{n} (y_i - \\hat{y}_i) = \\sum_{i=1}^{n} e_i = 0\\]\n\nOne also can show that the estimated residuals are always uncorrelated with the independent variable as well as with the predicted value:\n\n\\[Corr(\\hat{y}, e) = Corr(x, e) = 0\\]\n\\(\\Rightarrow\\) Consequently, the independent variable in the regression model cannot be used to explain any additional variation in the regression residuals beyond its already explained variation. Consequently, its explanatory linear power is fully exhausted.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#important-notational-note",
    "href": "w09_Chapt04BivariateRegression.html#important-notational-note",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "5.5 Important Notational Note",
    "text": "5.5 Important Notational Note\nBurt, Barber and Rigby’s notation differs from the commonly used notation:\n\nBBR use RSS as “Regression Sum of Squares” and ESS means “Errors Sum of Squares”\nThe lecture sticks to the standard notation where RSS stands for “Residual Sum of Squares” and ESS stands for “Explained Sum of Squares”",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#decomposition-tss-ess-rss",
    "href": "w09_Chapt04BivariateRegression.html#decomposition-tss-ess-rss",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "5.6 Decomposition: TSS = ESS + RSS",
    "text": "5.6 Decomposition: TSS = ESS + RSS\nSince \\(y = \\hat{y} + e\\)\n\n\n\nVariance decomposition diagrams\n\n\nFIGURE 13-11. Decomposition of the total variation.\nFIGURE 13-12. Geometrical representation of (a) total variation, (b) residual variation, and (c) explained variation.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#definition-of-variance-terms",
    "href": "w09_Chapt04BivariateRegression.html#definition-of-variance-terms",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "5.7 Definition of Variance Terms",
    "text": "5.7 Definition of Variance Terms\nTotal Sum of Squares (TSS):\n\\[TSS = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\quad \\text{with } df = n - 1\\]\nResidual Sum of Squares (RSS):\n\\[RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\quad \\text{with } df = n - K\\]\nExplained Sum of Squares (ESS):\n\\[ESS = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 \\quad \\text{with } df = K - 1\\]\n\n\\(K = 2\\) is the number of the estimated regression coefficients \\(b_0\\) and \\(b_1\\).\nThe values TSS, RSS and ESS with their degrees of freedom can be found in the so-called regression ANOVA table of the standard regression output.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w09_Chapt04BivariateRegression.html#interpretation",
    "href": "w09_Chapt04BivariateRegression.html#interpretation",
    "title": "Chapter 04: Bivariate Regression Analysis",
    "section": "8.1 Interpretation",
    "text": "8.1 Interpretation\n\nThe estimated intercept \\(b_0\\) is 2.7 which means that with zero wing length it will take the helicopter to hit the floor in 2.7 seconds.\nThe wing length slope parameter \\(b_1\\) implies that with each additional centimeter of wing length the flight duration will increase on average by 0.47 seconds.\nThe stars inform us how robustly different (more precisely statistically significant) the estimated regression parameters \\(b_0\\) and \\(b_1\\) are different from zero.\nThe standard error of the residuals, i.e., the remaining variation, is 0.4941.\nThe \\(R^2\\) tells us that in total 43.7% of the variation in flight time is explained by the independent variable wing length.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk09-ch4.Bivariate Regression Analysis"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html",
    "href": "w08_Chapt04Relationships.html",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "",
    "text": "A clear temporal precedence of the cause before the effect is necessary for a causal relationship:\n\n\\[effect_t \\leftarrow cause_{t-1}\\]\n\nThe role of the dependent variable and the independent variable may switch depending on the perspective. E.g., the parent’s income influences the children’s education, but also a person’s education level influences her/his income potential:\n\n\\[income \\leftarrow education \\quad \\text{or} \\quad education \\leftarrow income\\]\nNote: the parent’s income precedes the child’s education and a person’s education precedes her/his income potential.\n\nCorrelation may be caused by plain random coincidences. Statistical significance tests will help quantify the potential impact of these random effects.\n\n\\[effect \\xleftarrow{random} cause\\]\n\nA lurking third variable, jointly influencing the two variables under investigation, may induce a relationship. This is known as spurious correlation or confounding effect.\n\n\\[\\begin{array}{c}\neffect \\leftarrow cause \\\\\n\\nwarrow \\quad \\nearrow \\\\\nconfounding\\ variable\n\\end{array}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#example-confounding-variable",
    "href": "w08_Chapt04Relationships.html#example-confounding-variable",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "4.1 Example: Confounding Variable",
    "text": "4.1 Example: Confounding Variable\nBirth rates and the density of storks in a rural German province over several decades in the last century:\n \n\n4.1.1 Birth Rate Analysis\n  \nBoth the stork density and the birth rate vary over time (i.e., decades). With time industrialization increases, this reduces the marshy habitat of storks, and it changes the reproductive behavior of the now increasingly urban population.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#example-induced-correlation",
    "href": "w08_Chapt04Relationships.html#example-induced-correlation",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "4.2 Example: Induced Correlation",
    "text": "4.2 Example: Induced Correlation\n  \nLeft: Absolute Population Counts by Sex | Right: %Males by %Females",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#example-spurious-correlation",
    "href": "w08_Chapt04Relationships.html#example-spurious-correlation",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "4.3 Example: Spurious Correlation",
    "text": "4.3 Example: Spurious Correlation\nThe variable “Size of Area” induces correlation. Normalization by the size of the area corrects for the absolute size effects.\n  \nLeft: Absolute numbers | Right: Relative Density Numbers",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#example-aggregation-effects",
    "href": "w08_Chapt04Relationships.html#example-aggregation-effects",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "4.4 Example: Aggregation Effects",
    "text": "4.4 Example: Aggregation Effects\nCorrelation of individual observations (e.g., individuals living within regions) visualized by dots versus correlation of aggregated observations (e.g., averaged individual data in regions) visualized by squares. =&gt; This is known as the modifiable areal unit problem (MAUP)\n\n\n\nAggregation effects on correlation\n\n\nIndividual Correlation: 0.457 vs Aggregate Correlation: 0.822",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#covariance",
    "href": "w08_Chapt04Relationships.html#covariance",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "5.1 Covariance",
    "text": "5.1 Covariance\nEmpirically the joint variation between two variables is expressed by their measure of covariance:\n\\[s_{x_1, x_2} = \\frac{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_1) \\cdot (x_{i2} - \\bar{x}_2)}{n - 1}\\]\nNote that the denominator remains \\((n - 1)\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#geometric-interpretation",
    "href": "w08_Chapt04Relationships.html#geometric-interpretation",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "5.2 Geometric Interpretation",
    "text": "5.2 Geometric Interpretation\nPoint pairs \\([x_1, x_2]\\) in quadrant defined by their variation \\([(x_{i1} - \\bar{x}_1), (x_{i2} - \\bar{x}_2)]\\) around their means \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\)\n\n\n\nLeft: Strong positive correlation \\(\\rho = 0.9\\) | Center: No correlation \\(\\rho = 0.0\\) | Right: Moderate negative correlation \\(\\rho = -0.4\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#standardization-to-correlation-coefficient",
    "href": "w08_Chapt04Relationships.html#standardization-to-correlation-coefficient",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "5.3 Standardization to Correlation Coefficient",
    "text": "5.3 Standardization to Correlation Coefficient\n\nThe covariance is not standardized to a comparable value range. It, therefore, becomes difficult to compare the covariance for different pairs of variables. This is because the scale of its parent variables \\(X_1\\) and \\(X_2\\) may differ.\nFor these reasons the covariance is standardized by the standard deviations \\(s_{x_1}\\) and \\(s_{x_2}\\) of its parent variables.\n\nThis gives the correlation coefficient:\n\\[r = \\frac{s_{x_1, x_2}}{s_{x_1} \\cdot s_{x_2}} = \\frac{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_1) \\cdot (x_{i2} - \\bar{x}_2)}{\\sqrt{\\sum_{i=1}^{n} (x_{i1} - \\bar{x}_1)^2 \\cdot \\sum_{i=1}^{n} (x_{i2} - \\bar{x}_2)^2}}\\]\n\n5.3.1 Notes\n\nThe value range of the correlation coefficient is restricted within the interval \\(r \\in [-1, 1]\\). Negative values measure a negative linear relationship in the data cloud, a value of zero implies no linear relationship and positive value implies a positive linear relationship.\nThe denominator \\(n - 1\\) in the covariance and both standard deviations denominators cancel out.\nBBR give an equivalent computational equation on p 168. One is definitional and the other is computational.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#rescue-vest-crosstabulation-overall",
    "href": "w08_Chapt04Relationships.html#rescue-vest-crosstabulation-overall",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "10.1 Rescue × Vest Crosstabulation (Overall)",
    "text": "10.1 Rescue × Vest Crosstabulation (Overall)\n\n\n\nRescue\nWearing Vest\nNo Vest\nTotal\n\n\n\n\nSurvived\nCount: 100\nCount: 71\n171\n\n\n\n% within Vest: 25.0%\n% within Vest: 71.0%\n34.2%\n\n\nDrowned\nCount: 300\nCount: 29\n329\n\n\n\n% within Vest: 75.0%\n% within Vest: 29.0%\n65.8%\n\n\nTotal\n400\n100\n500\n\n\n\n100.0%\n100.0%\n100.0%\n\n\n\nConclusions: Apparently wearing a vest is increasing the likelihood of drowning.\n\nThe supervisor felt extremely uncomfortable with these findings and asked the research assistant to investigate if weather conditions may influence the survival chances.\nThe partial results broken down by the weather conditions tell a totally different story, more in tune with common sense:",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#rescue-vest-weather-crosstabulation",
    "href": "w08_Chapt04Relationships.html#rescue-vest-weather-crosstabulation",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "10.2 Rescue × Vest × Weather Crosstabulation",
    "text": "10.2 Rescue × Vest × Weather Crosstabulation\n\n10.2.1 Fair Weather\n\n\n\nRescue\nWearing Vest\nNo Vest\nTotal\n\n\n\n\nSurvived\nCount: 19\nCount: 70\n89\n\n\n\n% within Vest: 95.0%\n% within Vest: 87.5%\n89.0%\n\n\nDrowned\nCount: 1\nCount: 10\n11\n\n\n\n% within Vest: 5.0%\n% within Vest: 12.5%\n11.0%\n\n\nTotal\n20\n80\n100\n\n\n\n\n\n10.2.2 Foul Weather\n\n\n\nRescue\nWearing Vest\nNo Vest\nTotal\n\n\n\n\nSurvived\nCount: 81\nCount: 1\n82\n\n\n\n% within Vest: 21.3%\n% within Vest: 5.0%\n20.5%\n\n\nDrowned\nCount: 299\nCount: 19\n318\n\n\n\n% within Vest: 78.7%\n% within Vest: 95.0%\n79.5%\n\n\nTotal\n380\n20\n400\n\n\n\nConclusions: Under both weather conditions wearing a vest increases the likelihood of survival.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w08_Chapt04Relationships.html#explanation",
    "href": "w08_Chapt04Relationships.html#explanation",
    "title": "Chapter 04: Bivariate Relationships",
    "section": "10.3 Explanation",
    "text": "10.3 Explanation\n\nIn foul weather, sailors have the habit of putting their life vests on, whereas in fair weather they mostly do not bother doing so.\nFurthermore, falling overboard in foul weather diminishes the likelihood of survival irrespectively of wearing a vest or not (see red numbers).\nApparently, the weather conditions are correlated with the status of wearing a vest and, therefore, the survival likelihood.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-ch4.Bivariate Relationships"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html",
    "href": "w06_DisplayingData.html",
    "title": "Chapter 01: Data Visualization",
    "section": "",
    "text": "Our brain is more accustomed to processing visual stimuli and to recognizing patterns in graphs than to finding patterns in tables with columns of numbers.\n\nExperiment: Try to describe the data either using the table or the graph. Which takes more effort?\nThe graphs organize data points in logical order onto a common scale and converts their numbers into information.\n\n\n\n2007 Sales Revenue - Table and Graph Comparison\n\n\n\n\n\n2007 Sales Revenue - Table and Graph Comparison\n\n\n\nNote: Common scale for both variables helps comparison.\n\n\n\n\nMinard’s depiction of the demise of Napoleon’s Grand Army in the Russian campaign June 23, 1812 to December 1812. An army of 422,000 left but only 10,000 returned.\n\n\n\nMinard’s Map of Napoleon’s Russian Campaign\n\n\nSee also the R script MinardTroops.R (generates separate plot-window). Note that the package HistData comprises of several historically interesting data sets.\n\n\n\n\n\nModern data graphics can do much more than simply substitute for small statistical tables. At their best, graphics are instruments for reasoning about quantitative information. Often the most effective way to describe, explore, and summarize a set of numbers – even a very large set – is to look at pictures of those numbers. Furthermore, of all methods for analyzing and communicating statistical information, well-designed data graphics are usually the simplest and at the same time the most powerful.\nFrom the Introduction to Edward R. Tufte, 1983. The Visual Display of Quantitative Information. Graphics Press\n\n\n\n\nIt is better to have a fuzzy answer to right questions than a precise answer to the wrong question.\nThe nice thing about being a statistician is that I can play in everybody’s backyard.\n\nNote: The progressing focus of GISciences from spatial data handling to geo-spatial data analytics.\nCaution: Sometimes our visual perception of patterns may be tricked by illusions happening in our brains: Face or vase?\n\n\n\nFace or Vase Optical Illusion\n\n\n\n\n\n\n\nStatic circles appear to rotate:\n\n\n\nRotating Circles Optical Illusion\n\n\nModern computer software supports the visual exploration of data and finding the most communicative way of presenting the information hidden within our data to gain meaningful knowledge about the underlying process that has generated our data.\n\n\n\n\n\nThe science of semiology (study of how signs and symbols are cognitively processed) has developed a grammar of graphics1. These rules have been implemented in R’s library ggplot2 for generating statistical graphs.\nColor can be a powerful graphical tool (see http://colorbrewer2.org/) if your output medium permits its use.\n\nFor choropleth map (coloring a set of regions) one distinguishes between:\n\n[a] categorical map themes\n[b] gradient continuous map themes\n[c] bi-polar or diverging map themes with a natural break point\n\n  \n\n\n\nFertility rate refers to the number of children born per woman. Approximately, for industrialized countries, 2.1 children per woman are needed to maintain a stable nationally population count.\nThe variable \\(\\log \\left(\\frac{TotalInFlow}{TotalOutFlow}\\right)\\) measures whether a province is gaining or losing population due to internal migration within the 95 provinces of Italy:\n\n\\[\\log \\left(\\frac{TotalInFlow}{TotalOutFlow}\\right) = \\begin{cases} &lt; 0 & \\text{population loss} \\\\ \\approx 0 & \\text{stable population} \\\\ &gt; 0 & \\text{population gain} \\end{cases}\\]\n\nSee the R script MapTX.R for example maps of the TX counties.\n\n\n\n\nNote: Approximately 12% of the population – males predominantly – are colorblind (mainly unable to distinguish between red and green).\n\n\n\nIshihara Test for Color Blindness\n\n\n\nNormal Color Vision: 25, 29, 45, 56, 6, 8\nRed-Green Color Blind: 25, spots, spots, 56, spots, spots",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#minards-famous-visualization",
    "href": "w06_DisplayingData.html#minards-famous-visualization",
    "title": "Chapter 01: Data Visualization",
    "section": "",
    "text": "Minard’s depiction of the demise of Napoleon’s Grand Army in the Russian campaign June 23, 1812 to December 1812. An army of 422,000 left but only 10,000 returned.\n\n\n\nMinard’s Map of Napoleon’s Russian Campaign\n\n\nSee also the R script MinardTroops.R (generates separate plot-window). Note that the package HistData comprises of several historically interesting data sets.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#the-role-of-graphics-in-information-processing",
    "href": "w06_DisplayingData.html#the-role-of-graphics-in-information-processing",
    "title": "Chapter 01: Data Visualization",
    "section": "",
    "text": "Modern data graphics can do much more than simply substitute for small statistical tables. At their best, graphics are instruments for reasoning about quantitative information. Often the most effective way to describe, explore, and summarize a set of numbers – even a very large set – is to look at pictures of those numbers. Furthermore, of all methods for analyzing and communicating statistical information, well-designed data graphics are usually the simplest and at the same time the most powerful.\nFrom the Introduction to Edward R. Tufte, 1983. The Visual Display of Quantitative Information. Graphics Press\n\n\n\n\nIt is better to have a fuzzy answer to right questions than a precise answer to the wrong question.\nThe nice thing about being a statistician is that I can play in everybody’s backyard.\n\nNote: The progressing focus of GISciences from spatial data handling to geo-spatial data analytics.\nCaution: Sometimes our visual perception of patterns may be tricked by illusions happening in our brains: Face or vase?\n\n\n\nFace or Vase Optical Illusion",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#visual-illusions",
    "href": "w06_DisplayingData.html#visual-illusions",
    "title": "Chapter 01: Data Visualization",
    "section": "",
    "text": "Static circles appear to rotate:\n\n\n\nRotating Circles Optical Illusion\n\n\nModern computer software supports the visual exploration of data and finding the most communicative way of presenting the information hidden within our data to gain meaningful knowledge about the underlying process that has generated our data.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#grammar-of-graphics-and-color",
    "href": "w06_DisplayingData.html#grammar-of-graphics-and-color",
    "title": "Chapter 01: Data Visualization",
    "section": "",
    "text": "The science of semiology (study of how signs and symbols are cognitively processed) has developed a grammar of graphics1. These rules have been implemented in R’s library ggplot2 for generating statistical graphs.\nColor can be a powerful graphical tool (see http://colorbrewer2.org/) if your output medium permits its use.\n\nFor choropleth map (coloring a set of regions) one distinguishes between:\n\n[a] categorical map themes\n[b] gradient continuous map themes\n[c] bi-polar or diverging map themes with a natural break point\n\n  \n\n\n\nFertility rate refers to the number of children born per woman. Approximately, for industrialized countries, 2.1 children per woman are needed to maintain a stable nationally population count.\nThe variable \\(\\log \\left(\\frac{TotalInFlow}{TotalOutFlow}\\right)\\) measures whether a province is gaining or losing population due to internal migration within the 95 provinces of Italy:\n\n\\[\\log \\left(\\frac{TotalInFlow}{TotalOutFlow}\\right) = \\begin{cases} &lt; 0 & \\text{population loss} \\\\ \\approx 0 & \\text{stable population} \\\\ &gt; 0 & \\text{population gain} \\end{cases}\\]\n\nSee the R script MapTX.R for example maps of the TX counties.\n\n\n\n\nNote: Approximately 12% of the population – males predominantly – are colorblind (mainly unable to distinguish between red and green).\n\n\n\nIshihara Test for Color Blindness\n\n\n\nNormal Color Vision: 25, 29, 45, 56, 6, 8\nRed-Green Color Blind: 25, spots, spots, 56, spots, spots",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#what-does-the-distribution-tell-us",
    "href": "w06_DisplayingData.html#what-does-the-distribution-tell-us",
    "title": "Chapter 01: Data Visualization",
    "section": "2.1 What Does the Distribution Tell Us?",
    "text": "2.1 What Does the Distribution Tell Us?\n\nReasonable value range.\nIt gives an indication of the most frequent values.\nIt gives an indication of the degree of spread and central location(s) of the distribution.\nIt points to unusual observations with respect to the remaining sample observation.\nSince histograms are based on the aggregation of observations within given intervals, information will be lost. Original data values within an interval are substituted by the interval midpoint.\nA frequency table is a numerical list which underlies a histogram.\n\n\n2.1.1 Example: Frequency Table\n&gt; freqTable(reactFreq)\n   startPoint endPoint midPoint freq\n1        0.50     0.75    0.625    7\n2        0.75     1.00    0.875   57\n3        1.00     1.25    1.125  132\n4        1.25     1.50    1.375   92\n5        1.50     1.75    1.625  125\n6        1.75     2.00    1.875   61\n7        2.00     2.25    2.125   50\n8        2.25     2.50    2.375   17\n9        2.50     2.75    2.625   25\n10       2.75     3.00    2.875   11\n11       3.00     3.25    3.125    5\n12       3.25     3.50    3.375    8\n13       3.50     3.75    3.625    1\n14       3.75     4.00    3.875    4\n15       4.00     4.25    4.125    3\n16       4.25     4.50    4.375    2\n17       4.50     4.75    4.625    0\n18       4.75     5.00    4.875    0\nReaction Time Density (in seconds)\n\n\n\nReaction Time Density (in seconds)\n\n\nNote: The histogram does not show the absolute frequencies but their proportional densities:\n\\[density_i = \\frac{frequency_i}{\\sum_{j=1}^{number\\ of\\ classes} frequency_j}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#construction-guidelines-for-class-intervals",
    "href": "w06_DisplayingData.html#construction-guidelines-for-class-intervals",
    "title": "Chapter 01: Data Visualization",
    "section": "2.2 Construction Guidelines for Class Intervals",
    "text": "2.2 Construction Guidelines for Class Intervals\n\nThe intervals cover the whole support of the data, they do not overlap, and they do not exhibit gaps.\nThe intervals are on the x-axis and the frequencies (either counts or relative frequencies) are on the y-axis.\nSelect the appropriate number of interval classes.\nSelection of inappropriate number of classes:\n\nIf the number of classes is too small it can mask critical characteristics of an underlying distribution.\nIf the number of classes is too large typical data clusters become diluted.\n\nRespect natural breaks, such as zero degrees Celsius or a ph-level of 7.\nIn general, the interval width needs to be constant. This allows proper comparison of the frequencies among classes.\n\n\n\n\nEffect of Number of Classes on Histogram\n\n\n\n2.2.1 Additional Guidelines\n\nDon’t make the error shown below (fortunately the histogram functions protect us from doing these mistakes):\n\n \n\nIn general, use the smallest and largest of the observation as lower and upper bound.\nIf there are a few far outlying observations it is advisable to plot first all data simultaneously and then zoom in a second graph into the bulk of the observations to gain a better visual resolution.\nIf more than one histogram of the same data (e.g., city A and city B) is plotted side-by-side use a common classification scale.\n\n\n\n\nZooming into Histogram",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#bubble-plots",
    "href": "w06_DisplayingData.html#bubble-plots",
    "title": "Chapter 01: Data Visualization",
    "section": "3.1 Bubble Plots",
    "text": "3.1 Bubble Plots\nFor bubble plots the circle area \\(area = \\pi \\cdot radius^2\\), for instance, the population count, needs to be proportional to radius. Thus:\n\\[radius = \\sqrt{\\frac{area}{\\pi}}\\]\nSee the R-script BUBBLEPLOT.R from the website http://www.flowingdata.com (visit the webpage for inspirations).\n\n\n\nComparison of Crime and Burglar Rates per 100,000 Inhabitants in 2005\n\n\n\n\n\nBubble Plot Example - Crime and Burglar Rates\n\n\n\n3.1.1 Avoid 3D Charts\nIf information can be displayed 2-dimensional, do so.\nThree-dimensional charts frequently distract from the underlying information in a graph:\n\n\n\n3D Chart Examples to Avoid",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#modality",
    "href": "w06_DisplayingData.html#modality",
    "title": "Chapter 01: Data Visualization",
    "section": "5.1 Modality",
    "text": "5.1 Modality\nThe number of meaningful clusters of observations is described by the term modality:\n\nUni-modality refers to just one peak\nBi-modality refers to two outstanding peaks. It may hint at a heterogeneous population.\nMultimodality refers to more than two outstanding peaks.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#example-order-function-in-r",
    "href": "w06_DisplayingData.html#example-order-function-in-r",
    "title": "Chapter 01: Data Visualization",
    "section": "6.1 Example: order() Function in R",
    "text": "6.1 Example: order() Function in R\nSee R-script QuantileDemo.r:\n&gt; (data.frame(x, idx, x[idx]))\n    x idx x.idx.\n1  4.0   4    2.5\n2  4.4   5    3.1\n3  3.8   3    3.8\n4  2.5   9    3.9\n5  3.1   1    4.0\n6  4.3  12    4.1\n7  5.1   6    4.3\n8  4.6   2    4.4\n9  3.9  11    4.4\n10 4.8   8    4.6\n11 4.4  10    4.8\n12 4.1   7    5.1",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#percentile-calculation",
    "href": "w06_DisplayingData.html#percentile-calculation",
    "title": "Chapter 01: Data Visualization",
    "section": "6.2 Percentile Calculation",
    "text": "6.2 Percentile Calculation\nFor a given data value \\(x_{[i]}\\) the percentile measures the proportion of sample observations less or equal to \\(x_{[i]}\\). The rank \\(i\\) and the number of observations \\(n\\) are used for their calculation.\nIn its general form a percentile is calculated as:\n\\[p(x_{[i]}) = \\frac{i - \\alpha}{n + (1 - \\alpha) - \\alpha}\\]\nwhere \\(x_{[i]}\\) is the \\(i\\)-th sorted data value with \\(0 \\leq \\alpha \\leq 1\\).\n\n6.2.1 Special Cases for \\(\\alpha\\)\n\nA value of \\(\\alpha = 0.0\\) gives \\(p(x_{[i]}) = \\frac{i}{n+1}\\) with \\(p \\in \\left[\\frac{1}{n+1}, \\frac{n}{n+1}\\right]\\).\nA value of \\(\\alpha = 0.5\\) gives \\(p(x_{[i]}) = \\frac{i - 0.5}{n}\\) with \\(p \\in \\left[\\frac{0.5}{n}, \\frac{n-0.5}{n}\\right]\\).\nA value of \\(\\alpha = 1.0\\) gives \\(p(x_{[i]}) = \\frac{i - 1}{n - 1}\\) with \\(p \\in [0, 1]\\).\n\n\n\n6.2.2 Adjustment Coefficient \\(\\alpha\\)\nThis adjustment coefficient \\(\\alpha\\) links the percentile back to an underlying hypothetical distribution:\nJustification for \\(\\alpha &lt; 1\\): if additional sample observations become available, then it would be possible to observe data values that are smaller or larger than the most extreme observations \\(x_{[1]}\\) and \\(x_{[n]}\\) in the current sample.\n\nThe R function ppoints(n, a) calculates the percentiles. Try the function for \\(\\alpha \\in \\{0, 0.5, 1\\}\\) to understand its behavior.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#quantiles",
    "href": "w06_DisplayingData.html#quantiles",
    "title": "Chapter 01: Data Visualization",
    "section": "6.3 Quantiles",
    "text": "6.3 Quantiles\nA quantile is that data value \\(x_{[i]}\\) of a distribution, which is associated with a particular percentile point.\n\nFor instance, if \\(\\alpha = 1\\) has been selected for the percentiles, then \\(x_{[i]} = q(p(x_{[i]}))\\).\nFor any other percentile not obtained in the dataset (a gap value), the quantile is linearly interpolated by:\n\n\\[q(p) = \\gamma \\cdot x_{[i]} + (1 - \\gamma) \\cdot x_{[i+1]}\\]\nwith \\(p(x_{[i]}) \\leq p \\leq p(x_{[i+1]})\\) and \\(0 \\leq \\gamma \\leq 1\\).\n\nSpecific distributions (normal, uniform, skewed) have characteristic quantile plots.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#important-quantiles",
    "href": "w06_DisplayingData.html#important-quantiles",
    "title": "Chapter 01: Data Visualization",
    "section": "6.4 Important Quantiles",
    "text": "6.4 Important Quantiles\n\n0.25 quantile also called \\(Q_1\\) quartile (25 percent of the observations are smaller or equal to this quantile value)\n0.50 quantile also called the median (50 percent of the observations are smaller or larger than the given quantile value)\n0.75 quantile also called \\(Q_3\\) quartile (75 percent of the observations are smaller or equal to this quantile value and 25 percent of the observations are larger than this value)\nA measure of spread is the inter-quartile range: \\(IQR = Q_3 - Q_1\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#construction-of-the-box-plot",
    "href": "w06_DisplayingData.html#construction-of-the-box-plot",
    "title": "Chapter 01: Data Visualization",
    "section": "7.1 Construction of the Box-Plot",
    "text": "7.1 Construction of the Box-Plot\n\nDraw a box from \\(Q_1\\) to \\(Q_3\\). Mark the median \\(Q_2\\) in the center of the box with a line.\nDefinition of adjacent values:\n\n\\[x_{low}^{adj} = \\min\\left(x_{[i]} \\in (Q_1, Q_1 - 1.5 \\cdot IQR) \\text{ and } x_{[i]} \\text{ in dataset}\\right)\\]\nand\n\\[x_{high}^{adj} = \\max\\left(x_{[i]} \\in (Q_3, Q_3 + 1.5 \\cdot IQR) \\text{ and } x_{[i]} \\text{ in dataset}\\right)\\]\nThe term \\(x \\in (a, b)\\) means all \\(x\\)-values in the interval between \\(a\\) and \\(b\\).\n\nDraw the “fences” so they just include the smallest and largest data values \\(x_{low}^{adj}\\) and \\(x_{high}^{adj}\\), respectively.\nOutliers are in the interval \\([1.5 \\cdot IQR, 3.0 \\cdot IQR]\\) starting from \\(Q_1\\) below or \\(Q_3\\) above, respectively.\nSevere outliers are beyond that range \\((&gt; 3.0 \\cdot IQR)\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#comparison-of-the-histogram-with-the-box-plot",
    "href": "w06_DisplayingData.html#comparison-of-the-histogram-with-the-box-plot",
    "title": "Chapter 01: Data Visualization",
    "section": "7.2 Comparison of the Histogram with the Box-Plot",
    "text": "7.2 Comparison of the Histogram with the Box-Plot\n\n\n\nBox Plot Compared to Histogram and Density Curve\n\n\nFigure 6-16. Box plot compared to histogram and density curve\n\nThe box-plot is not able to show multimodal distributions.\nThe box-plot highlights the tails of a distribution better (in particular for skewed distributions and distributions with outliers).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#use-of-box-plots",
    "href": "w06_DisplayingData.html#use-of-box-plots",
    "title": "Chapter 01: Data Visualization",
    "section": "7.3 Use of Box-Plots",
    "text": "7.3 Use of Box-Plots\n\nEasy visual description of the distribution of a variable and potential outliers\nComparison of distributions for several variables side-by-side.\n\n\n\n\nWater Consumption Box Plots by Year\n\n\nSee the R-script Boxplots.r",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w06_DisplayingData.html#footnotes",
    "href": "w06_DisplayingData.html#footnotes",
    "title": "Chapter 01: Data Visualization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Leland Wilkinson, 2005. The Grammar of Graphics. 2nd edition, Springer Verlag.↩︎",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk06-ch2.Data Visualization Data"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html",
    "href": "w00_IntroToScriptsAndData.html",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "",
    "text": "An excellent website outlining data operations and basic data analysis with R can be found at http://www.statmethods.net. It makes frequent references to the book by Kabacoff.\nExplore the R-Project website: https://www.r-project.org/\nExplore RStudio’s website: https://www.rstudio.com/",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#file-extensions",
    "href": "w00_IntroToScriptsAndData.html#file-extensions",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "3.1 File Extensions",
    "text": "3.1 File Extensions\nConcept of environment, history file and script. The associate file extensions are:\n\n*.RData or *.rda: Copy of the environment with all its data objects and custom user functions.\n*.Rhistory contains all commands issued during a session at the command prompt &gt;.\n*.R is a file that contains scripts, which can be a set of basic R data analysis commands, individual functions, or an elaborate program.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#working-directory",
    "href": "w00_IntroToScriptsAndData.html#working-directory",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "3.2 Working Directory",
    "text": "3.2 Working Directory\nGet and change the working directory where your scripts and workspace are stored and searched by default.\nEnter the command getwd() into the editor window and select “Run” from the toolbar. Your current working directory is displayed in the Console below.\nEach command in R ends with parentheses, e.g., getwd(), which may include optional parameters. Even if no options are specified a function always ends with parentheses ( ).\n\n3.2.1 Comments:\n\nThe returned string, e.g., \"C:/Users/Michael/Documents\", is the path to the working directory. In R strings are always enclosed by quotation marks, i.e., \"…\".\nThe Windows convention is to separate sub-directories by a backward slash \\.\nHowever, R uses the forward slash / or alternatively a double backward slash \\\\.\nIn R the single backslash \\ is reserved to start an escape character, for instance, \\n becomes a line break and carriage return in text output, e.g., &gt; cat(\"First line\\n Second line\") is shown in the Console on two lines.\nThe character &gt; in the Console window is the command prompt which indicates that R is ready to receive new commands. It shows up when R completed executing a script.\nThe ESC key or pressing the stop icon in the Console window – available while a script is executing – can terminate the ongoing execution of a script.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#receiving-help",
    "href": "w00_IntroToScriptsAndData.html#receiving-help",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "3.3 Receiving Help",
    "text": "3.3 Receiving Help\nReceiving help at the command prompt &gt; in the Console:\n\n&gt; help(\"FunctionName\") or short &gt; ?FunctionName\nThis searches all active libraries (currently linked to your session) for the help on this function.\nOr through the HTML-help menu system:\n\n\n\n\nR Help System\n\n\n\nFuzzy help can be obtained with the double question mark &gt; ??PartName or help.search(\"PartName\"). This searches all installed libraries for the help on functions, data or vignettes containing the string “PartName”.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#make-your-working-directory",
    "href": "w00_IntroToScriptsAndData.html#make-your-working-directory",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "4.1 Make your Working Directory",
    "text": "4.1 Make your Working Directory\n\nMy suggestion is that you setup a specific directory on your hard- or jump-drive for this course, e.g., a dedicated folder.\nYou store your scripts, data etc. in this directory or its subdirectories.\n\n\n\n\nSource Button\n\n\n\nThen set your working directory to this location with the command:\n&gt; setwd(\"E:\\\\Fall2024\\\\GettingStartedWithR\")\nNotice the double backward slash \\\\ here as substitute for the single forward slash / and the use of the quotation \"…\" to enclose the directory string.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#interacting-with-the-r-console",
    "href": "w00_IntroToScriptsAndData.html#interacting-with-the-r-console",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "4.2 Interacting with the R-Console",
    "text": "4.2 Interacting with the R-Console\n\nCollections of editor commands (or programs) can be stored in external *.R script-files (see File menu)\nSingle commands in the script editor can be run from the command prompt or with Run for a highlighted line or set of highlighted lines in a script.\nTo run a script (all lines in your editor) use the Source button.\n\n\n\n\nSource Button\n\n\n\nIn the Console window the arrow Up and Down keys scrolls through the History of previously issued R commands.\nIn the Console window previously issued commands can be edited by moving with the arrow keys the edit prompt to a particular location of your command line.\nWhile being in the Console window, commands with their list of parameters can be broken over several lines. Then the continuing prompt + will be displayed automatically at the beginning of a new line until the command is completed.\nTo clean a cluttered Console window use the key combination Ctrl-L or use the broom icon.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#variable-names",
    "href": "w00_IntroToScriptsAndData.html#variable-names",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "5.1 Variable Names",
    "text": "5.1 Variable Names\n\nNames must begin with a letter and can consist of an alpha/numeric combination of letters including the period . and/or the underscore _\nNote: Specific characters and keywords such $ @ & and % or + - * / and ^ cannot be used because they have special meanings. For a full list see &gt; ?Reserved\nWarning: Variable and function names in R are case sensitive, e.g., my.Var and my.var are different data objects (⇒ characters in wrong cases are extremely difficult to spot in a script)\nTip: Name variables properly so an external reader or you, after a few weeks have passed, can understand what you were doing.\nUse the dot to structure associated variable names, e.g., sales.plano and sales.dallas, or the camelback convention salesPlano and salesDallas\nJust for experts: The document google-r-style.pdf suggests professional naming and typesetting conventions of your R code.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#managing-objects-in-environment",
    "href": "w00_IntroToScriptsAndData.html#managing-objects-in-environment",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "5.2 Managing Objects in Environment",
    "text": "5.2 Managing Objects in Environment\n\nAny function or data structure that is defined during a R-session becomes an object in the Environment\nThese can be removed from the Environment with the remove function:\n&gt; rm(ipold)\nTo clean everything from the environment use the nested commands &gt; rm(list=ls()) or the broom icon in the Environment menu bar.\nWarning: if you happen to name a variable or function identically to an existing R object, which already exists in the search path of your session, that object will be masked and is no longer directly accessible:\n&gt; pi                # gives the system constant 3.141593\n&gt; pi &lt;- 2.71        # this masks the system constant pi\n&gt; base::pi          # pi still be found in its library base. Note the \"::\"\n&gt; rm(pi)            # removes user's pi and makes the constant pi available",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#hard-wired-values-in-r",
    "href": "w00_IntroToScriptsAndData.html#hard-wired-values-in-r",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "5.3 Hard-wired Values in R",
    "text": "5.3 Hard-wired Values in R\n\nLogical values T and F (alternatively TRUE and FALSE can be used)\nAn object without content has the storage value NULL\nImpossible operations, such as log(-1), lead to a not-a-number NaN.\nMissing values have the value NA, which stands for not available.\nSome predefined numbers are infinity Inf and pi (that is, \\(\\pi = 3.141593\\))",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#data-sets",
    "href": "w00_IntroToScriptsAndData.html#data-sets",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "6.1 Data-Sets",
    "text": "6.1 Data-Sets\n\nFor statistical analyses data-sets are usually arranged in rectangular data-frames and imported from external files (such as SPSS, STATA, EXCEL or DBASE) or embedded in workspaces (with the extension *.RData) of libraries.\nFor instance, to read an SPSS file use:\n&gt; library(foreign)  # makes import functions available during a session\n&gt; setwd(\"E:\\\\Lectures2022\\\\WorkingWithR\")\n&gt; MyPower &lt;- read.spss(\"DallasTempPower.sav\", to.data.frame=T)\nor more compactly skipping the attachment of library foreign:\n&gt; MyPower &lt;- foreign::read.spss(\"DallasTempPower.sav\", to.data.frame=T)\nSome data file types can also be imported using File ► Import Dataset or with the “Import Dataset” button in the Environment window. Note, however, using the menu violates the reproducible paradigm. Furthermore, the data are imported as a tibble and not a data.frame which can lead to problems.\nNotice that the object MyPower is added as data-frame to your Environment.\nTo calculate a new variable and assign it to the data-frame use the syntax df$NewVar, e.g.\n&gt; MyPower$DiffTemp &lt;- MyPower$MaxTemp - MyPower$MinTemp\nCheck the class of an object use:\n&gt; class(MyPower)\n[1] \"data.frame\"\nTo preview the data-frame double click on it in the Environment.\nEach column in a data-frame has an associated elementary data type.\nSome R libraries also include their own data-frame. These can be opened by the command:\n&gt; data(\"CPS1985\", package=\"AER\")",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#elementary-data-types-mode",
    "href": "w00_IntroToScriptsAndData.html#elementary-data-types-mode",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "6.2 Elementary Data Types (mode)",
    "text": "6.2 Elementary Data Types (mode)\nWithin a data-frame the variables can be of any elementary R data type (also called mode):\n\n6.2.1 logical\nFALSE or TRUE (also binary 0 or 1 and abbreviated F or T), e.g.:\ndo.I.make.sense &lt;- TRUE\n\n\n6.2.2 character strings\nAlways enclosed by single or double quotation marks, e.g.:\nmy.name &lt;- \"Michael\"\n\n\n6.2.3 factors\nInternally each value is stored with a specific integer number. Each integer has a descriptive label assigned to it, which is displayed.\nNotice: factor level values are not enclosed within quotation marks. For instance:\n&gt; MyPower$Month\n[1] JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV DEC \n[13] JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV DEC \n[25] JAN FEB MAR APR MAY JUN JUL AUG\nLevels: JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV DEC\nIndividual factor levels group the observations according to their specific factor level.\nThe syntax MyPower$Month addresses the factor variable Month in the data-frame MyPower.\n\n\n6.2.4 numeric\nEither real integer, double precision, or complex numbers (not used in this course).\n\nTrue integer values can be enforced with the added symbol L:\n&gt; two &lt;- 2L\n&gt; typeof(two)\n[1] \"integer\"\nIntegers, under specific circumstance, are treated as real numbers or may be coerced to real numbers:\n&gt; two.sq &lt;- two * 2.0\n&gt; typeof(two.sq)\n[1] \"double\"\nVery small remainders after internal rounding operations are interpreted as zeros, such as:\n&gt; (sqrt(2))^2 - 2\n[1] 4.440892e-16\nThis discrepancy is due to rounding errors associated with floating point computations, i.e. taking the square root and squaring the remaining results. This problem applies to all software environments, operating systems and computer chips dealing with floating point numbers.\nFloating point numbers with an infinite number of digits can only be digitally approximated up to a given depth due to the limited number of bits implemented within operating systems.\nE.g., while the constant \\(\\pi = 3.14\\ldots\\) has an infinite number of digits, only the first 16 can be numerically represented:\n&gt; options(digits=20)\n&gt; pi\n[1] 3.1415926535897931\n\n\n\n6.2.5 Dates\nDates come in different formats. This course will not cover dates.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#basic-data-objects-in-r",
    "href": "w00_IntroToScriptsAndData.html#basic-data-objects-in-r",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "6.3 Basic Data Objects in R",
    "text": "6.3 Basic Data Objects in R\n\n6.3.1 scalar\nAn individual datum. All data objects are composed of a collection of scalars.\n\n\n6.3.2 vector\nAtomic data structure that collects more than one value of identical mode. Example:\n&gt; score &lt;- c(23,53,45,30,53,60)\nor\n&gt; catName &lt;- c(\"Austin\",\"Gretchen\",\"Charlie\")\nwhere the function c() concatenates – combines – several scalars into a vector.\n&gt; length(score)  # gives the number of elements in the vector\nIndividual elements of a vector can be addressed by indices, e.g. score[2], score[3:5], score[-2], or leading to an outside range error score[99].\nNote: if a printed vector stretches over several rows in the console the position of the first value in each row will be numbered by [elementNumber]\nMixture of scalars with different data types will be coerced to the lowest level, i.e., characters:\n&gt; mixture &lt;- c(1,2,3,T,pi,\"A\")\n&gt; mixture\n[1] \"1\" \"2\" \"3\" \"TRUE\" \"3.14159265358979\" \"A\"\n\n\n6.3.3 matrix\nTwo-dimensional arrangement of a set of vectors that are all of the same mode and length.\n\n\n6.3.4 data frames\nCollection of vectors of the same length but perhaps different mode per column.\n\n\n6.3.5 list\nCollection of vectors of any type and length.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#inspecting-data-objects",
    "href": "w00_IntroToScriptsAndData.html#inspecting-data-objects",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "6.4 Inspecting Data Objects",
    "text": "6.4 Inspecting Data Objects\nTo obtain information about a data object and to look at its structure use the function str():\n&gt; str(MyPower)\n'data.frame':   32 obs. of  8 variables:\n $ SeqID   : num  1 2 3 4 5 6 7 8 9 10 ...\n $ Year    : num  2009 2009 2009 2009 2009 ...\n $ Month   : Factor w/ 12 levels \"JAN\",\"FEB\",\"MAR\",...: 1 2 3 4 5 6 7...\n $ DaysBill: num  34 29 30 32 29 30 32 29 30 31 ...\n - attr(*, \"codepage\")= int 1252",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#analysis-results-and-plots",
    "href": "w00_IntroToScriptsAndData.html#analysis-results-and-plots",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "8.1 Analysis Results and Plots",
    "text": "8.1 Analysis Results and Plots\nThe easiest way of saving [a] Console output and [b] generated plots in the graphics window are to copy and paste them into a graphically enhanced text editor such as Word.\nImportant: Any text output needs to be typeset in a fix pitch font such as Courier New. Otherwise the aligned column formatting of the output will be lost. Perhaps also reduce the font size and line-spacing of imported output as well as switch to a landscape layout for long lines of output.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#data",
    "href": "w00_IntroToScriptsAndData.html#data",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "8.2 Data",
    "text": "8.2 Data\nThe collection of all variables, data-frames and functions, which were created during a session, can be saved for subsequent sessions in a workspace.\nBefore saving the workspace non-desired data-objects, variables and functions should be dropped with the remove command:\n&gt; rm(objectName)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#exporting-data",
    "href": "w00_IntroToScriptsAndData.html#exporting-data",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "8.3 Exporting Data",
    "text": "8.3 Exporting Data\nData-frames within the workspace can also be exported into different file formats (see the package foreign or connections to SQL servers).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#command-history",
    "href": "w00_IntroToScriptsAndData.html#command-history",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "8.4 Command History",
    "text": "8.4 Command History\nAll commands, which were issued during session, can be saved into a history file.\n\n\n\nHistory Panel",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#exercise",
    "href": "w00_IntroToScriptsAndData.html#exercise",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "9.1 Exercise",
    "text": "9.1 Exercise\n&gt; x &lt;- seq(0.1, 2, by=0.1)  # sequence of numbers: 0.1, 0.2, …, 2\n&gt; x                          # show numbers\n&gt; y &lt;- log(x)               # calculate the natural logarithm\n&gt; plot(x, y)                # Plot y against x\n&gt; plot(y ~ x)               # same plot conceiving y as a function of x\n&gt; help(plot)                # Explore options\n&gt; plot(x, y, type=\"l\")      # Connect points by lines",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#plots-window",
    "href": "w00_IntroToScriptsAndData.html#plots-window",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "9.2 Plots Window",
    "text": "9.2 Plots Window\nSelect in the Plots window scroll through your list of plots with the arrow icons:\n\n\n\nPlot Window with Logarithm Function",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#exercise-continued",
    "href": "w00_IntroToScriptsAndData.html#exercise-continued",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "9.3 Exercise (continued)",
    "text": "9.3 Exercise (continued)\n&gt; z &lt;- rnorm(length(x))     # vector of standard normal random numbers\n&gt; mat &lt;- cbind(x, y, z)     # merge vectors of same length into a matrix\n&gt; dim(mat)                  # see the dimensions of the matrix\n&gt; summary(mat)              # get statistics of each matrix column\n&gt; class(mat)                # evaluate object type\n[1] \"matrix\"\n&gt; df &lt;- data.frame(x=x, xlog=y, rand=z)  # build dataframe\n&gt; class(df)\n[1] \"data.frame\"",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "w00_IntroToScriptsAndData.html#data-frames-1",
    "href": "w00_IntroToScriptsAndData.html#data-frames-1",
    "title": "Introduction to Scripts in the RStudio-Environment",
    "section": "10.1 Data-Frames",
    "text": "10.1 Data-Frames\n\nData-frames can pool several vectors of same length but potentially of different data-types together.\nAlmost all statistical analysis functions are defined on data-frames.\n\n&gt; catName &lt;- c(\"Austin\", \"Gretchen\", \"Charlie\")  # character vector\n&gt; catAge &lt;- c(9, 10, 20)                         # numeric vector\n&gt; my.data &lt;- data.frame(Name=catName, Age=catAge)  # Define data-frame\n&gt; my.data                                        # Show data-frame\n     Name Age\n1  Austin   9\n2 Gretchen  10\n3 Charlie  20\n\nThe variables catName and catAge are stored now in the data frame my.data under new names Name and Age\nTo access individual variables in the data-frame several commands can be used:\n\n&gt; my.data$Name\n&gt; my.data[\"Name\"]\n&gt; my.data[1]\n&gt; with(my.data, Name)\n&gt; my.data[ , \"Name\"]\n&gt; my.data[ , 1]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk00-Introduction to Scripts in the RStudio-Environment"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "final_exam_quarto",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html",
    "href": "w05_Chapter01_Data_and_Statistics.html",
    "title": "Chapter 01: Data and Statistics",
    "section": "",
    "text": "According to Russell Ackoff, a systems theorist and professor of organizational change, the content of the human mind can be classified into a hierarchy of categories:\n\n\n\nData: symbols (either numeric or iconic)\nData is raw. It simply exists and has no significance beyond its symbolic existence. It can exist in any form, usable or not.\nInformation: data that are processed to be useful; i.e., data that are placed into a context (relationship to similar data) providing answers to “who”, “what”, “where”, “how much” and “when” questions\nInformation is data that has been given meaning by way of relational connection, such that \\(x_1 &lt; x_2\\) or \\(x_1 \\text{ precedes } x_2\\) etc.\nKnowledge: application of data and information; answers “how” questions leading to some causal statements.\nKnowledge is the appropriate connection of information, such that when X increases Y will also increase.\nWisdom: Construction of individual pieces of knowledge into a “general” framework (perhaps even theory including the explanation of apparent contradictions. A theory should be general enough to allow predictions based on new observations.\nDeveloping an encompassing perspective of understanding real world events.\n\n\nNote: “Big data” exploration and mining cannot improve our wisdom, it just provides a description of observed patterns in given data (see BIGDATAANDSTATISTICS.PDF).\n\n\n\n\nAccording to Gene Bellinger, Durval Castro, and Anthony Mills “understanding” connects all levels:\n\n\n\nData-Information-Knowledge-Wisdom Hierarchy\n\n\nScience advances by:\n\n[a] uncovering meaningful patterns within previously noisy information or\n[b] explaining the mechanism and conditions under which patterns emerge,\n[c] revising current knowledge so it can accommodate in a unified framework (a theory) previously contradicting information.\n\n\nStatistics provides tools that help us uncover patterns in observed data and test hypotheses about their underlying data generating process.\nHypotheses are derived from theory and establish predictions which can be tested against given empirical observations.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#the-hierarchy",
    "href": "w05_Chapter01_Data_and_Statistics.html#the-hierarchy",
    "title": "Chapter 01: Data and Statistics",
    "section": "",
    "text": "Data: symbols (either numeric or iconic)\nData is raw. It simply exists and has no significance beyond its symbolic existence. It can exist in any form, usable or not.\nInformation: data that are processed to be useful; i.e., data that are placed into a context (relationship to similar data) providing answers to “who”, “what”, “where”, “how much” and “when” questions\nInformation is data that has been given meaning by way of relational connection, such that \\(x_1 &lt; x_2\\) or \\(x_1 \\text{ precedes } x_2\\) etc.\nKnowledge: application of data and information; answers “how” questions leading to some causal statements.\nKnowledge is the appropriate connection of information, such that when X increases Y will also increase.\nWisdom: Construction of individual pieces of knowledge into a “general” framework (perhaps even theory including the explanation of apparent contradictions. A theory should be general enough to allow predictions based on new observations.\nDeveloping an encompassing perspective of understanding real world events.\n\n\nNote: “Big data” exploration and mining cannot improve our wisdom, it just provides a description of observed patterns in given data (see BIGDATAANDSTATISTICS.PDF).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#understanding-connects-all-levels",
    "href": "w05_Chapter01_Data_and_Statistics.html#understanding-connects-all-levels",
    "title": "Chapter 01: Data and Statistics",
    "section": "",
    "text": "According to Gene Bellinger, Durval Castro, and Anthony Mills “understanding” connects all levels:\n\n\n\nData-Information-Knowledge-Wisdom Hierarchy\n\n\nScience advances by:\n\n[a] uncovering meaningful patterns within previously noisy information or\n[b] explaining the mechanism and conditions under which patterns emerge,\n[c] revising current knowledge so it can accommodate in a unified framework (a theory) previously contradicting information.\n\n\nStatistics provides tools that help us uncover patterns in observed data and test hypotheses about their underlying data generating process.\nHypotheses are derived from theory and establish predictions which can be tested against given empirical observations.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#objectives-of-science",
    "href": "w05_Chapter01_Data_and_Statistics.html#objectives-of-science",
    "title": "Chapter 01: Data and Statistics",
    "section": "2.1 Objectives of Science",
    "text": "2.1 Objectives of Science\n\nDevelop a logical structure for the objects and events of the world surrounding us. This logical structure erects a theoretical framework that relates objects and events with each other and explains why relationships among them exist.\nThis logical structure should be generally applicable. The goal of the logical structure is to understand universal cause-effect relationships, predict and control objects and events.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#key-concepts-in-measurement-theory",
    "href": "w05_Chapter01_Data_and_Statistics.html#key-concepts-in-measurement-theory",
    "title": "Chapter 01: Data and Statistics",
    "section": "5.1 Key Concepts in Measurement Theory",
    "text": "5.1 Key Concepts in Measurement Theory\nMeasurement theory is concerned with the development of:\n\na numerical scale that reflects the empirically observed relationships among the objects and\nthe assignment of a numerical representation to the objects’ properties.\n\nDifferent assignment rules of numerical values to properties of objects, which preserve the empirically observed relationships among the objects, are classed together into a measurement scale.\nE.g., kilometers and miles share a common measurement scale.\n\n5.1.1 Trilateration Example\n \nPhysical properties are easier to measure than abstract social, economic, or psychological concepts.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#overview-over-the-system-of-scales",
    "href": "w05_Chapter01_Data_and_Statistics.html#overview-over-the-system-of-scales",
    "title": "Chapter 01: Data and Statistics",
    "section": "6.1 Overview over the System of Scales",
    "text": "6.1 Overview over the System of Scales\n\n6.1.1 Nominal Scale\nNumbers are used only to distinguish among objects and label their attributes. I.e., numbers are used as symbols.\n\nAllows classifying similar observations together into groups\nApplicable mathematical relationships: \\(= \\text{ and } \\neq\\)\nNotes: Categorical variables allow to count the frequency of equal values in a sample or a population.\nR codes categorical variables into factors, which group similar objects together.\n\n\n\n6.1.2 Ordinal Scale\nNumbers are used to place objects in an order.\n\nAllows ranking observations.\nApplicable mathematical relationships: \\(=, \\neq, &lt; \\text{ and } &gt;\\)\nNotes: Sample observations can be ranked. Due to the coarseness of the measurement instrument (lack of precision), some observations may have equal rank (also called ties).\n\n\n\n6.1.3 Interval Scale\nScale on which equal numerical intervals between objects represent equal differences.\n\nA fixed step length is defined, which is equal at each location on the scale.\nApplicable mathematical relationship: \\[=, \\neq, &lt;, &gt;, \\text{ and the transformation } y = b_0 + b_1 \\cdot x \\text{ without changing the relation}\\] with \\(b_1 &gt; 0\\) (the fixed distance unit/step length) and an arbitrary point of origin \\(b_0\\) on the scale.\nNotes: Equal difference allows performing summations and subtractions of values, but not size comparisons.\n\n\n\n6.1.4 Ratio Scale\nScale with a true zero reference point allowing to measure size differences by ratios.\n\nAllow using size comparison phrases such as “twice as big”.\nApplicable mathematical relationships: \\[=, \\neq, &lt;, &gt;, \\text{ and the transformation } y = \\underbrace{b_0}_{=0} + b_1 \\cdot x\\] with \\(b_1 &gt; 0\\) (the distance unit) the origin \\(b_0 = 0\\) constraint to zero (fixed base level).\nNotes: Metric and continuous variable. Frequently used in the physical sciences.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#example-hierarchy-of-temperature-scales",
    "href": "w05_Chapter01_Data_and_Statistics.html#example-hierarchy-of-temperature-scales",
    "title": "Chapter 01: Data and Statistics",
    "section": "6.2 Example: Hierarchy of Temperature Scales",
    "text": "6.2 Example: Hierarchy of Temperature Scales\n(see the history of temperature scales)\n\n\n\nHierarchy of Temperature Scales\n\n\n\n\n\n\n\n\n\n\nScale\nZero Point\nDescription\n\n\n\n\nKelvin\n0\nAbsolute zero\n\n\nFahrenheit\n0\nArbitrary\n\n\nCelsius\n0\nFreezing point of water\n\n\nOrdinal\n[1] freezing, [2] cold, [3] mild, [4] hot, [5] burning\nOrdered categories\n\n\nNominal\nuncomfortable, comfortable, uncomfortable\nUnordered categories\n\n\n\n\n6.2.1 Transformations and Recoding\n\nMeasurements can be recoded and transformed.\nExample: Transforming Celsius into Fahrenheit: \\[[°F] = [°C] \\times \\frac{9}{5} + 32\\] or Fahrenheit into Celsius: \\[[°C] = ([°F] - 32) \\times \\frac{5}{9}\\] vice versa.\nFurthermore, recoding temperatures: mild → comfortable, hot and cold → uncomfortable.\n\n\n\n6.2.2 Additional Properties\n\nMetric measurement scales can have a fixed upper and lower bound within which the numerical representation of an attribute can vary. Example: The percentage scale.\nConcatenation (counting) operations allow combining and aggregating ratio-scaled observations. Example: [area of region A] + [area of region B] or number of people in both areas.\n\n\nNote: Measurements on areal objects are always based on aggregation of elemental areal units within the area.\n\nHow many reference copies are necessary to give the observed aggregated value? The unit size of the reference copy determines the resolution of the scale and subsequent measurement precision. The smaller the unit size the higher the precision (remember the machine epsilon).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#further-differentiation",
    "href": "w05_Chapter01_Data_and_Statistics.html#further-differentiation",
    "title": "Chapter 01: Data and Statistics",
    "section": "6.3 Further Differentiation",
    "text": "6.3 Further Differentiation\n\nDiscrete measurements comprise of a countable number of representations (usually a small number of integer numbers).\nContinuous measurement allow for fractional representations. Continuous measurements are always quantitatively scaled (floating point numbers).\nFundamental measurements are directly observable, e.g., distances.\nDerived measurement must be calculated from fundamental measurements:\n\nthe ratio of different units e.g., kilo-watt-hours-per-day = power-consumption-during-billing-period / length-of-billing-period, the population density\nthe rate of equal units where the numerator is part of the denominator: murders among total number of crimes, the percentage of population living in urbanized areas.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#the-measurement-scale-and-spatial-objects",
    "href": "w05_Chapter01_Data_and_Statistics.html#the-measurement-scale-and-spatial-objects",
    "title": "Chapter 01: Data and Statistics",
    "section": "6.4 The Measurement Scale and Spatial Objects",
    "text": "6.4 The Measurement Scale and Spatial Objects\n\n\n\nMeasurement scales and spatial objects (point, line, area) for nominal, ordinal, and interval-ratio data\n\n\nFigure: Some examples of the three classes of representation (point, line, area) and how they might be used to portray nominal, ordinal and interval-ratio data.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#critical-data-analysis",
    "href": "w05_Chapter01_Data_and_Statistics.html#critical-data-analysis",
    "title": "Chapter 01: Data and Statistics",
    "section": "7.1 Critical Data Analysis",
    "text": "7.1 Critical Data Analysis\nAn analyst must know as much as possible about the measurement process that has been used to generate the observed data. She/he must make sure that the data make “sense”:\n\nWhat do we expect to see → do the data and analysis results match our expectations? If they do not match our expectations, then we either have messed up or we are discovering something surprisingly and previously unknown to us.\nOne way to gain confidence is through replicability by using:\n\n[a] two independent samples or\n[b] two different analysis approaches.\n\nIf the outcomes match, then we are more confident in the feasibility of the results. In statistics this is called meta-analysis.\nIn technical systems, such as in aviation, replicability is embedded in redundant systems.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#common-problems-with-data",
    "href": "w05_Chapter01_Data_and_Statistics.html#common-problems-with-data",
    "title": "Chapter 01: Data and Statistics",
    "section": "7.2 Common Problems with Data",
    "text": "7.2 Common Problems with Data\n\nInstrument calibration, instrument precision and rounding.\nDealing with putative outliers and re-editing and inconsistent data.\nExample: Ozone layer hole over Antarctica has not been discovered before mid-1980 because the software analyzing data from the Nimbus-7 satellite was programmed to delete extreme measurements. Extreme measurements were initially regarded to reflect a sensor problem.\nOmissions or selective over-sampling of specific groups of data.\nSoftware limitations and operation: Example: Trying to import 120,000 records into an old 16-bit dBase file or incorrectly joining two different data sets.\nAggregation into summary measures drops the variability among the objects by compressing their attributes into a simple summary measure.\nErrors in the data may be systematic (e.g., biased) or random variability:\n\n \nFigure 2: Bias and lack of precision in sample results.\n\n\nHigh bias, high precision\n\n\nLow bias, low precision\n\n\nHigh bias, low precision\n\n\nLow bias, high precision",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#gps-measurement-example",
    "href": "w05_Chapter01_Data_and_Statistics.html#gps-measurement-example",
    "title": "Chapter 01: Data and Statistics",
    "section": "7.3 GPS Measurement Example",
    "text": "7.3 GPS Measurement Example\nExample: A temporal sequence of 100 measurements were taken by 4 GPS units at the same time and the same fixed location. The red square is the true location and the blue dot the average measured location.\n\nWhat errors and random variations influence the measured locational information of the GPS receiver?\nIs there some temporal persistence in the measurements?",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#descriptive-statistics",
    "href": "w05_Chapter01_Data_and_Statistics.html#descriptive-statistics",
    "title": "Chapter 01: Data and Statistics",
    "section": "8.1 Descriptive Statistics",
    "text": "8.1 Descriptive Statistics\nCollection, organization, and presentation of data\nSummary: Reducing the variability in a distribution into a small number of meaningful characteristics\n\\(\\Rightarrow\\) minimize the effects of information loss. Learn how to meaningfully interpret the summary statistics. Perform comparisons to understand the internal variability",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#inferential-statistics",
    "href": "w05_Chapter01_Data_and_Statistics.html#inferential-statistics",
    "title": "Chapter 01: Data and Statistics",
    "section": "8.2 Inferential Statistics",
    "text": "8.2 Inferential Statistics\nDraw conclusions about an unknown population (or data generating process) from a random or a structured random samples. Bayesian statistics incorporates prior believes about the underlying population. The sample must be representative of its underlying population.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w05_Chapter01_Data_and_Statistics.html#specialized-meaning",
    "href": "w05_Chapter01_Data_and_Statistics.html#specialized-meaning",
    "title": "Chapter 01: Data and Statistics",
    "section": "8.3 Specialized Meaning",
    "text": "8.3 Specialized Meaning\nDevelopment and evaluation of methodologies to collect data, the presentation of data, and the analysis of data under uncertainty.\n\n[a] Methods leading to summarization of vast amounts of data within a given model framework,\n[b] theory validation and generalization,\n[c] forecasting (planning, selection of course of action)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk05-ch1.Data & Statistics"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html",
    "href": "w07_Chapter03_MeanVar.html",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "The center of a distribution can be measured by several statistics: arithmetic mean, median, mode and others.\nThe center of a distribution provides us with the summary information about that location around which the data predominately vary. It can be considered as the most representative data value.\nEach of the central tendency measures have advantages and disadvantages depending on properties of the underlying data distribution and the problem at hand.\nIn general, using just a locational summary measure for a full-fledged distribution of the sample is accompanied by substantial loss of information (e.g., shape of the distribution)\n\n\n\n\n\nFor metric variables the value halfway point between \\(x_{min}\\) and \\(x_{max}\\):\n\n\\[x_{mid} = \\frac{x_{max} + x_{min}}{2}\\]\n\nDoes not work well for skewed distributions.\nIt depends on just two observations, which are also the most extreme ones. Therefore, it ignores all the other sample data.\nIt is highly influenced by sampling variability.\n\n\n\n\n\n\nThe most frequently observed data value (or histogram bar mid-point) is described by the mode.\nIn essence, it is the value that has the highest likelihood of being observed in our data.\nThe mode is a valid statistic for categorical and metric variables.\nThe mode is influenced by the definition of the bins in a histogram.\nFor multi-modal distributions the definition needs to be modified:\n\nIf several modes are in close vicinity of each other, then we pick a representative value from this ensemble of modes.\nFor a small number of outstanding modes, we could report locations of all modes as a set of representative values.\nRecall, multimodality usually hints at the fact that distinct underlying mechanisms have generated a heterogeneous set of data.\n\n\n\n\n\n\n\nThe median is the middle number with 50% of the observations larger than the median and 50% of the observations less than the median.\nThe median is only valid for metric data.\nThe median is closely related to the order statistic. Data in the order statistic are sorted ascending.\n\n\n\n\\[x_{[1]}, x_{[2]}, \\ldots, \\underbrace{x_{\\left[\\frac{N+1}{2} - 1\\right]}}_{50\\% \\text{ observations} \\leq x_{\\left[\\frac{N+1}{2}\\right]}}, \\overbrace{x_{\\left[\\frac{N+1}{2}\\right]}}^{Median}, \\underbrace{x_{\\left[\\frac{N+1}{2} + 1\\right]}, \\ldots, x_{[N-1]}, x_{[N]}}_{50\\% \\text{ observations} \\geq x_{\\left[\\frac{N+1}{2}\\right]}}\\]\n\n\n\n\\[x_{[1]}, x_{[2]}, \\ldots, \\underbrace{x_{\\left[\\frac{N}{2}\\right]}}_{50\\% \\text{ observations} \\leq x_{\\left[\\frac{N}{2}\\right]}}, \\overbrace{\\frac{x_{\\left[\\frac{N}{2}\\right]} + x_{\\left[\\frac{N}{2} + 1\\right]}}{2}}^{Artificial\\ Median}, \\underbrace{x_{\\left[\\frac{N}{2} + 1\\right]}, \\ldots, x_{[N-1]}, x_{[N]}}_{50\\% \\text{ observations} \\geq x_{\\left[\\frac{N}{2} + 1\\right]}}\\]\n\nSome software packages offer alternative definitions for the mid-point of an order sequence of data values. Recall the quantile() function in R.\nThe median is also called the 50% quantile, i.e., second quartile.\n\n\n\n\nThe median minimizes the absolute distances to all data values:\n\\[\\sum_{i=1}^{n} |x_i - X_{median}| &lt; \\sum_{i=1}^{n} |x_i - \\theta| \\quad \\text{for any } \\theta \\neq X_{median}\\]\nwith \\(|x_i - X_{median}| \\equiv \\sqrt{(x_i - X_{median})^2}\\)\nThis can be used to find, for instance, the optimal location (on a line) to which the overall distance is the smallest (see Euclidian median definition in BBR, p. 136)\n\n\n\n\n\n\nThe arithmetic mean is commonly known as just the mean.\nThis statistic denoted by \\(\\bar{X}\\) and calculated by:\n\n\\[\\bar{X} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} x_i\\]\nwhere \\(n\\) is the number of sample observations.\n\nThe mean is only valid for metric data.\nAssume that we place the data at their proper location on a scale. Data points have identical weights. Then the mean is the pivot point where the scale is in balance (center of gravity):\n\n\n\n\nMeasures of Central Tendency using DO data\n\n\nFIGURE 3-2. Measures of central tendency using the DO data.\n\n\n\nZero-sum property: The sum of the differences around the mean is zero:\n\n\\[\\sum_{i=1}^{n} (x_i - \\bar{X}) = 0\\]\nTo see this property note:\n\\[\\sum_{i=1}^{n} (x_i - \\bar{X}) = \\sum_{i=1}^{n} x_i - n \\cdot \\bar{X} = \\sum_{i=1}^{n} x_i - n \\cdot \\frac{\\sum_{i=1}^{n} x_i}{n} = 0\\]\nTherefore, it is the center of gravity of the data, i.e., the pivot at which the scale is balanced.\n\nLoss of degrees of freedom: A consequence of the zero-sum property is that once the mean has been calculated from the sample observations only \\(n - 1\\) observations can vary freely (loss of one degree of freedom).\nMinimum squared deviations: The squared deviations of the data around the mean is smaller than for any other central tendency statistic (see the math review and animation OUTLIERANIMATION.MP4):\n\n\\[\\sum_{i=1}^{n} (x_i - \\bar{X})^2 &lt; \\sum_{i=1}^{n} (x_i - \\theta)^2 \\quad \\text{for any } \\theta \\neq \\bar{X}\\]\nSee the R-script OptimizeMeanMedian.R to see the minimizing properties of both the mean and median.\n\n\n\n\n\n\nIf a metric distribution is both symmetric and uni-modal, then the mode, median and mean are all identical.\n\nThis property can be used to test for skewness:\n\nFor positively skewed uni-modal distributions: \\(x_{mode} &lt; x_{median} &lt; \\bar{x}\\)\nFor negatively skewed uni-modal distributions: \\(\\bar{x} &lt; x_{median} &lt; x_{mode}\\)\n\n\n\n\nEasy to calculate\nCan be used for categorical data\n\n\n\n\n\nThe mode may not be representative for the central tendency in the data if it is located in one tail.\nFor multimodal data its definition is somewhat arbitrary, and all relevant peaks should be reported\n\n\n\n\n\nUnaffected by extreme observation or skewed data distributions. It is more informative in these cases than the mean. Therefore, you frequently find the median in reports.\nFixed distance units of the measurement scale are not required.\n\n\n\n\n\nNot as stable from sample to sample because it depends on just one data value (for odd sample sizes) or two data values (for even sample sizes).\nHas disadvantages in mathematical derivations because it is based on an optimization function with a discontinuous pole.\nComputationally it requires that the data are first sorted first, which takes extra time.\n\n\n\n\n\nEasy to calculate with several useful properties in mathematical statistics.\nRelates well to many parameters of underlying theoretical population distributions.\nIt provides a stable estimate from sample to sample\n\n\n\n\n\nSensitive to skewed distributions and outliers\n\n\n\n\n\n\n\nThe weighted mean is used for data which are first aggregated into \\(G\\) homogenous groups. Each representative group value \\(x_g\\) has a weight \\(w_g \\geq 0\\) equal to the size of the group:\n\n\\[\\bar{x}_{weig} = \\frac{1}{\\sum_{g=1}^{G} w_g} \\cdot \\sum_{g=1}^{G} w_g \\cdot x_g\\]\n\nThe arithmetic mean is a special case of the weighted mean by setting \\(w_g = 1\\) and \\(G = n\\).\n\n\n\n\n\n\nThe trimmed mean discards an equal proportion of data (say 5%, 10%, or 20%) from both tails of the distribution. Subsequently, the mean is calculated from the remaining observations.\nIf \\(x_{[1]} \\leq x_{[2]} \\leq \\cdots \\leq x_{[n]}\\) represents the ordered sample values, then the trimmed mean is given by:\n\n\\[\\bar{x}_{\\tau} = \\frac{1}{n - 2 \\cdot k} \\cdot \\sum_{i=k+1}^{n-k} x_{[i]}\\]\nwhere \\(k\\) is the smallest integer greater than or equal to \\(n \\cdot \\tau\\) with \\(0 \\leq \\tau \\leq 0.5\\).\n\n\n\nIt becomes robust against the effects of outliers and skewed distribution while still focusing on the most representative data point.\nFor \\(\\tau = 0\\) we get the arithmetic mean \\(\\bar{x}\\) and for \\(\\tau \\cong 0.5\\) we get the median \\(x_{median}\\).\n\n\n\n\n\nWe disregard a certain percentage of observation from both tails of our sample and, therefore, loses information.\nThe data must be sorted to calculate the trimmed mean.\nThere are no fixed rules on how many observations we would need to trim away from either end of the distribution. Therefore, it is important to first visually inspect the distribution and to experiment with different \\(\\tau\\) values.\n\n\n\n\nLike the trimmed mean, however, each of the smallest dropped observations are replaced by \\(x_{[k+1]}\\) and vice versa for the largest dropped observations by \\(x_{[n-k]}\\):\n\\[\\bar{x}_{\\tau}^{W} = \\frac{1}{n} \\cdot \\left( k \\cdot (x_{[k+1]} + x_{[n-k]}) + \\sum_{i=k+1}^{n-k} x_{[i]} \\right)\\]\nThis has the advantage that the sample size is not reduced. The winsorized mean is calculated with the trim option in the R function mean(x, trim = 0, na.rm = FALSE).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nObservations\n12\n14\n15\n17\n20\n21\n23\n24\n27\n29\n\n\nTrimmed\n–\n–\n15\n17\n20\n21\n23\n24\n–\n–\n\n\nWinsorized\n15\n15\n15\n17\n20\n21\n23\n24\n24\n24\n\n\n\n\n\n\n\n\n\nThe average rate of change \\(r_i\\) per time period is calculated over several successive time period change rates as the geometric mean:\n\n\\[\\bar{r} = \\sqrt[n]{r_1 \\cdot r_2 \\cdot \\cdots \\cdot r_n} = \\left( \\prod_{i=1}^{n} r_i \\right)^{\\frac{1}{n}} = \\exp\\left( \\frac{\\sum_{i=1}^{n} \\ln(r_i)}{n} \\right)\\]\nNote, all rates must be positive, i.e., \\(r_i &gt; 0\\) where \\(r_i = y_i / y_{i-1}\\).\n\n\n\n\n\nAccount Beginning of Year\nAccount End of Year\nAnnual Growth Rate\n\n\n\n\n$ 100\n$ 115\n\\(\\frac{115}{100} = 1.15\\%\\)\n\n\n$ 115\n$ 110\n\\(\\frac{110}{115} \\cong 0.96\\%\\)\n\n\n$ 110\n$ 120\n\\(\\frac{120}{110} \\cong 1.09\\%\\)\n\n\n\nThe average annual growth rate is:\n\\[\\sqrt[3]{\\frac{120}{100}} \\cong 1.063\\% \\quad \\text{or} \\quad \\sqrt[3]{1.15\\% \\cdot 0.96\\% \\cdot 1.09\\%} \\cong 1.063\\%\\]\n\n\n\n\n\n\nThe harmonic mean is used to calculate the average for ratio variables with a fixed common denominator, that is, \\(x_i\\) miles/one hour or \\(x_i\\) miles/one gallon, where each measured value refers to the same distance segment travelled.\nThe harmonic mean is:\n\n\\[\\bar{x}_{harm} = \\frac{n}{\\sum_{i=1}^{n} \\frac{1}{x_i}}\\]\n\n\n[a] If a vehicle travels the first distance at \\(60\\) miles/h and an identical second distance at \\(40\\) miles/h, then it would have covered both equal distances on average at:\n\\[\\frac{2}{\\frac{1}{60} + \\frac{1}{40}} = 48 \\text{ miles/h}\\]\n[b] On the other hand, if a vehicle would have travelled two equal time intervals at \\(60\\) miles/h and at \\(40\\) miles/h, respectively, then the average speed of \\(50\\) miles/h would have covered the same distance.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#the-mid-range",
    "href": "w07_Chapter03_MeanVar.html#the-mid-range",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "For metric variables the value halfway point between \\(x_{min}\\) and \\(x_{max}\\):\n\n\\[x_{mid} = \\frac{x_{max} + x_{min}}{2}\\]\n\nDoes not work well for skewed distributions.\nIt depends on just two observations, which are also the most extreme ones. Therefore, it ignores all the other sample data.\nIt is highly influenced by sampling variability.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#the-mode",
    "href": "w07_Chapter03_MeanVar.html#the-mode",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "The most frequently observed data value (or histogram bar mid-point) is described by the mode.\nIn essence, it is the value that has the highest likelihood of being observed in our data.\nThe mode is a valid statistic for categorical and metric variables.\nThe mode is influenced by the definition of the bins in a histogram.\nFor multi-modal distributions the definition needs to be modified:\n\nIf several modes are in close vicinity of each other, then we pick a representative value from this ensemble of modes.\nFor a small number of outstanding modes, we could report locations of all modes as a set of representative values.\nRecall, multimodality usually hints at the fact that distinct underlying mechanisms have generated a heterogeneous set of data.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#the-median",
    "href": "w07_Chapter03_MeanVar.html#the-median",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "The median is the middle number with 50% of the observations larger than the median and 50% of the observations less than the median.\nThe median is only valid for metric data.\nThe median is closely related to the order statistic. Data in the order statistic are sorted ascending.\n\n\n\n\\[x_{[1]}, x_{[2]}, \\ldots, \\underbrace{x_{\\left[\\frac{N+1}{2} - 1\\right]}}_{50\\% \\text{ observations} \\leq x_{\\left[\\frac{N+1}{2}\\right]}}, \\overbrace{x_{\\left[\\frac{N+1}{2}\\right]}}^{Median}, \\underbrace{x_{\\left[\\frac{N+1}{2} + 1\\right]}, \\ldots, x_{[N-1]}, x_{[N]}}_{50\\% \\text{ observations} \\geq x_{\\left[\\frac{N+1}{2}\\right]}}\\]\n\n\n\n\\[x_{[1]}, x_{[2]}, \\ldots, \\underbrace{x_{\\left[\\frac{N}{2}\\right]}}_{50\\% \\text{ observations} \\leq x_{\\left[\\frac{N}{2}\\right]}}, \\overbrace{\\frac{x_{\\left[\\frac{N}{2}\\right]} + x_{\\left[\\frac{N}{2} + 1\\right]}}{2}}^{Artificial\\ Median}, \\underbrace{x_{\\left[\\frac{N}{2} + 1\\right]}, \\ldots, x_{[N-1]}, x_{[N]}}_{50\\% \\text{ observations} \\geq x_{\\left[\\frac{N}{2} + 1\\right]}}\\]\n\nSome software packages offer alternative definitions for the mid-point of an order sequence of data values. Recall the quantile() function in R.\nThe median is also called the 50% quantile, i.e., second quartile.\n\n\n\n\nThe median minimizes the absolute distances to all data values:\n\\[\\sum_{i=1}^{n} |x_i - X_{median}| &lt; \\sum_{i=1}^{n} |x_i - \\theta| \\quad \\text{for any } \\theta \\neq X_{median}\\]\nwith \\(|x_i - X_{median}| \\equiv \\sqrt{(x_i - X_{median})^2}\\)\nThis can be used to find, for instance, the optimal location (on a line) to which the overall distance is the smallest (see Euclidian median definition in BBR, p. 136)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#the-arithmetic-mean",
    "href": "w07_Chapter03_MeanVar.html#the-arithmetic-mean",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "The arithmetic mean is commonly known as just the mean.\nThis statistic denoted by \\(\\bar{X}\\) and calculated by:\n\n\\[\\bar{X} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} x_i\\]\nwhere \\(n\\) is the number of sample observations.\n\nThe mean is only valid for metric data.\nAssume that we place the data at their proper location on a scale. Data points have identical weights. Then the mean is the pivot point where the scale is in balance (center of gravity):\n\n\n\n\nMeasures of Central Tendency using DO data\n\n\nFIGURE 3-2. Measures of central tendency using the DO data.\n\n\n\nZero-sum property: The sum of the differences around the mean is zero:\n\n\\[\\sum_{i=1}^{n} (x_i - \\bar{X}) = 0\\]\nTo see this property note:\n\\[\\sum_{i=1}^{n} (x_i - \\bar{X}) = \\sum_{i=1}^{n} x_i - n \\cdot \\bar{X} = \\sum_{i=1}^{n} x_i - n \\cdot \\frac{\\sum_{i=1}^{n} x_i}{n} = 0\\]\nTherefore, it is the center of gravity of the data, i.e., the pivot at which the scale is balanced.\n\nLoss of degrees of freedom: A consequence of the zero-sum property is that once the mean has been calculated from the sample observations only \\(n - 1\\) observations can vary freely (loss of one degree of freedom).\nMinimum squared deviations: The squared deviations of the data around the mean is smaller than for any other central tendency statistic (see the math review and animation OUTLIERANIMATION.MP4):\n\n\\[\\sum_{i=1}^{n} (x_i - \\bar{X})^2 &lt; \\sum_{i=1}^{n} (x_i - \\theta)^2 \\quad \\text{for any } \\theta \\neq \\bar{X}\\]\nSee the R-script OptimizeMeanMedian.R to see the minimizing properties of both the mean and median.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#comparison-of-properties-of-measures-of-central-tendency",
    "href": "w07_Chapter03_MeanVar.html#comparison-of-properties-of-measures-of-central-tendency",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "If a metric distribution is both symmetric and uni-modal, then the mode, median and mean are all identical.\n\nThis property can be used to test for skewness:\n\nFor positively skewed uni-modal distributions: \\(x_{mode} &lt; x_{median} &lt; \\bar{x}\\)\nFor negatively skewed uni-modal distributions: \\(\\bar{x} &lt; x_{median} &lt; x_{mode}\\)\n\n\n\n\nEasy to calculate\nCan be used for categorical data\n\n\n\n\n\nThe mode may not be representative for the central tendency in the data if it is located in one tail.\nFor multimodal data its definition is somewhat arbitrary, and all relevant peaks should be reported\n\n\n\n\n\nUnaffected by extreme observation or skewed data distributions. It is more informative in these cases than the mean. Therefore, you frequently find the median in reports.\nFixed distance units of the measurement scale are not required.\n\n\n\n\n\nNot as stable from sample to sample because it depends on just one data value (for odd sample sizes) or two data values (for even sample sizes).\nHas disadvantages in mathematical derivations because it is based on an optimization function with a discontinuous pole.\nComputationally it requires that the data are first sorted first, which takes extra time.\n\n\n\n\n\nEasy to calculate with several useful properties in mathematical statistics.\nRelates well to many parameters of underlying theoretical population distributions.\nIt provides a stable estimate from sample to sample\n\n\n\n\n\nSensitive to skewed distributions and outliers",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#weighted-mean",
    "href": "w07_Chapter03_MeanVar.html#weighted-mean",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "The weighted mean is used for data which are first aggregated into \\(G\\) homogenous groups. Each representative group value \\(x_g\\) has a weight \\(w_g \\geq 0\\) equal to the size of the group:\n\n\\[\\bar{x}_{weig} = \\frac{1}{\\sum_{g=1}^{G} w_g} \\cdot \\sum_{g=1}^{G} w_g \\cdot x_g\\]\n\nThe arithmetic mean is a special case of the weighted mean by setting \\(w_g = 1\\) and \\(G = n\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#trimmed-mean-and-winsorized-mean",
    "href": "w07_Chapter03_MeanVar.html#trimmed-mean-and-winsorized-mean",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "The trimmed mean discards an equal proportion of data (say 5%, 10%, or 20%) from both tails of the distribution. Subsequently, the mean is calculated from the remaining observations.\nIf \\(x_{[1]} \\leq x_{[2]} \\leq \\cdots \\leq x_{[n]}\\) represents the ordered sample values, then the trimmed mean is given by:\n\n\\[\\bar{x}_{\\tau} = \\frac{1}{n - 2 \\cdot k} \\cdot \\sum_{i=k+1}^{n-k} x_{[i]}\\]\nwhere \\(k\\) is the smallest integer greater than or equal to \\(n \\cdot \\tau\\) with \\(0 \\leq \\tau \\leq 0.5\\).\n\n\n\nIt becomes robust against the effects of outliers and skewed distribution while still focusing on the most representative data point.\nFor \\(\\tau = 0\\) we get the arithmetic mean \\(\\bar{x}\\) and for \\(\\tau \\cong 0.5\\) we get the median \\(x_{median}\\).\n\n\n\n\n\nWe disregard a certain percentage of observation from both tails of our sample and, therefore, loses information.\nThe data must be sorted to calculate the trimmed mean.\nThere are no fixed rules on how many observations we would need to trim away from either end of the distribution. Therefore, it is important to first visually inspect the distribution and to experiment with different \\(\\tau\\) values.\n\n\n\n\nLike the trimmed mean, however, each of the smallest dropped observations are replaced by \\(x_{[k+1]}\\) and vice versa for the largest dropped observations by \\(x_{[n-k]}\\):\n\\[\\bar{x}_{\\tau}^{W} = \\frac{1}{n} \\cdot \\left( k \\cdot (x_{[k+1]} + x_{[n-k]}) + \\sum_{i=k+1}^{n-k} x_{[i]} \\right)\\]\nThis has the advantage that the sample size is not reduced. The winsorized mean is calculated with the trim option in the R function mean(x, trim = 0, na.rm = FALSE).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nObservations\n12\n14\n15\n17\n20\n21\n23\n24\n27\n29\n\n\nTrimmed\n–\n–\n15\n17\n20\n21\n23\n24\n–\n–\n\n\nWinsorized\n15\n15\n15\n17\n20\n21\n23\n24\n24\n24",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#the-geometric-mean",
    "href": "w07_Chapter03_MeanVar.html#the-geometric-mean",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "The average rate of change \\(r_i\\) per time period is calculated over several successive time period change rates as the geometric mean:\n\n\\[\\bar{r} = \\sqrt[n]{r_1 \\cdot r_2 \\cdot \\cdots \\cdot r_n} = \\left( \\prod_{i=1}^{n} r_i \\right)^{\\frac{1}{n}} = \\exp\\left( \\frac{\\sum_{i=1}^{n} \\ln(r_i)}{n} \\right)\\]\nNote, all rates must be positive, i.e., \\(r_i &gt; 0\\) where \\(r_i = y_i / y_{i-1}\\).\n\n\n\n\n\nAccount Beginning of Year\nAccount End of Year\nAnnual Growth Rate\n\n\n\n\n$ 100\n$ 115\n\\(\\frac{115}{100} = 1.15\\%\\)\n\n\n$ 115\n$ 110\n\\(\\frac{110}{115} \\cong 0.96\\%\\)\n\n\n$ 110\n$ 120\n\\(\\frac{120}{110} \\cong 1.09\\%\\)\n\n\n\nThe average annual growth rate is:\n\\[\\sqrt[3]{\\frac{120}{100}} \\cong 1.063\\% \\quad \\text{or} \\quad \\sqrt[3]{1.15\\% \\cdot 0.96\\% \\cdot 1.09\\%} \\cong 1.063\\%\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#the-harmonic-mean",
    "href": "w07_Chapter03_MeanVar.html#the-harmonic-mean",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "",
    "text": "The harmonic mean is used to calculate the average for ratio variables with a fixed common denominator, that is, \\(x_i\\) miles/one hour or \\(x_i\\) miles/one gallon, where each measured value refers to the same distance segment travelled.\nThe harmonic mean is:\n\n\\[\\bar{x}_{harm} = \\frac{n}{\\sum_{i=1}^{n} \\frac{1}{x_i}}\\]\n\n\n[a] If a vehicle travels the first distance at \\(60\\) miles/h and an identical second distance at \\(40\\) miles/h, then it would have covered both equal distances on average at:\n\\[\\frac{2}{\\frac{1}{60} + \\frac{1}{40}} = 48 \\text{ miles/h}\\]\n[b] On the other hand, if a vehicle would have travelled two equal time intervals at \\(60\\) miles/h and at \\(40\\) miles/h, respectively, then the average speed of \\(50\\) miles/h would have covered the same distance.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#introduction",
    "href": "w07_Chapter03_MeanVar.html#introduction",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n\nThe central tendency gives us information about the approximate location of a distribution. However, it does not provide any information about its spread.\nAll measures of spread discussed here assume that we have a variable on the metric measurement scale.\nThe spread (also called dispersion or variability) is the degree to which all individual data points are jointly distributed around the central tendency. It provides an additional summary measure of the underlying distribution of our data.\nThe spread can range from anywhere between highly clustered around the central tendency to extremely dispersed around the central tendency.\nThe smaller the spread the more representative the central tendency is for observed data points.\nSmall spread translated into high accuracy as long as the central tendency is unbiased (lacks a systematic error). Recall BBR pp. 29-31. In Bayesian statistics reference is frequently made to precision which is simply the inverse to the spread:\n\n\\[precision = \\frac{1}{variance}\\]\n\n2.1.1 Climate Change Example\nThe climate change debate is not only concerned with an overall increase in temperature, but also apprehensive about an increase in the variability of the temperature. This implies that a small shift in temperature in combination with a higher variability will increase the risk for extreme weather events:\n\n\n\nClimate Change and Variability",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#overall-range",
    "href": "w07_Chapter03_MeanVar.html#overall-range",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.2 Overall Range",
    "text": "2.2 Overall Range\n\nThe range just depends just on the two most extreme observations:\n\n\\[\\text{range}(X) = \\max(X) - \\min(x) = x_{[n]} - x_{[1]}\\]\n\nIt is, therefore, highly sensitive to sample variations and outliers.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#interquartile-range",
    "href": "w07_Chapter03_MeanVar.html#interquartile-range",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.3 Interquartile Range",
    "text": "2.3 Interquartile Range\n\nTo calculate the interquartile range, 25% of the observations at the bottom and the top of a distribution are discarded.\nThe range of the remaining 50% of the inner observations defines the interquartile range:\n\n\\[IQR = x_{[75\\%]} - x_{[25\\%]}\\]\n\nTrimming away 25% of the data in each of the distribution’s tails may discard much information and, therefore, may not represent the variability well. It has been suggested to work with statistics with less than 25% trim.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#variation-around-the-mean-variance-and-standard-deviation",
    "href": "w07_Chapter03_MeanVar.html#variation-around-the-mean-variance-and-standard-deviation",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.4 Variation around the Mean: Variance and Standard Deviation",
    "text": "2.4 Variation around the Mean: Variance and Standard Deviation\n\n\n\nDiagrammatic representation of mean and variance calculation\n\n\nFIGURE 2.29 Diagrammatic representation of the calculation of mean and variance from five observations. Mean X is calculated from all five observations. Variance is calculated from differences between observations and the mean. When four differences have been found, the fifth difference is known.\n\n2.4.1 Population Perspective\nIf we would have all \\(N\\) observations from an underlying population of a random variable \\(X\\) with the population mean \\(\\mu_X\\) then the population variance \\(\\sigma_X^2\\) is calculated by:\n\\[\\sigma_X^2 = \\frac{1}{N} \\cdot \\sum_{i=1}^{N} (x_i - \\mu)^2\\]\n\n\n2.4.2 Sample Perspective\nIn contrast, for a given sample with an estimated mean \\(\\bar{x}\\) and \\(n\\) sample observations the estimated variance \\(s_X^2\\) is calculated by:\n\\[s_X^2 = \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\]\nNote the use of \\(n - 1\\) in the denominator and the estimated mean \\(\\bar{x}\\) in the squared deviations.\n\n\n2.4.3 Degrees of Freedom\nOnce we know the mean \\(\\bar{x}\\) and \\(n - 1\\) observations, the last observation can be calculated due to the zero-sum restriction:\n\\[x_n = \\sum_{i=1}^{n-1} x_i - n \\cdot \\bar{x}\\]\nThis is known as a loss of degrees of freedom.\n\n\n2.4.4 Standard Deviation\nThe measurement unit of the variance is in terms of squared units of the original variable \\(X\\). Squared units are difficult to interpret; therefore, by taking the square root of the variance we get a measurement unit that matches that of our original variable.\nThe square root of the variance is called standard deviation:\n\\[\\sigma_X = \\sqrt{\\sigma_X^2} \\text{ for the population and } s_X = \\sqrt{s_X^2} \\text{ for the sample, respectively.}\\]\n\n\n2.4.5 The 2/3 Rule\n\n\n\nSmooth Histogram with 2/3 Rule\n\n\nFor approximately symmetric distributions with a noticeable tendency to cluster around their mean, roughly \\(2/3\\) of the observations will be not more than one standard deviation to the left and right away from the mean.\nHowever, this rule of thumb will not hold for highly skewed distributions or multimodal distributions.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#variation-around-the-mean-mean-absolute-deviation",
    "href": "w07_Chapter03_MeanVar.html#variation-around-the-mean-mean-absolute-deviation",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.5 Variation around the Mean: Mean Absolute Deviation",
    "text": "2.5 Variation around the Mean: Mean Absolute Deviation\n\nRecall, we have seen for the mean that \\(\\sum_{i=1}^{n} (x_i - \\bar{x}) = 0\\).\nConsequently, positive and negative variations around the mean cancel out.\nTo overcome this problem the mean of absolute differences could be used:\n\n\\[MAD = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} |x_i - \\bar{x}|\\]\n\n\\(MAD\\) is in the same measurement unit as the original variable \\(X\\).\nRemember, if the median \\(x_{median}\\) is used instead of the mean \\(\\bar{x}\\) then \\(\\sum_{i=1}^{n} |x_i - x_{median}|\\) minimizes the sum of the absolute distances between all \\(x_i\\)s and the median.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#properties-of-the-mean-and-variance-estimation-rules",
    "href": "w07_Chapter03_MeanVar.html#properties-of-the-mean-and-variance-estimation-rules",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.6 Properties of the Mean and Variance Estimation Rules",
    "text": "2.6 Properties of the Mean and Variance Estimation Rules\n\nEstimation rules are briefly called estimators in the statistical literature.\nThe sample mean \\(\\bar{x}\\) is an estimator for the population mean \\(\\mu\\) and the sample variance \\(s_X^2\\) is an estimator for the population variance \\(\\sigma_X^2\\).\nAn estimator preferably should satisfy specific criteria:\n\nUnbiased: On average over many different samples from the same population, estimation rule should give values equal to their associated true population parameter.\nEfficient: Average deviation of a sample estimator around the true population parameter should be as small as possible (will be addressed in a later chapter).\n\nThe concept of biasedness is somewhat academic because, in order to assess a potential bias of an estimator, we would need to know the true population parameter.\nHowever, one can show with mathematical arguments that some estimation rules will lead to biased estimates, whereas another estimation rule will not be biased.\n\n\n2.6.1 Expected Value and Bias\nThe average of an estimator over many different independent samples of the same length from a population is called in statistical terms the expected value \\(E[\\cdot]\\).\nThe expected value for the estimation rule \\(\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n}\\) for the spread is:\n\\[E\\left[ \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n} \\right] = \\underbrace{\\frac{(n-1)}{n}}_{&lt; 1} \\cdot \\sigma_X^2 \\quad \\text{whereas} \\quad E\\left[ \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1} \\right] = \\sigma_X^2\\]\nTherefore, \\(\\frac{1}{n-1} \\cdot \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\) is an unbiased estimation rule for \\(\\sigma_X^2\\) whereas \\(\\frac{1}{n} \\cdot \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\) slightly under-estimates the variance.\n\n\n2.6.2 Heuristic Explanations for Using \\(n-1\\)\n(i) Because the estimator \\(\\bar{x}\\) minimizes the sum of the squared deviations, that is, \\(\\min_{\\theta} \\sum_{i=1}^{n} (x_i - \\theta)^2\\), for the sample observations, we always have:\n\\[\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\leq \\sum_{i=1}^{n} (x_i - \\mu_X)^2\\]\n(compared to the true population expectation \\(\\mu_X\\)).\nTo correct for this shrinkage in the numerator of the sample variance estimator, the denominator must also be slightly reduced from \\(n\\) to \\(n - 1\\) in order to avoid an under-estimation of the variance.\n(ii) Due to zero-sum restriction in \\(\\sum_{i=1}^{n} (x_i - \\bar{x}) = 0\\) one observation cannot vary freely anymore. We therefore lose one degree of freedom.\n\nFor a large number of sample observations the bias becomes negligibly small because \\(n \\approx n - 1\\) and therefore \\(\\lim_{n \\to \\infty} \\frac{n-1}{n} \\approx 1\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#problem-of-the-trimmed-variance-estimator",
    "href": "w07_Chapter03_MeanVar.html#problem-of-the-trimmed-variance-estimator",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.7 Problem of the Trimmed Variance Estimator",
    "text": "2.7 Problem of the Trimmed Variance Estimator\n\nWhile the trimmed mean \\(\\bar{x}_{\\tau}\\) and the winsorized mean \\(\\bar{x}_{\\tau}^{W}\\) will not differ much, the trimmed variance substantially underestimates the spread, because it ignores any tail observations.\nA winsorized statistic, however, accounts for the number of dropped observations at the bottom and the top of the distribution by padding the trimmed dataset with the smallest and largest remaining observations in the sample.\nThe padding with smallest and largest feasible values in the winsorized variance adjusts for this underestimation by including valid but extreme observations.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#other-measures-of-spread-and-shape-of-distributions",
    "href": "w07_Chapter03_MeanVar.html#other-measures-of-spread-and-shape-of-distributions",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.8 Other Measures of Spread and Shape of Distributions",
    "text": "2.8 Other Measures of Spread and Shape of Distributions\n\n2.8.1 Skewness (Third Moment Measure)\nA measure for the skewness is:\n\\[\\text{skew}(X) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^3}{\\left( \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\right)^{\\frac{3}{2}}}\\]\nwhich is zero for symmetric distributions and positive or negative for positively or negatively skewed distributions, respectively.\nUnderlying structure of the equation:\n\nThe third power (and any odd power) in the numerator preserves the sign of the deviation around the mean. Therefore, the sign of overall sum determines the direction of skewness.\nOutlying observations substantially inflate the numerator due to the third power.\nThe denominator expression \\(\\left( \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\right)^{\\frac{3}{2}}\\) standardizes the numerator so that the skewness estimator becomes independent of the variance. In essence, this is the numerator of the variance estimator taken to the power of \\(3/2\\).\n\nAlternative measure: Pearson’s \\(\\gamma = \\frac{3 \\cdot (\\bar{x} - x_{median})}{s}\\)\n\n\n2.8.2 Kurtosis (Fourth Moment Measure)\nA measure for the kurtosis is:\n\\[\\text{kurt}(X) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^4}{\\left( \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\right)^{\\frac{4}{2}}} - 3\\]\nIt follows the same construction principle as the skewness, however, it uses the fourth power.\nProperties:\n\nIn order to make the kurtosis for the normal distribution, which is used frequently as reference distribution, equal to zero the value 3 is subtracted.\nFor an even power of 4 all deviations around the mean are always positive.\nCompared to a normal distribution:\n\nIf a distribution has more mass in the tails than in the center, then the kurtosis becomes larger than 0\nIf it has more mass in the center relative to the tails, then it becomes less than 0\n\nThe denominator \\(\\left( \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\right)^{\\frac{4}{2}}\\) again standardizes the numerator, however, now the power \\(4/2\\) is used.\nThe kurtosis becomes difficult to interpret for skewed distributions, because one tail of the distribution is long whereas the other is short.\n\n\nNote: There are different software implementations for the skewness and kurtosis estimators. See the online help for the functions e1071::skewness() and e1071::kurtosis() in the library e1071 and their options.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#the-bimodality-index",
    "href": "w07_Chapter03_MeanVar.html#the-bimodality-index",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.9 The Bimodality Index",
    "text": "2.9 The Bimodality Index\n\nThe bimodality index is defined as:\n\n\\[BM = \\frac{(\\text{skewness}^2 + 1)}{\\text{kurtosis} + \\frac{3 \\cdot (n-1)^2}{(n-2) \\cdot (n-3)}}\\]\n\nThis is implemented in the user-specified function BiModalityIndex() in the R-code Chapter03SampleCode.r.\nIt works best for nearly symmetric but potentially multimodal distributions.\n\nFor a uniform distribution its value is \\(\\approx 0.55\\)\nFor bimodal or multimodal distributions this index will be larger than 0.55\nFor a unimodal distribution this index is less than 0.55\n\n\n\n2.9.1 Examples Using a Beta-Distribution",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w07_Chapter03_MeanVar.html#coefficient-of-variation",
    "href": "w07_Chapter03_MeanVar.html#coefficient-of-variation",
    "title": "Chapter 03: Measures of Central Tendency and Variation",
    "section": "2.10 Coefficient of Variation",
    "text": "2.10 Coefficient of Variation\nVariation relative to the mean:\nIf two distributions have different means and a positive support (all observations \\(x_i\\) must be for theoretical reasons \\(x_i \\geq 0\\)) then their standard deviations may not be directly comparable:\n\nA larger mean allows for more spread toward the zero-origin point of the distribution.\nTo adjust for the induced potential extra variability, the coefficient of variation is used:\n\n\\[CV = \\frac{s}{\\bar{x}} \\quad \\forall x_i \\geq 0\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk07-ch3.Measures of Central Tendency and Variation"
    ]
  },
  {
    "objectID": "w08_MotivationDataStory.html",
    "href": "w08_MotivationDataStory.html",
    "title": "Data Tell Stories – Statistics will help Unveiling these Stories",
    "section": "",
    "text": "Your task is to uncover the story, which led to the data shown in the table below. The data were generated from a real major event that took place within the last century.\n\n\nLook at both tables carefully and quietly think about them for a few minutes:\n\nWhat do these tables show?\nDo you see any outstanding patterns?\n\nGroup Activity:\nBreak up into groups; I will answer one question per group with either “yes” or “no”:\n\nYour questions should successively narrow down which event may have led to the given data.\nYour questions are testing hypotheses about how, where, when and why the data were generated.\nWith each of my answers, you may increase your knowledge about the underlying event, which in turn will help you to revise your guesses and will lead to more specific hypothesis about the underlying event.\n\nAfter each group has asked its question, discuss within your group what event potentially has generated the observed data and how you justify your answer. Write down your answer.\n\nNote: If you think you know what the underlying event was, please do not spoil the fun for the remaining participants and remain quiet until the end.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-Motivation Data Story"
    ]
  },
  {
    "objectID": "w08_MotivationDataStory.html#instructions",
    "href": "w08_MotivationDataStory.html#instructions",
    "title": "Data Tell Stories – Statistics will help Unveiling these Stories",
    "section": "",
    "text": "Look at both tables carefully and quietly think about them for a few minutes:\n\nWhat do these tables show?\nDo you see any outstanding patterns?\n\nGroup Activity:\nBreak up into groups; I will answer one question per group with either “yes” or “no”:\n\nYour questions should successively narrow down which event may have led to the given data.\nYour questions are testing hypotheses about how, where, when and why the data were generated.\nWith each of my answers, you may increase your knowledge about the underlying event, which in turn will help you to revise your guesses and will lead to more specific hypothesis about the underlying event.\n\nAfter each group has asked its question, discuss within your group what event potentially has generated the observed data and how you justify your answer. Write down your answer.\n\nNote: If you think you know what the underlying event was, please do not spoil the fun for the remaining participants and remain quiet until the end.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-Motivation Data Story"
    ]
  },
  {
    "objectID": "w08_MotivationDataStory.html#table-1-death-rates-by-economic-status-and-sex",
    "href": "w08_MotivationDataStory.html#table-1-death-rates-by-economic-status-and-sex",
    "title": "Data Tell Stories – Statistics will help Unveiling these Stories",
    "section": "2.1 Table 1: Death Rates by Economic Status and Sex",
    "text": "2.1 Table 1: Death Rates by Economic Status and Sex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEconomic Status\nPopulation Exposed to Risk\n\n\nNumber of Deaths\n\n\nDeaths per 100 Exposed to Risk\n\n\n\n\n\n\n\nMale\nFemale\nBoth\nMale\nFemale\nBoth\nMale\nFemale\nBoth\n\n\nI (high)\n172\n132\n304\n111\n6\n117\n64.5%\n4.5%\n38.5%\n\n\nII\n172\n103\n275\n150\n13\n163\n87.2%\n12.6%\n59.3%\n\n\nIII\n504\n208\n712\n419\n107\n526\n83.1%\n51.4%\n73.9%\n\n\nUnknown\n9\n23\n32\n8\n5\n13\n88.9%\n21.7%\n40.6%\n\n\nTotal\n857\n466\n1323\n688\n131\n819\n80.3%\n28.1%\n61.9%",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-Motivation Data Story"
    ]
  },
  {
    "objectID": "w08_MotivationDataStory.html#table-2-death-rates-by-economic-status-and-age",
    "href": "w08_MotivationDataStory.html#table-2-death-rates-by-economic-status-and-age",
    "title": "Data Tell Stories – Statistics will help Unveiling these Stories",
    "section": "2.2 Table 2: Death Rates by Economic Status and Age",
    "text": "2.2 Table 2: Death Rates by Economic Status and Age\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEconomic Status\nPopulation Exposed to Risk\n\n\nNumber of Deaths\n\n\nDeaths per 100 Exposed to Risk\n\n\n\n\n\n\n\nMale\nFemale\nBoth\nMale\nFemale\nBoth\nMale\nFemale\nBoth\n\n\nI (high)\n172\n132\n304\n111\n6\n117\n64.5%\n4.5%\n38.5%\n\n\nII\n172\n103\n275\n150\n13\n163\n87.2%\n12.6%\n59.3%\n\n\nIII\n504\n208\n712\n419\n107\n526\n83.1%\n51.4%\n73.9%\n\n\nUnknown\n9\n23\n32\n8\n5\n13\n88.9%\n21.7%\n40.6%\n\n\nTotal\n857\n466\n1323\n688\n131\n819\n80.3%\n28.1%\n61.9%",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-Motivation Data Story"
    ]
  },
  {
    "objectID": "w08_MotivationDataStory.html#table-3-population-at-risk-deaths-and-death-rates-for-the-sinking-of-the-titanic",
    "href": "w08_MotivationDataStory.html#table-3-population-at-risk-deaths-and-death-rates-for-the-sinking-of-the-titanic",
    "title": "Data Tell Stories – Statistics will help Unveiling these Stories",
    "section": "5.1 Table 3: Population at Risk, Deaths, and Death Rates for the Sinking of the Titanic",
    "text": "5.1 Table 3: Population at Risk, Deaths, and Death Rates for the Sinking of the Titanic\n\n5.1.1 By Economic Status and Sex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEconomic Status\nPopulation Exposed to Risk\n\n\nNumber of Deaths\n\n\nDeaths per 100 Exposed to Risk\n\n\n\n\n\n\n\nMale\nFemale\nBoth\nMale\nFemale\nBoth\nMale\nFemale\nBoth\n\n\nI (high)\n180\n145\n325\n118\n4\n122\n65\n3\n37\n\n\nII\n179\n106\n285\n154\n13\n167\n87\n12\n59\n\n\nIII\n510\n196\n706\n422\n106\n528\n83\n54\n73\n\n\nCrew\n862\n23\n885\n670\n3\n673\n78\n13\n76\n\n\nTotal\n1731\n470\n2201\n1364\n126\n1490\n80\n27\n67\n\n\n\n\n\n5.1.2 By Economic Status and Age\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEconomic Status\nPopulation Exposed to Risk\n\n\nNumber of Deaths\n\n\nDeaths per 100 Exposed to Risk\n\n\n\n\n\n\n\nAdult\nChild\nBoth\nAdult\nChild\nBoth\nAdult\nChild\nBoth\n\n\nI (high)\n319\n6\n325\n122\n0\n122\n38\n0\n37\n\n\nII\n261\n24\n285\n167\n0\n167\n64\n0\n59\n\n\nIII\n627\n79\n706\n476\n52\n528\n76\n66\n73\n\n\nCrew\n885\n0\n885\n673\n0\n673\n76\n-\n76\n\n\nTotal\n2092\n109\n2201\n1438\n52\n1490\n69\n48\n67",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-Motivation Data Story"
    ]
  },
  {
    "objectID": "w08_MotivationDataStory.html#lifeboat-launch-time-number-of-occupants",
    "href": "w08_MotivationDataStory.html#lifeboat-launch-time-number-of-occupants",
    "title": "Data Tell Stories – Statistics will help Unveiling these Stories",
    "section": "8.1 Lifeboat Launch Time & Number of Occupants",
    "text": "8.1 Lifeboat Launch Time & Number of Occupants\n\n\n\nBoat Deck of the Titanic and Location of Lifeboats\n\n\n*Floated off the deck half swamped; swimmers scrambled aboard.\n**Floated off upside down; swimmers scrambled onto its back.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-Motivation Data Story"
    ]
  },
  {
    "objectID": "w08_MotivationDataStory.html#officers-in-charge-and-interpretation-of-loading-the-captains-order",
    "href": "w08_MotivationDataStory.html#officers-in-charge-and-interpretation-of-loading-the-captains-order",
    "title": "Data Tell Stories – Statistics will help Unveiling these Stories",
    "section": "8.2 Officers in Charge and Interpretation of Loading the Captain’s Order",
    "text": "8.2 Officers in Charge and Interpretation of Loading the Captain’s Order\n\nFirst Officer Murdoch for starboard side: women and children first\nSecond Officer Lightroller for port side: women and children only\n\n\n\n\nBoat Deck of the Titanic and Location of Lifeboats\n\n\nDiagram from “Olympic & Titanic: Ocean Liners of the Past”\nLifeboat Layout: - Port side (left): Lifeboats 16, 14, 12, 10, 8, 6, 4, 2 - Starboard side (right): Lifeboats 15, 13, 11, 9, 7, 5, 3, 1 - Collapsible A is on the roof of the Officers Quarters, above Collapsible C - Collapsible B is above D",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-Motivation Data Story"
    ]
  },
  {
    "objectID": "w08_MotivationDataStory.html#patterns-revealed",
    "href": "w08_MotivationDataStory.html#patterns-revealed",
    "title": "Data Tell Stories – Statistics will help Unveiling these Stories",
    "section": "10.1 Patterns Revealed",
    "text": "10.1 Patterns Revealed\n\nGender Disparity: Female death rate (28.1%) was dramatically lower than male death rate (80.3%)\nEconomic Class Effect:\n\nFirst class passengers had the lowest death rate (38.5%)\nThird class passengers had the highest death rate (73.9%)\n\nChildren’s Survival:\n\nNo children died in first and second class\n73.1% death rate among third class children\n\n“Women and Children First”: The data strongly supports that this protocol was followed, but with significant class bias",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-Motivation Data Story"
    ]
  },
  {
    "objectID": "w08_MotivationDataStory.html#statistical-lessons",
    "href": "w08_MotivationDataStory.html#statistical-lessons",
    "title": "Data Tell Stories – Statistics will help Unveiling these Stories",
    "section": "10.2 Statistical Lessons",
    "text": "10.2 Statistical Lessons\n\nData can tell powerful stories when properly analyzed\nCross-tabulation reveals patterns invisible in aggregate statistics\nMultiple variables (sex, age, economic class) interact to determine outcomes\nHistorical context is essential for interpretation",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk08-Motivation Data Story"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html",
    "href": "w10_Chapt13MultipleRegression.html",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "",
    "text": "Multiple regression is a logical extension of bivariate regression analysis. It combines several independent variables through a linear combination in the regression coefficients and allows performing predictions of the dependent variable.\n\nExample: Prediction equation with three independent variables\n\\[\\hat{Y} \\leftarrow b_0 + b_1 \\cdot X_1 + b_2 \\cdot X_2 + b_3 \\cdot X_3\\]\n\nMultiple regression allows to control for any confounding effects that a variable may have on the dependent and other independent variables in the model.\nMultiple regression combines the influences of several independent variables on a dependent variable.\n\nThe measurement units of the independent variables can differ from each other and the dependent variable.\nThe sign and magnitude of the associate weighting factors, i.e., the regression coefficient \\(b_k\\), simultaneously:\n\ncompensate for the differences in the measurement scales and\ncaptures any influence of the independent variable on the dependent variable.\n\n\nControl for Confounding: Revisit the Stork example\n\n\n\n\n\n\n\n\n\n\nStork Example: Relationships between variables\n\n\n\n\n\n\n\nStork Example: Relationships between variables\n\n\n\n\n\n\n\nStork Example: Relationships between variables\n\n\n\n\nBivariate Regression Model Output:\nlm(formula = birth ~ stork, data = myStork)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.939020   0.168849   11.48 1.64e-08 ***\nstork       0.046458   0.002807   16.55 1.37e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.317 on 14 degrees of freedom\nMultiple R-squared:  0.9514,    Adjusted R-squared:  0.9479 \nF-statistic:   274 on 1 and 14 DF,  p-value: 1.372e-10\n\n\n\n\nlm(formula = birth ~ stork + decade, data = myStork)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 45.640053  10.678527   4.274 0.000906 ***\nstork        0.010918   0.008895   1.227 0.241429    \ndecade      -0.022300   0.005449  -4.093 0.001270 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2175 on 13 degrees of freedom\nMultiple R-squared:  0.9788,    Adjusted R-squared:  0.9755 \nF-statistic: 299.5 on 2 and 13 DF,  p-value: 1.338e-11",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#introduction",
    "href": "w10_Chapt13MultipleRegression.html#introduction",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "",
    "text": "Multiple regression is a logical extension of bivariate regression analysis. It combines several independent variables through a linear combination in the regression coefficients and allows performing predictions of the dependent variable.\n\nExample: Prediction equation with three independent variables\n\\[\\hat{Y} \\leftarrow b_0 + b_1 \\cdot X_1 + b_2 \\cdot X_2 + b_3 \\cdot X_3\\]\n\nMultiple regression allows to control for any confounding effects that a variable may have on the dependent and other independent variables in the model.\nMultiple regression combines the influences of several independent variables on a dependent variable.\n\nThe measurement units of the independent variables can differ from each other and the dependent variable.\nThe sign and magnitude of the associate weighting factors, i.e., the regression coefficient \\(b_k\\), simultaneously:\n\ncompensate for the differences in the measurement scales and\ncaptures any influence of the independent variable on the dependent variable.\n\n\nControl for Confounding: Revisit the Stork example",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#bivariate-regression-model",
    "href": "w10_Chapt13MultipleRegression.html#bivariate-regression-model",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "",
    "text": "Stork Example: Relationships between variables\n\n\n\n\n\n\n\nStork Example: Relationships between variables\n\n\n\n\n\n\n\nStork Example: Relationships between variables\n\n\n\n\nBivariate Regression Model Output:\nlm(formula = birth ~ stork, data = myStork)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.939020   0.168849   11.48 1.64e-08 ***\nstork       0.046458   0.002807   16.55 1.37e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.317 on 14 degrees of freedom\nMultiple R-squared:  0.9514,    Adjusted R-squared:  0.9479 \nF-statistic:   274 on 1 and 14 DF,  p-value: 1.372e-10",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#multiple-regression-model-controlling-for-decade",
    "href": "w10_Chapt13MultipleRegression.html#multiple-regression-model-controlling-for-decade",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "",
    "text": "lm(formula = birth ~ stork + decade, data = myStork)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 45.640053  10.678527   4.274 0.000906 ***\nstork        0.010918   0.008895   1.227 0.241429    \ndecade      -0.022300   0.005449  -4.093 0.001270 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2175 on 13 degrees of freedom\nMultiple R-squared:  0.9788,    Adjusted R-squared:  0.9755 \nF-statistic: 299.5 on 2 and 13 DF,  p-value: 1.338e-11",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#variables-in-the-model",
    "href": "w10_Chapt13MultipleRegression.html#variables-in-the-model",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "3.1 Variables in the Model",
    "text": "3.1 Variables in the Model\n\\[SAT = f(Expend, PctSAT)\\]\n\nSAT: The state’s average SAT score for all those student who took the exam.\nExpend: The average per student state’s educational expenditure.\nPctSAT: Percentage of students within a state who took the SAT exam.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#comments-on-selection-bias",
    "href": "w10_Chapt13MultipleRegression.html#comments-on-selection-bias",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "3.2 Comments on Selection Bias",
    "text": "3.2 Comments on Selection Bias\nWe can assume that only students aiming at attending prestigious universities prefer to take the exam unless they are forced to do so by state law.\nTherefore we have a selection bias:\n\na low percentage of state’s participation means that predominately students with better academic achievements will take the test leading to a higher test score;\nwhereas, a high percentage of state’s participation implies that larger fraction of mediocre performing students will also take the test most likely leading to a lower state’s score.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#bivariate-models",
    "href": "w10_Chapt13MultipleRegression.html#bivariate-models",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "5.1 Bivariate Models",
    "text": "5.1 Bivariate Models\n\\[\\widehat{SAT} = 1089.3 - 20.9 \\cdot Expend\\]\n\\[\\widehat{SAT} = 1053.3 - 2.5 \\cdot PctSAT\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#key-observations",
    "href": "w10_Chapt13MultipleRegression.html#key-observations",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "5.2 Key Observations",
    "text": "5.2 Key Observations\n\nCounterintuitive bivariate result: We expect that the expenditures in primary education will have a positive impact on the SAT scores but both variables are negatively correlated.\nIn the multiple model the variable EXPEND has a positive impact on the SAT scores.\nThe variable PCTSAT can be viewed as a confounder because it is jointly correlated with the dependent variable SAT and the independent variable EXPEND.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#results-of-the-multivariate-regression-model",
    "href": "w10_Chapt13MultipleRegression.html#results-of-the-multivariate-regression-model",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "5.3 Results of the Multivariate Regression Model",
    "text": "5.3 Results of the Multivariate Regression Model\nlm(formula = SAT ~ Expend + PctSAT, data = StateSchool)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 993.8317    21.8332  45.519  &lt; 2e-16 ***\nExpend       12.2865     4.2243   2.909  0.00553 ** \nPctSAT       -2.8509     0.2151 -13.253  &lt; 2e-16 ***\n\nResidual standard error: 32.46 on 47 degrees of freedom\nMultiple R-squared:  0.8195,    Adjusted R-squared:  0.8118 \nF-statistic: 106.7 on 2 and 47 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#interpretation",
    "href": "w10_Chapt13MultipleRegression.html#interpretation",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "5.4 Interpretation",
    "text": "5.4 Interpretation\n\nIn the multivariate model the confounding effect is clearly controlled and the regression coefficient \\(b_1 = 12.3\\) of the independent variable EXPEND expresses its sole effect on the SAT score without interference from the confounder PCTSAT.\nThe intercept just guarantees that the prediction surface goes through the mean point of SAT, EXPEND and PCTSAT.\nIt is meaningless here, because for zero participation rate, a SAT score would not exist.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#rationale-underlying-the-standardized-regression-coefficients",
    "href": "w10_Chapt13MultipleRegression.html#rationale-underlying-the-standardized-regression-coefficients",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "8.1 Rationale Underlying the Standardized Regression Coefficients",
    "text": "8.1 Rationale Underlying the Standardized Regression Coefficients\n\nThe common interpretation of a regression coefficient is that one unit change in \\(X_j\\) will lead to \\(b_j\\) units change in \\(Y\\).\nThus, the magnitude of an estimated regression coefficient \\(b_j\\) depends on the scale, e.g., pennies or dollars, of its associated variable \\(X_j\\).\nThe standardized regression coefficient makes the scale of each independent variable comparable, thus, identifying their importance.\nThe interpretation of the standardized regression coefficient is: one standard deviation in \\(X_j\\) will lead to \\(b_j^*\\) standard deviations change in \\(Y\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#selection-of-independent-variables",
    "href": "w10_Chapt13MultipleRegression.html#selection-of-independent-variables",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "8.2 Selection of Independent Variables",
    "text": "8.2 Selection of Independent Variables\n\nIn confirmatory analysis the objective is to build a meaningful and interpretable regression model, which is guided by an underlying theory.\nThus, it should include only relevant explanatory variables and be generalizable to other datasets.\nIn contrast, in exploratory analysis, such as machine learning, the objective is to build a well fitting predictive model.\nHere an automated selection procedure of good predictive variables becomes the objective.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#multicollinearity",
    "href": "w10_Chapt13MultipleRegression.html#multicollinearity",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "8.3 Multicollinearity",
    "text": "8.3 Multicollinearity\n\nA multicollinearity analysis allows us to identify redundant variables.\nIf two variables are highly correlated only one of them is needed because it comprises of the same information.\nHigh multicollinearity among the independent variables makes an estimated regression model unstable.\nThe identification of the unique contribution of the independent variables in a model, e.g., the partial regression coefficient, controls for the redundancy among the independent variables. See the stork and school expenditure examples.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#regression-diagnostics",
    "href": "w10_Chapt13MultipleRegression.html#regression-diagnostics",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "8.4 Regression Diagnostics",
    "text": "8.4 Regression Diagnostics\nRegression diagnostics allows:\n\nthe identification violations of the underlying model assumptions, e.g., independent identical distributed disturbances, and perhaps control for these violations.\nthe identification and treatment of influential observations and potential outliers that do not belong to the underlying study population.\nThe identification of potential model refinements, e.g., missing relevant variables and accounting for an underlying spatial or temporal pattern.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#deviation-from-linearity",
    "href": "w10_Chapt13MultipleRegression.html#deviation-from-linearity",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "8.5 Deviation from Linearity",
    "text": "8.5 Deviation from Linearity\n\nRegression analysis works well when the model is linear.\nSpecialized techniques are available to estimate an explicitly non-linear model.\nUnder specific conditions a model can be transformed to linearity before it is calibrated.\nHowever, transformations change the interpretation of the model.\nDifferent regression regimes in the data can be modelled with the use of dummy variables.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#the-spatial-link-matrix",
    "href": "w10_Chapt13MultipleRegression.html#the-spatial-link-matrix",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "9.1 The Spatial Link Matrix",
    "text": "9.1 The Spatial Link Matrix\n\nThe spatial connectivity matrix operationalizes the underlying structure of the potential spatial relationships among the observations.\nFor potential distance relationships we have the distance matrix (known from road atlases, perhaps using spherical distances)\nFor potential neighborhood relationships we must use a binary spatial connectivity matrix",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#example-encoding-a-spatial-tessellation-as-a-binary-connectivity-matrix",
    "href": "w10_Chapt13MultipleRegression.html#example-encoding-a-spatial-tessellation-as-a-binary-connectivity-matrix",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "9.2 Example: Encoding a Spatial Tessellation as a Binary Connectivity Matrix",
    "text": "9.2 Example: Encoding a Spatial Tessellation as a Binary Connectivity Matrix\n\n\n\nSpatial tessellation of 9 hexagonal cells with underlying connectivity structure &lt;=&gt; Binary 9x9 spatial connectivity matrix B\n\n\n\nFor study areas with a large number \\(n\\) of individual regions the generation of the connectivity matrix \\(\\mathbf{B}\\) (or distance matrix) must be left to a GIS program. The connectivity matrix has \\(n \\times n\\) elements.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#the-applied-regression-model-to-explain-the-total-fertility-rate",
    "href": "w10_Chapt13MultipleRegression.html#the-applied-regression-model-to-explain-the-total-fertility-rate",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "10.1 The Applied Regression Model to Explain the Total Fertility Rate",
    "text": "10.1 The Applied Regression Model to Explain the Total Fertility Rate\nlm(formula = TOTFERTRAT ~ FEMMARAGE9 + DIVORCERAT + log(ILLITERRAT) + \n    TELEPERFAM, data = prov.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.19958 -0.05474 -0.01284  0.05272  0.42922 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      4.78139    0.48606   9.837 6.23e-16 ***\nFEMMARAGE9      -0.09647    0.02050  -4.706 9.11e-06 ***\nDIVORCERAT      -0.11839    0.05772  -2.051   0.0431 *  \nlog(ILLITERRAT)  0.03072    0.01707   1.799   0.0753 .  \nTELEPERFAM      -1.28499    0.18078  -7.108 2.69e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1047 on 90 degrees of freedom\nMultiple R-squared:  0.8051,    Adjusted R-squared:  0.7965 \nF-statistic: 92.96 on 4 and 90 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nItaly fertility analysis: Scatterplot matrix (left) and Studentized Residual Map (right)\n\n\n\n\n\n\n\nItaly fertility analysis: Scatterplot matrix (left) and Studentized Residual Map (right)\n\n\n\n\n\nThe map pattern of the residuals \\(e_i = y_i - \\hat{y}_i\\) indicates spatial clustering of negative and positive values and thus positive spatial autocorrelation.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w10_Chapt13MultipleRegression.html#exploratory-visualization",
    "href": "w10_Chapt13MultipleRegression.html#exploratory-visualization",
    "title": "Chapter 13: Multiple Regression Analysis",
    "section": "12.1 Exploratory Visualization",
    "text": "12.1 Exploratory Visualization\n\nThe observed residuals \\(e_i\\) can be plotted against the average values of their neighboring values \\(e_i^{avg}\\) using the Moran’s scatterplot.\n\n\n\n\n\n\nMoran’s Scatterplot: Residuals vs Spatially Lagged Residuals. The positive slope indicates positive spatial autocorrelation. Several provinces are labeled including Lecce, Brindisi, Taranto, Bolzano-Bozen, Trento, Foggia, Rovigo, Venezia, and Catanzaro.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk10-ch13.Multiple Regression Analysis"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html",
    "href": "w11_Chapter05B.html",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "",
    "text": "Definition: Probability Distribution or Function (p 211). A table, graph, or mathematical function that describes all potential outcomes \\(x\\) of the random variable \\(X\\) from its underlying population and their corresponding probabilities.\nNotation: \\(\\Pr(X = x)\\) where \\(x\\) is an outcome of random variable \\(X\\).\nRecall: these probabilities following Kolmogorov’s axioms.\n\n[a] \\(0 \\leq \\Pr(X = x) \\leq 1\\)\n[b] \\(\\Pr(a \\leq X \\leq b) = \\sum_{x \\in [a,b]} \\Pr(X = x)\\)\n[c] \\(\\Pr\\left(\\sum_{x \\in \\mathcal{X}} \\Pr(X = x)\\right) = 1\\) because the set of all elementary outcomes are mutually exclusive and exhaustively cover the sample space \\(\\mathcal{X}\\) (all possible outcomes) of the random variable \\(X\\).\n\n\n\n\n\n\n\nTABLE 6-2: Relative Frequency Probabilities for the Random Variable Called Household Size of Table 6-1\n\n\n\n\n\nFIGURE 6-1: A probability mass function\n\n\n\n\n\nIf the individual data values \\(x_i\\) are sorted ascending, i.e., \\(x_1 &lt; x_2 &lt; \\ldots &lt; x_n\\), then we can express the cumulative probability function, denoted by a capital \\(F()\\), as:\n\\[F(x_i) = \\sum_{X \\leq x_i} \\Pr(X = x_i) = \\Pr(X = x_1) + \\Pr(X = x_2) + \\cdots + \\Pr(X = x_i)\\]\nNote: we always can retrieve the individual probabilities from the cumulative probability function:\n\\[\\Pr(X = x_1) = F(x_1)\\] \\[\\Pr(X = x_2) = F(x_2) - F(x_1)\\] \\[\\vdots\\] \\[\\Pr(X = x_n) = F(x_n) - F(x_{n-1})\\]\n\n\n\nTABLE 6-4: Cumulative Mass Function\n\n\n\n\n\nFIGURE 6-2: A cumulative probability mass function\n\n\n\n\n\n\n\n\nThe expected value and the variance are population counterparts to the sample statistics of the estimated mean and estimated variance.\nNote that the summation must proceed over the complete range of possible data values of \\(X\\).\n\n\n\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, \\ldots, x_k\\), its expected value is:\n\\[E(X) = \\sum_{i=1}^{k} x_i \\cdot \\Pr(X = x_i)\\]\n\n\n\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, \\ldots, x_k\\), its variance is:\n\\[Var(X) = \\sum_{i=1}^{k} \\left[x_i - E(X)\\right]^2 \\cdot \\Pr(X = x_i)\\]\n\\[= \\sum_{i=1}^{k} x_i^2 \\cdot \\Pr(X = x_i) - \\left[E(X)\\right]^2\\]\n\nIn both expressions the constant observation weight factor \\(\\frac{1}{n}\\) \\(\\forall i\\), that has been used when calculating the sample means and variances, has been replaced by the probability \\(\\Pr(X = x_i)\\)\n\n\n\n\n\n\nSince we are dealing with an infinite number of representations of a random variable in its support along the real domain \\([a, b]\\) with the possible bounds \\(-\\infty \\leq a\\) and \\(b \\leq \\infty\\), it follows that:\n\nThe probability of an individual representation \\(X = x_i\\) becomes zero, that is, \\(\\Pr(X = x_i) = 0\\).\nOtherwise, the probability over all representations would sum to a value larger than one.\nThe individual probabilities are replaced by the continuous probability density function \\(f(x)\\) with:\n\n\\[f(x) = \\begin{cases} &gt; 0 & \\text{for } x \\in [a, b] \\\\ 0 & \\text{for } x \\notin [a, b] \\end{cases}\\]\nand the area under the density function has to integrate to one: \\(\\int_a^b f(x) \\cdot dx = 1\\).\n\nProbabilities of subsets \\([c, d]\\) with \\(a \\leq c &lt; d \\leq b\\) can still be expressed by integrals:\n\n\\[\\Pr(c \\leq X \\leq d) = \\int_c^d f(x) \\cdot dx\\]\n\nNote: An integral is the area underneath the function \\(f(x)\\) within the interval from \\(a\\) to \\(b\\).\n\n\n\nNote: the density function at a given values \\(X = x\\) can be larger than one, i.e., \\(f(x) &gt; 1\\).\nExample: \\(X \\sim U(0.0, 0.5)\\) (read: the random variable \\(X\\) is distributed according to a uniform distribution with a support starting at 0 and ending at 0.5).\nIts density function is: \\[f(x) = \\begin{cases} 2 & x \\in [0.0, 0.5] \\\\ 0 & \\text{otherwise} \\end{cases} \\text{ with } \\int_a^b f(x) \\cdot dx = 1\\]\n\n\n\nIts cumulative distribution function becomes: \\[F(x) = \\Pr(X \\leq x) = \\int_{-\\infty}^{x} f(x) \\cdot dx\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#probability-density-and-distribution-for-discrete-random-variables",
    "href": "w11_Chapter05B.html#probability-density-and-distribution-for-discrete-random-variables",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "",
    "text": "Definition: Probability Distribution or Function (p 211). A table, graph, or mathematical function that describes all potential outcomes \\(x\\) of the random variable \\(X\\) from its underlying population and their corresponding probabilities.\nNotation: \\(\\Pr(X = x)\\) where \\(x\\) is an outcome of random variable \\(X\\).\nRecall: these probabilities following Kolmogorov’s axioms.\n\n[a] \\(0 \\leq \\Pr(X = x) \\leq 1\\)\n[b] \\(\\Pr(a \\leq X \\leq b) = \\sum_{x \\in [a,b]} \\Pr(X = x)\\)\n[c] \\(\\Pr\\left(\\sum_{x \\in \\mathcal{X}} \\Pr(X = x)\\right) = 1\\) because the set of all elementary outcomes are mutually exclusive and exhaustively cover the sample space \\(\\mathcal{X}\\) (all possible outcomes) of the random variable \\(X\\).\n\n\n\n\n\n\n\nTABLE 6-2: Relative Frequency Probabilities for the Random Variable Called Household Size of Table 6-1\n\n\n\n\n\nFIGURE 6-1: A probability mass function\n\n\n\n\n\nIf the individual data values \\(x_i\\) are sorted ascending, i.e., \\(x_1 &lt; x_2 &lt; \\ldots &lt; x_n\\), then we can express the cumulative probability function, denoted by a capital \\(F()\\), as:\n\\[F(x_i) = \\sum_{X \\leq x_i} \\Pr(X = x_i) = \\Pr(X = x_1) + \\Pr(X = x_2) + \\cdots + \\Pr(X = x_i)\\]\nNote: we always can retrieve the individual probabilities from the cumulative probability function:\n\\[\\Pr(X = x_1) = F(x_1)\\] \\[\\Pr(X = x_2) = F(x_2) - F(x_1)\\] \\[\\vdots\\] \\[\\Pr(X = x_n) = F(x_n) - F(x_{n-1})\\]\n\n\n\nTABLE 6-4: Cumulative Mass Function\n\n\n\n\n\nFIGURE 6-2: A cumulative probability mass function",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#expectation-and-variance-of-discrete-random-variables",
    "href": "w11_Chapter05B.html#expectation-and-variance-of-discrete-random-variables",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "",
    "text": "The expected value and the variance are population counterparts to the sample statistics of the estimated mean and estimated variance.\nNote that the summation must proceed over the complete range of possible data values of \\(X\\).\n\n\n\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, \\ldots, x_k\\), its expected value is:\n\\[E(X) = \\sum_{i=1}^{k} x_i \\cdot \\Pr(X = x_i)\\]\n\n\n\nFor a discrete random variable \\(X\\) with values \\(x_1, x_2, \\ldots, x_k\\), its variance is:\n\\[Var(X) = \\sum_{i=1}^{k} \\left[x_i - E(X)\\right]^2 \\cdot \\Pr(X = x_i)\\]\n\\[= \\sum_{i=1}^{k} x_i^2 \\cdot \\Pr(X = x_i) - \\left[E(X)\\right]^2\\]\n\nIn both expressions the constant observation weight factor \\(\\frac{1}{n}\\) \\(\\forall i\\), that has been used when calculating the sample means and variances, has been replaced by the probability \\(\\Pr(X = x_i)\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#probability-density-and-distribution-for-continuous-random-variables",
    "href": "w11_Chapter05B.html#probability-density-and-distribution-for-continuous-random-variables",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "",
    "text": "Since we are dealing with an infinite number of representations of a random variable in its support along the real domain \\([a, b]\\) with the possible bounds \\(-\\infty \\leq a\\) and \\(b \\leq \\infty\\), it follows that:\n\nThe probability of an individual representation \\(X = x_i\\) becomes zero, that is, \\(\\Pr(X = x_i) = 0\\).\nOtherwise, the probability over all representations would sum to a value larger than one.\nThe individual probabilities are replaced by the continuous probability density function \\(f(x)\\) with:\n\n\\[f(x) = \\begin{cases} &gt; 0 & \\text{for } x \\in [a, b] \\\\ 0 & \\text{for } x \\notin [a, b] \\end{cases}\\]\nand the area under the density function has to integrate to one: \\(\\int_a^b f(x) \\cdot dx = 1\\).\n\nProbabilities of subsets \\([c, d]\\) with \\(a \\leq c &lt; d \\leq b\\) can still be expressed by integrals:\n\n\\[\\Pr(c \\leq X \\leq d) = \\int_c^d f(x) \\cdot dx\\]\n\nNote: An integral is the area underneath the function \\(f(x)\\) within the interval from \\(a\\) to \\(b\\).\n\n\n\nNote: the density function at a given values \\(X = x\\) can be larger than one, i.e., \\(f(x) &gt; 1\\).\nExample: \\(X \\sim U(0.0, 0.5)\\) (read: the random variable \\(X\\) is distributed according to a uniform distribution with a support starting at 0 and ending at 0.5).\nIts density function is: \\[f(x) = \\begin{cases} 2 & x \\in [0.0, 0.5] \\\\ 0 & \\text{otherwise} \\end{cases} \\text{ with } \\int_a^b f(x) \\cdot dx = 1\\]\n\n\n\nIts cumulative distribution function becomes: \\[F(x) = \\Pr(X \\leq x) = \\int_{-\\infty}^{x} f(x) \\cdot dx\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#uniform-distribution-skipped",
    "href": "w11_Chapter05B.html#uniform-distribution-skipped",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "2.1 Uniform Distribution (skipped)",
    "text": "2.1 Uniform Distribution (skipped)\nFor \\(k\\) representations numbered \\(x \\in \\{1, 2, \\ldots, k\\}\\) with equal probabilities, the probability function becomes:\n\\[\\Pr(X = x \\mid k) = \\frac{1}{k} \\text{ with } x \\in \\{1, 2, \\ldots, k\\}\\]\n\n2.1.1 Expectation\n\\[E(X) = \\sum_{x=1}^{k} x \\cdot \\Pr(X = x \\mid k) = \\sum_{x=1}^{k} x \\cdot \\frac{1}{k}\\]\n\\[= \\frac{1}{k} \\cdot \\left[\\frac{k \\cdot (k+1)}{2}\\right] = \\frac{k+1}{2}\\]\n\n\n2.1.2 Variance\n\\[Var(X) = \\frac{k^2 - 1}{12}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#binomial-distribution",
    "href": "w11_Chapter05B.html#binomial-distribution",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "2.2 Binomial Distribution",
    "text": "2.2 Binomial Distribution\n\n2.2.1 Underlying Assumptions of the Binomial Distribution Model\n[a] There are \\(n\\) independent trials of an experiment, that is, the outcome of previous trials will not influence the outcome of current or future trials (i.e., sampling with replacement).\n[b] Only binary outcomes (e.g., success/failure, head/tail, male/female, 0/1, etc.) are possible at each trial.\n[c] The probabilities \\(\\pi\\) for successes and the complement probability \\(1 - \\pi\\) for failures remain constant for all trials.\n[d] The random variable \\(S\\) is the total number of “successes” (sum of the individual successes) after \\(n\\) trials have been completed, that is, \\(0 \\leq S \\leq n\\).\n\n\n2.2.2 Bernoulli Distribution\nEach individual trial follows a Bernoulli distribution (just one trial, i.e., \\(n = 1\\)):\n\\[\\Pr(S = s) = \\pi^s \\cdot (1 - \\pi)^{n-s} \\text{ with } s = \\begin{cases} 1 & \\text{for either a success} \\\\ 0 & \\text{or for a failure} \\end{cases}\\]\nThis reduces to \\(\\Pr(S = 0) = 1 \\cdot (1 - \\pi)^1\\) and \\(\\Pr(S = 1) = \\pi^1 \\cdot 1\\) because \\(x^0 = 1\\).\n\n\n2.2.3 Example: Two Independent Trials\nThe joint probability of \\(\\{S_1 = 0 \\cap S_2 = 1\\}\\), that is one success \\(S = 1\\) in two independent trails (allows multiplication of probabilities) and \\(n = 2\\), becomes:\n\\[\\Pr(S_1 = 0 \\cap S_2 = 1) = \\left[\\underbrace{\\pi^0}_{=1} \\cdot \\underbrace{(1-\\pi)^{1-0}}_{=1-\\pi}\\right] \\cdot \\left[\\underbrace{\\pi^1}_{=\\pi} \\cdot \\underbrace{(1-\\pi)^{1-1}}_{=1}\\right]\\]\nThis probability is identical for the individual event \\(\\{S_1 = 1 \\cap S_2 = 0\\}\\), therefore, order does not matter.\n\n\n2.2.4 Mutually Exclusive Events\nThe events \\(\\{S_1 = 0 \\cap S_2 = 1\\}\\) and \\(\\{S_1 = 1 \\cap S_2 = 0\\}\\) are both mutually exclusive (thus their probabilities can be summed up), thus:\n\\[\\Pr(X = 1 \\mid n = 2, \\pi) = \\Pr(S_1 = 0 \\cap S_2 = 1) + \\Pr(S_1 = 1 \\cap S_2 = 0)\\] \\[= 2 \\cdot \\pi \\cdot (1 - \\pi)\\]\n\n\n2.2.5 General Binomial Formula\nThe general equation for the binomial distribution takes the sum of successes from experiments with a different sequence of successes into account:\n\\[\\Pr(X = x \\mid n, \\pi) = \\frac{n!}{x! \\cdot (n-x)!} \\cdot \\pi^x \\cdot (1 - \\pi)^{n-x}\\]\n\n\n2.2.6 Example: Coin Toss Outcomes\n\n\n\nTABLE 6-7: Possible Outcomes of the Coin Toss Experiment\n\n\n\n\n\nTABLE 6-9: Binomial Distributions\n\n\n\n\n2.2.7 Expectation and Variance of Binomial Distribution\n\nExpectation: \\(E(X) = n \\cdot \\pi\\)\nVariance: \\(Var(X) = n \\cdot \\pi \\cdot (1 - \\pi)\\)\n\n\n\n2.2.8 Example: Binomial Distribution\nExamples of the binomial distribution can be generated with the R script BinomPoisson.r:\n\\(X \\sim Binomial(\\pi = 0.3, n = 6)\\)\n\n\n\nCount\nPr(Count)\n\n\n\n\n0\n0.117649\n\n\n1\n0.302526\n\n\n2\n0.324135\n\n\n3\n0.185220\n\n\n4\n0.059535\n\n\n5\n0.010206\n\n\n6\n0.000729\n\n\n\n\n\n\nBinominal Density Function & Binominal Distribution Function\n\n\nExamples of Binominal Distribution Functions: \\(X \\sim Binomial(\\pi = 0.3, n = 60)\\)\n\n\n\nBinomial Density Function & Binomial Distribution Function\n\n\nNote that for large \\(n\\), the discrete density almost looks like being almost continuous normal distributed.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#the-poisson-distribution",
    "href": "w11_Chapter05B.html#the-poisson-distribution",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "2.3 The Poisson Distribution",
    "text": "2.3 The Poisson Distribution\n\nImportant probability model for counting and queuing processes\n\n\n2.3.1 Underlying Assumptions\n[a] The total number of events in two mutually exclusive intervals is independent\n[b] The probability of just one event in a small interval is small and proportional to the length of the interval, i.e., the event is rare\n[c] The probability of two or more events in a small interval is near zero\n\n\n2.3.2 Density Function\n\\[\\Pr(X = x \\mid \\lambda) = \\frac{\\exp(-\\lambda) \\cdot \\lambda^x}{x!} \\text{ with } x \\in \\{0, 1, 2, \\ldots\\}\\]\n\n\n2.3.3 Expectation and Variance\nThe expectation and variance are equal: \\[E(X) = Var(X) = \\lambda\\]\n\n\n2.3.4 Derivation from Binomial Distribution\nThe Poisson distribution can be derived from the Binomial Distribution (see BBR, pp 228-230 and the script BinomApproachPoisson.r) as the number of trials moves to infinity, i.e., \\(n \\to \\infty\\), under the assumptions:\n[a] The number of intervals \\(n\\) per unit \\(U\\) increases → the intervals become shorter \\(U/n\\).\n[b] However, the probability of one event proportionally decreases \\(\\Pr(X = 1) \\sim U/n\\).\nNote however, the expectation remains fixed, i.e., \\(E(X) = n \\cdot \\pi = \\text{constant}\\).\nExamples of Poisson Distributions for \\(\\lambda \\in \\{0.5, 1.0, 5.0\\}\\)\n\n\n\nExamples of Poisson Distributions",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#other-frequently-encountered-discrete-distributions-skipped",
    "href": "w11_Chapter05B.html#other-frequently-encountered-discrete-distributions-skipped",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "2.4 Other Frequently Encountered Discrete Distributions (skipped)",
    "text": "2.4 Other Frequently Encountered Discrete Distributions (skipped)\n\n2.4.1 Geometric Distribution\nNumber of Bernoulli trials before the first success occurs.\n\n\n2.4.2 Negative Binomial Distribution\nNumber of Bernoulli trials is \\(x + r\\) until the \\(x^{th}\\) success occurs with \\(r\\) being the random number of failures. Therefore, the total number of experiments is not fixed.\nCan be viewed as a special case of the Binomial distribution where the number of trials is not fixed:\n\\[\\Pr(R = r \\mid \\pi, x) = \\binom{r + x - 1}{x - 1} \\cdot (1 - \\pi)^r \\cdot \\underbrace{\\pi^{x-1}}_{r \\text{ failures and } x-1 \\text{ successes}} \\cdot \\underbrace{\\pi}_{x^{th} \\text{ success}}\\]\nThe minus one terms \\(r + x - 1\\) and \\(x - 1\\) appear because the order of outcomes for the first \\(r + x - 1\\) experiments is irrelevant. The last experiment must be a success and, therefore, the order of the last experiment matters.\nThe negative binomial distribution is also a generalization of the Poisson distribution where the variance is allowed to be larger than the expectation.\n\n\n2.4.3 Multinomial Distribution\nMore than two classes are allowed. The probability is associated with the number of observed counts in each class given fixed class probabilities.\nThe Binomial distribution can be rewritten as:\n\\[\\Pr(X_1 = x_1, X_2 = x_2 \\mid \\pi_1, \\pi_2; n) = \\frac{n!}{x_1! \\cdot x_2!} \\cdot \\pi_1^{x_1} \\cdot \\pi_2^{x_2}\\]\nunder the constraints \\(\\pi_1 + \\pi_2 = 1\\) and \\(x_1 + x_2 = n\\).\nAs long as these constraints are satisfied the probability for the counts \\(\\{x_1, x_2, \\ldots, x_k\\}\\) in \\(k\\) classes with the class probabilities \\(\\{\\pi_1, \\pi_2, \\ldots, \\pi_k\\}\\) is given by:\n\\[\\Pr(X_1 = x_1, X_2 = x_2, \\ldots, X_k = x_k \\mid \\pi_1, \\pi_2, \\ldots, \\pi_k; n) = \\frac{n!}{x_1! \\cdot x_2! \\cdots x_k!} \\cdot \\pi_1^{x_1} \\cdot \\pi_2^{x_2} \\cdots \\pi_k^{x_k}\\]\n\n\n2.4.4 Hypergeometric Distribution\nDescribes the distribution of number of samples \\(x_1\\) obtained from one group with \\(n_1\\) elements when sampling without replacement from two groups is considered. The second group has \\(n_2\\) elements and the total sample size is \\(x_1 + x_2 = k\\).\n\\[\\Pr(X_1 = x_1, X_2 = x_2 \\mid n_1, n_2; k) = \\frac{\\binom{n_1}{x_1} \\cdot \\binom{n_2}{x_2}}{\\binom{n_1 + n_2}{x_1 + x_2}}\\]\n\nNote: WIKIPEDIA provides good discussion of these discrete distributions.\n\n\n\n2.4.5 Example: GPS Units with Software Error\nA surveying company has 8 identical GPS units. Three of these units received a software update, which later was identified to lead to a systematic error in the elevation estimate. For redundancy and cross-evaluation purposes, each surveying team is using two instruments in the field.\nCalculate the probability that a surveying team did not spot the software error while doing measurements, that is, they were using two GPS units with faulty software updates?\nRefer to the probability rules that you applied in your calculations and justify their use.\nApproach 1: \\[\\Pr(S_1 = F, S_2 = F) = \\frac{3}{8} \\cdot \\frac{2}{7} = \\frac{6}{56} = 0.1071\\] → Sampling without replacement\nApproach 2: \\[\\Pr(S_1 = F, S_2 = F) = \\frac{|\\text{Event Space}|}{|\\text{Sample Space}|} = \\frac{C_2^3 \\cdot C_0^5}{C_2^8} = \\frac{3 \\cdot 1}{\\frac{8 \\cdot 7}{2}} = 0.1071\\] → Use of the hypergeometric rule and analytical definition of probabilities.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#the-uniform-distribution-skipped",
    "href": "w11_Chapter05B.html#the-uniform-distribution-skipped",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "3.1 The Uniform Distribution (skipped)",
    "text": "3.1 The Uniform Distribution (skipped)\nIt has two parameters describing its support. Its density is a constant density over its support.\n\n3.1.1 Density and Distribution Functions\n\\[f(x \\mid a, b) = \\begin{cases} \\frac{1}{b-a} & \\text{for } x \\in [a, b] \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\\[F(x \\mid a, b) = \\begin{cases} \\frac{x - a}{b - a} & \\text{for } x \\in [a, b] \\\\ 1 & \\text{for } x &gt; b \\\\ 0 & \\text{for } x &lt; a \\end{cases}\\]\n\n\n3.1.2 Expectation and Variance\n\nExpectation: \\(E(X) = \\frac{a + b}{2}\\)\nVariance: \\(Var(X) = \\frac{(b - a)^2}{12}\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#the-exponential-distribution",
    "href": "w11_Chapter05B.html#the-exponential-distribution",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "3.2 The Exponential Distribution",
    "text": "3.2 The Exponential Distribution\nIt is usually viewed as the continuous analog to the Poisson distribution and expresses the probability of the wait time between two events in a queue.\n\n3.2.1 Density Function\n\\[f(x \\mid \\lambda) = \\begin{cases} \\lambda \\cdot \\exp(-\\lambda \\cdot x) & \\text{for } x \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\n\n3.2.2 Distribution Function\n\\[F(x \\mid \\lambda) = \\begin{cases} 1 - \\exp(-\\lambda \\cdot x) & \\text{for } x \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\n\n3.2.3 Expectation and Variance\n\nExpectation: \\(E(X) = 1/\\lambda\\)\nVariance: \\(Var(X) = 1/\\lambda^2\\)\n\n\n\n3.2.4 Parameter Estimation\nThe parameter \\(\\lambda\\) can be estimated from sample observations by \\(\\hat{\\lambda} = 1/\\bar{x}\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#the-normal-or-gaussian-distribution",
    "href": "w11_Chapter05B.html#the-normal-or-gaussian-distribution",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "3.3 The Normal (or Gaussian) Distribution",
    "text": "3.3 The Normal (or Gaussian) Distribution\nOne of the most important distributions:\n[a] It emerges in many natural processes such as the sum of many small random effects (⇒ more when we deal with the central limit theorem in the sampling chapter)\n[b] Many other continuous distributions can be derived from the normal distribution (log-normal, \\(t\\)-, \\(F\\)-, and \\(\\chi^2\\)-distribution etc.)\n[c] Its multivariate (2 or more random variables jointly) equivalent has many elegant properties. E.g., marginal distributions again normal, the strength of a linear relationship between two random variables is related to their correlation coefficient etc.\n[d] Its conditional distribution remains a normal distribution.\n \n\n3.3.1 Shape of the Normal Distribution\nThe shape of the normal distribution is based on an exponential function \\(f(x) \\propto \\exp(-|x|^p)\\) with its argument \\(p\\) in the 2nd power:\n\n\n\nDistribution shapes with different powers: 1 (Double Exponential), 2 (Normal), 3, and 4\n\n\n\n\n3.3.2 Density Function\nThe univariate normal distribution \\(N(\\mu, \\sigma^2)\\) is characterized by two parameters, \\(\\mu\\) is the central location parameter, \\(\\sigma^2\\) is its spread parameter:\n\\[f(x) = \\frac{1}{\\sigma \\cdot \\sqrt{2\\pi}} \\cdot \\exp\\left[-\\frac{1}{2} \\cdot \\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\text{ with } E(X) = \\mu \\text{ and } Var(X) = \\sigma^2\\]\n\n\n3.3.3 Cumulative Distribution Function\nIts cumulative distribution function cannot be given in a closed form but must be derived through numerical integration:\n\\[F(x) = \\int_{-\\infty}^{x} \\frac{1}{\\sigma \\cdot \\sqrt{2\\pi}} \\cdot \\exp\\left[-\\frac{1}{2} \\cdot \\frac{(x - \\mu)^2}{\\sigma^2}\\right] \\cdot dx\\]\nNotice the upper bound \\(x\\) in the integral.\n\n\n3.3.4 Properties\n\nThe normal distribution is symmetric around its expected value, i.e., its skewness is zero and the mode, median and expectation are identical.\nIts kurtosis (balance between probability mass in the center and in the tails) is 0.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#the-z-transformation-for-comparing-the-shape-of-distributions",
    "href": "w11_Chapter05B.html#the-z-transformation-for-comparing-the-shape-of-distributions",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "3.4 The z-transformation for Comparing the Shape of Distributions",
    "text": "3.4 The z-transformation for Comparing the Shape of Distributions\nAny normal distributed variable \\(X \\sim N(\\mu, \\sigma^2)\\) can be expressed as a standard normal distributed variable \\(z(X) \\sim N(0,1)\\) using the transformation:\n\\[z(X) = \\frac{X - \\mu}{\\sigma}\\]\n\n\n\nFIGURE 6-15: Converting normal distributions to the standard normal\n\n\n\n3.4.1 Effect of z-transformation\n\nSubtracting \\(\\mu_X\\) or \\(\\bar{x}\\) in the numerator shifts the distribution over the value 0\nDividing by the numerator \\(\\sigma_X\\) or \\(s_X\\) rescales the distribution to a standard deviation of 1\n\n\n\n\nFigure 6.6: A normal distribution with various transformations on the abscissa\n\n\n\n\n3.4.2 Objective of z-transformation\nThe objective of the z-transformation is to transform a random variable \\(X\\) with the mean \\(\\mu_X\\) and variance \\(\\sigma_X^2\\) into a variable \\(Z\\) with mean \\(\\mu_Z = 0\\) and standard deviation \\(\\sigma_Z^2 = \\sigma_Z = 1\\) without changing the general shape of the distribution.\n\n\n3.4.3 Preserving Percentiles\nThis transformation does not change the percentiles \\(p(x_{[i]}) = p(z_{[i]})\\). Therefore, the distribution of metric variables measured in different units (e.g., Celsius versus Fahrenheit) become comparable.\n\n\n3.4.4 z-transformation Formulas\nThe linear transformation:\n\\[Z_i = \\frac{X_i - \\mu_X}{\\sigma_X} \\text{ for theoretical parameters or}\\]\n\\[z_i = \\frac{x_i - \\bar{x}}{s_X} \\text{ for estimated parameters}\\]\nachieves the desired task.\n\n\n3.4.5 Reverse Transformation\nIts reverse transformation back into the original units is:\n\\[X_i = \\sigma_X \\cdot Z_i + \\mu_X \\text{ or } x_i = s_X \\cdot z_i + \\bar{x}, \\text{ respectively}\\]\n\n\n3.4.6 R Function\nThe standard R function scale() performs the z-transformation for all numeric vectors in a data-frame or columns of a numeric matrix.\n\n\n3.4.7 Statistics Using z-transformed Variables\nSeveral statistics can be rewritten in terms of the z-transformed variable:\n\nSkewness: \\[\\text{skew}(x) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^3}{s^3} = \\sum_{i=1}^{n} z_i^3\\]\nKurtosis: \\[\\text{kurt}(x) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^4}{s^4} - 3 = \\sum_{i=1}^{n} z_i^4 - 3\\]\nCorrelation coefficient: \\[r_{12} = \\frac{\\sum_{i=1}^{n}(x_{i1} - \\bar{x}_1) \\cdot (x_{i2} - \\bar{x}_2)}{\\sqrt{\\sum_{i=1}^{n}(x_{i1} - \\bar{x}_1)^2 \\cdot \\sum_{i=1}^{n}(x_{i2} - \\bar{x}_2)^2}} = \\frac{\\sum_{i=1}^{n} z_{i1} \\cdot z_{i2}}{n - 1}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#other-important-continuous-distributions-skipped",
    "href": "w11_Chapter05B.html#other-important-continuous-distributions-skipped",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "3.5 Other Important Continuous Distributions (skipped)",
    "text": "3.5 Other Important Continuous Distributions (skipped)\n\n3.5.1 Gamma Distribution\nThe support is \\(0 \\leq X &lt; \\infty\\). It is a generalization of exponential distribution.\n\n\n3.5.2 Beta Distribution\nThe support is \\(0 &lt; X &lt; 1\\). It is used to model the distribution of probabilities and can take many different shapes depending on its underlying parameters, which can be estimated by the methods of moments.\n\n\n3.5.3 Log-normal Distribution\nIf \\(X \\sim N(\\mu_X, \\sigma_X^2)\\) then \\(Y = \\exp(X)\\) with \\(Y &gt; 0\\) follows a log-normal distribution with:\n\\[E(Y) = \\exp\\left(\\mu_X + \\frac{1}{2} \\cdot \\sigma_X^2\\right)\\]\n\\[Var(Y) = \\exp(2 \\cdot \\mu_X + \\sigma_X^2) \\cdot (\\exp(\\sigma_X^2) - 1)\\]\nAnalogy to the emergence of the normal distribution as a sum of many small random variables the log-normal distribution can be conceived of as a product of many small positive random effects.\n\n\n3.5.4 Chi-square (\\(\\chi^2\\)) Distribution\nLet \\(z_i\\) be \\(n\\) independent standard normal distributed variables, then:\n\\[\\chi^2_{df=n} = \\sum_{i=1}^{n} z_i^2\\]\nis a \\(\\chi^2\\) distributed random variable with \\(n\\) degrees of freedom.\n\n\n3.5.5 t Distribution\nLet \\(z\\) and \\(z_i\\) be \\(n + 1\\) independent standard normal distributed variables, then:\n\\[t_{df=n} = \\frac{z}{\\sqrt{\\frac{\\sum_{i=1}^{n} z_i^2}{n}}}\\]\nis a \\(t\\) distributed random variable with \\(n\\) degrees of freedom.\n\n\n3.5.6 F Distribution\nLet \\(z_i\\) and \\(z_j\\) be \\(n\\) and \\(m\\) distributed standard normal distributed variables, then:\n\\[F_{df_1=n, df_2=m} = \\frac{\\frac{\\sum_{i=1}^{n} z_i^2}{n}}{\\frac{\\sum_{j=1}^{m} z_j^2}{m}}\\]\nis a \\(F\\) distributed random variable with \\(n\\) and \\(m\\) degrees of freedom.\n\nAgain see Wikipedia for these and other continuous distributions.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#expectation-and-variance-of-continuous-distributions-skipped",
    "href": "w11_Chapter05B.html#expectation-and-variance-of-continuous-distributions-skipped",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "3.6 Expectation and Variance of Continuous Distributions (skipped)",
    "text": "3.6 Expectation and Variance of Continuous Distributions (skipped)\nSince the random variable is no longer countable in discrete increments, integration over the support has to replace summation.\n\n3.6.1 Probability Calculation\nThe probability of a random variable in an interval \\(X \\in [a, b]\\) with \\(a \\leq b\\) is evaluated with the distribution functions \\(F(a) = \\int_{-\\infty}^{a} f(x) \\cdot dx\\) and \\(F(b) = \\int_{-\\infty}^{b} f(x) \\cdot dx\\) as:\n\\[F(b) - F(a) = \\int_a^b f(x) \\cdot dx\\]\n\n\n3.6.2 Derivative Relationship\nNote the first derivative of the distribution function gives the density function:\n\\[\\lim_{\\Delta x \\to 0} \\frac{F(x) - F(x + \\Delta x)}{\\Delta x} = f(x)\\]\n\n\n3.6.3 Expectation and Variance\nThe expectation and variance require integration over the whole support of \\(X\\):\n\nExpectation: \\[E(X) = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\cdot dx\\]\nVariance: \\[Var(X) = \\int_{-\\infty}^{\\infty} \\left(x - E(X)\\right)^2 \\cdot f(x) \\cdot dx\\] \\[= \\int_{-\\infty}^{\\infty} x^2 \\cdot f(x) \\cdot dx - \\left[E(X)\\right]^2\\]\n\n\n\n3.6.4 Analytical vs Numerical Integration\nFor some density functions their distribution function, expectation, and variance are known analytically. For others, however, they need to be approximated by numerical integration methods.\n\n\n3.6.5 Example: Exponential Distribution\n\n\n\nExponential distribution density at λ = 1 with Riemann sum approximation\n\n\nExponential distribution \\(f(x \\mid \\lambda = 1)\\) with:\n\\[F(2) = \\int_0^2 f(x) \\cdot dx = 0.8646647\\]\nThe numerical approximated value using a Riemann sum with 10 bars is \\(F(2) \\approx 0.8643045\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#other-distribution-function-names-in-r",
    "href": "w11_Chapter05B.html#other-distribution-function-names-in-r",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "4.1 Other Distribution Function Names in R",
    "text": "4.1 Other Distribution Function Names in R\n\n[dpqr]pois for the Poisson distribution\n[dpqr]binom for the binomial distribution\n[dpqr]geom for the geometric distribution\n[dpqr]nbinom for the negative binomial distribution\n[dpqr]exp for the exponential distribution\n[dpqr]beta for the beta distribution\n[dpqr]gamma for the gamma distribution\n[dpqr]norm for the normal distribution\n[dpqr]t for the \\(t\\)-distribution\n[dpqr]f for the \\(F\\)-distribution\n[dpqr]chisq for the \\(\\chi^2\\)-distribution",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#joint-probabilities-of-nominal-and-ordinal-scaled-pairs-of-variables-skipped",
    "href": "w11_Chapter05B.html#joint-probabilities-of-nominal-and-ordinal-scaled-pairs-of-variables-skipped",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "6.1 Joint Probabilities of Nominal and Ordinal Scaled Pairs of Variables (skipped)",
    "text": "6.1 Joint Probabilities of Nominal and Ordinal Scaled Pairs of Variables (skipped)\nNominal and ordinal scaled pairs of variables can be jointly displayed in contingency tables.\nAssuming that the 100 households are the population, then the relative frequencies become the joint probabilities \\(\\Pr(C = i \\cap P = j)\\) and the marginal probabilities become:\n\\[\\Pr(P = j) = \\sum_{i=0}^{3} \\Pr(C = i \\cap P = j)\\]\n\n6.1.1 Frequency Table \\(n_{ij}\\)\n\n\n\n\nHHSize\n\n\n\n\n\n\n\n\ncars\np:2\np:3\np:4\np:5\nSum\n\n\nc:0\n10\n7\n4\n1\n22\n\n\nc:1\n8\n10\n5\n2\n25\n\n\nc:2\n3\n6\n12\n6\n27\n\n\nc:3\n2\n3\n6\n15\n26\n\n\nSum\n23\n26\n27\n24\n100\n\n\n\n\n\n6.1.2 Relative Frequency Table \\(f_{ij} = n_{ij}/n_{++}\\)\n\n\n\n\nHHSize\n\n\n\n\n\n\n\n\ncars\np:2\np:3\np:4\np:5\nSum\n\n\nc:0\n0.10\n0.07\n0.04\n0.01\n0.22\n\n\nc:1\n0.08\n0.10\n0.05\n0.02\n0.25\n\n\nc:2\n0.03\n0.06\n0.12\n0.06\n0.27\n\n\nc:3\n0.02\n0.03\n0.06\n0.15\n0.26\n\n\nSum\n0.23\n0.26\n0.27\n0.24\n1.00\n\n\n\n\n\n\n3D bar plot showing frequencies table of cars by household size\n\n\n\n\n6.1.3 Conditional Probabilities\nThe column percentages are \\(f_{i|j} = n_{ij}/n_{+j}\\). These can be viewed as conditional probabilities:\n\\[\\Pr(C = i \\mid P = j) = \\frac{\\Pr(C = i \\cap P = j)}{\\Pr(P = j)}\\]\nwith the marginal probability.\n&gt; addmargins(prop.table(hh.car.tab, 2), 1)\n       HHSize\ncars         p:2        p:3        p:4        p:5\n  c:0 0.43478261 0.26923077 0.14814815 0.04166667\n  c:1 0.34782609 0.38461538 0.18518519 0.08333333\n  c:2 0.13043478 0.23076923 0.44444444 0.25000000\n  c:3 0.08695652 0.11538462 0.22222222 0.62500000\n  Sum 1.00000000 1.00000000 1.00000000 1.00000000\n\n\n\nConditional Relative Frequency of Cars given Household Size\n\n\nAn analog procedure can be applied to calculate row percentages and plot the household size percentage given the number of cars. However, this does not make sense in this example because one rarely can assume that the number of cars influences the household size.\n\n\n6.1.4 General Terms\n\nFor population probabilities the row and column indices become random variables \\(I\\) and \\(J\\).\nThe joint probability of a particular cell \\(i, j\\) simply is \\(\\Pr(I = i \\cap J = j)\\).\nRelationship to the Multinomial Distribution: In many cases one can assume that the cell counts \\(n_{ij}\\) in a contingency table follow a multinomial distribution with theoretical population cell probabilities \\(\\pi_{ij} = \\Pr(I = i \\cap J = j)\\) and the total number of experiments being \\(n = \\sum_{i=1}^{I} \\sum_{j=1}^{J} n_{ij}\\).\nA marginal column probability is \\(\\Pr(J = j) = \\sum_i \\Pr(I = i \\cap J = j)\\).\nBecause the elementary outcomes \\((I = i \\cap J = j)\\) are assumed to be independent we are allowed to sum these. Analogue for the marginal row probabilities \\(\\Pr(I = i) = \\sum_j \\Pr(I = i \\cap J = j)\\).\nThe total probability sums to one according to the Kolmogorov’s axioms: \\(\\sum_j \\sum_i \\Pr(I = i, J = j) = 1\\).\nThe conditional probability is \\(\\Pr(I = i \\mid J = j) = \\frac{\\Pr(I = i, J = j)}{\\Pr(J = j)}\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#the-covariance-between-pairs-of-variables-skipped",
    "href": "w11_Chapter05B.html#the-covariance-between-pairs-of-variables-skipped",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "6.2 The Covariance between Pairs of Variables (skipped)",
    "text": "6.2 The Covariance between Pairs of Variables (skipped)\nThe covariance between metric pairs of variables is defined for discrete and continuous random variables.\n\n6.2.1 Discrete Case\n\\[Cov(I, J) = \\sum_i \\sum_j \\left(i - E(I)\\right) \\cdot \\left(j - E(J)\\right) \\cdot \\Pr(I = i \\cap J = j)\\]\n\n\n6.2.2 Continuous Case\n\\[Cov(X, Y) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x, y) \\cdot \\left(x - E(X)\\right) \\cdot \\left(y - E(Y)\\right) \\cdot dx \\cdot dy\\]\nwhere \\(f(x, y)\\) is the joint density of the random variables \\(X = x\\) and \\(Y = y\\).\n\n\n6.2.3 Relationship to Correlation Coefficient\nThe covariance equations are closely related to the numerator of the correlation coefficient:\n\nHowever, the sample correlation coefficient assumes an equal weight of \\(\\Pr(I = i, J = j) \\Leftrightarrow 1/n\\).\nWhereas the population covariance has a probability weight of \\(\\Pr(I = i \\cap J = j)\\) for a discrete distribution or \\(f(x, y)\\) for a continuous distribution.\n\n\n\n6.2.4 Statistical Independence\nA covariance of zero implies that a pair random variables is linearly stochastically independent:\n\\[\\Pr(I, J) = \\Pr(I) \\cdot \\Pr(J) \\text{ or } f(X, Y) = f(X) \\cdot f(Y)\\]\n\n\n6.2.5 Joint Independence\nPairwise independence, however, does not imply that three or more random variables are jointly independent.\nDefinition of jointly independence for three random variables:\n\\[\\Pr(H, I, J) = \\Pr(H) \\cdot \\Pr(I) \\cdot \\Pr(J) \\text{ or } f(X, Y, Z) = f(X) \\cdot f(Y) \\cdot f(Z)\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w11_Chapter05B.html#continuous-distributions-the-bivariate-normal-distribution-skipped",
    "href": "w11_Chapter05B.html#continuous-distributions-the-bivariate-normal-distribution-skipped",
    "title": "Chapter 05 (Part B): Probabilities and Random Variables",
    "section": "6.3 Continuous Distributions: The Bivariate Normal Distribution (skipped)",
    "text": "6.3 Continuous Distributions: The Bivariate Normal Distribution (skipped)\nThe bivariate normal distribution between two continuous random variables \\(X_1 \\in [-\\infty, \\infty]\\) and \\(X_2 \\in [-\\infty, \\infty]\\) is characterized by five parameters:\n\n\\(\\mu_{X_1}\\) and \\(\\mu_{X_2}\\) for the central tendency on each axis \\(X_1\\) and \\(X_2\\).\n\\(\\sigma_{X_1}\\) and \\(\\sigma_{X_2}\\) for the spread along each axis \\(X_1\\) and \\(X_2\\).\n\\(\\sigma_{X_1, X_2}\\) for the covariance between both \\(X_1\\) and \\(X_2\\).\n\n\n6.3.1 3-D Examples of Bivariate Normal Distribution\n  \n\n\n6.3.2 Conditional Density\nThe conditional density is given by:\n\\[f(x_1 \\mid x_2) = \\frac{f(x_1, x_2)}{f(x_2)}\\]\nis shown by the \\(x_1\\) and \\(x_2\\) gridlines.\n\n\n6.3.3 Conditional Distribution\nThe conditional distribution is again normal distributed with the expectation:\n\\[\\mu_{X_1 | X_2 = x_2} = \\mu_{X_1} + \\frac{\\sigma_{X_1, X_2}}{\\sigma_{X_2}^2} \\cdot \\left(x_2 - \\mu_{X_2}\\right)\\]\nand the variance:\n\\[\\sigma_{X_1 | X_2 = x_2}^2 = \\left(1 - \\rho_{X_1, X_2}^2\\right) \\cdot \\sigma_{X_1}^2\\]\n\n\n6.3.4 Contour Plot with Marginal Densities\n\n\n\nBi-variate Normal Distribution: Contour plot with equal density isolines and marginal densities\n\n\n\n\n6.3.5 Marginal Distributions\nThe marginal distributions are again normal distributed with the parameters \\(\\mu_{X_1}\\) and \\(\\sigma_{X_1}^2\\):\ne.g. \\(N(\\mu_{X_1}, \\sigma_{X_1}^2) = f(X_1 = x_1) = \\int_{-\\infty}^{\\infty} f(x_1, x_2) \\cdot dx_2\\)\nThese marginal distributions are schematically shown by the blue curves at the margins.\n\n\n6.3.6 Total Probability\nThe total probability under the density is:\n\\[\\int_{-\\infty}^{\\infty} \\left(\\underbrace{\\int_{-\\infty}^{\\infty} f(x_1, x_2) \\cdot dx_2}_{= f(x_1)}\\right) \\cdot dx_1 = 1\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk11-ch5B.Probabilities and Random Variables"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html",
    "href": "w13_Chapter07Estimation.html",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "",
    "text": "Estimation concepts.\nExample point estimates.\nExample interval estimates.\nBasic sample size calculations.\nExcursion: Confidence Intervals for regression coefficients",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#biased-versus-unbiased-estimator",
    "href": "w13_Chapter07Estimation.html#biased-versus-unbiased-estimator",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "5.1 Biased versus Unbiased Estimator",
    "text": "5.1 Biased versus Unbiased Estimator\n\n\n\nFIGURE 8-3. Sampling distributions for a biased and an unbiased estimator of θ.\n\n\n\nA biased estimator may become asymptotically unbiased as the sample size \\(n\\) increases. That is, the bias is consistently shrinking.\nAn example is the biased variance estimator \\(\\frac{1}{n} \\cdot \\sum_{i=1}^{n}(x_i - \\bar{x})^2\\), because for large \\(n\\) we get \\(\\lim_{n \\to \\infty} n \\approx n - 1\\).\nIn how far an estimator varies around the true population value from sample to sample is measured by the expected squared differences over all possible samples \\(E\\left[(\\hat{\\theta} - \\theta)^2\\right]\\), that is, \\(Var(\\hat{\\theta})\\).\nUltimately, we want to have an unbiased estimation rule \\(T\\) that, in addition, has the smallest possible variability for sample to sample, i.e., the smallest variance.\nThis estimation rule has the lowest uncertainty (or equivalently the highest precision).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#unbiased-estimators-with-different-variance",
    "href": "w13_Chapter07Estimation.html#unbiased-estimators-with-different-variance",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "5.2 Unbiased Estimators with Different Variance",
    "text": "5.2 Unbiased Estimators with Different Variance\n\n\n\nFIGURE 8-4. Sampling distributions for two unbiased estimators of θ.\n\n\n\nDef efficient estimator: An unbiased estimation rule \\(T\\) is called efficient or best unbiased estimator, if it has the smallest variance compared to any other possible unbiased estimation rules.\nWhich estimation rule \\(T\\) is the most efficient may be dependent on the underlying, yet unknown, population distribution.\nDef consistency: An estimator is called consistent if for an increasing sample size \\(n \\to \\infty\\) its estimated value \\(\\hat{\\theta}_n\\) (\\(n\\) stands for the underlying sample size) approaches the true population value \\(\\theta\\) and its variance is shrinking.\nThat is, it converges in probability to the true population value:\n\n\\[\\lim_{n \\to \\infty} \\Pr(|\\hat{\\theta}_n - \\theta| &lt; \\delta) = 1\\]\nfor any small positive value of \\(\\delta\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#mean-square-error-for-biased-estimators",
    "href": "w13_Chapter07Estimation.html#mean-square-error-for-biased-estimators",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "5.3 Mean Square Error for Biased Estimators",
    "text": "5.3 Mean Square Error for Biased Estimators\n\nFor biased estimation rules with \\(E(\\hat{\\theta}) \\neq \\theta\\) the concept of efficiency does not apply and their mean square error needs to be evaluated:\n\n\\[MSE = E(\\hat{\\theta} - \\theta)^2 = \\underbrace{\\left[E(\\hat{\\theta}) - \\theta\\right]^2}_{Bias^2 \\text{ of } T} + \\underbrace{E\\left[\\hat{\\theta} - E(\\hat{\\theta})\\right]^2}_{Variance \\text{ of } T}\\]\n\nThere may be a tradeoff between a bias and the variance of an estimation rule \\(T\\) in the \\(MSE\\):\n\n\n\n\nFIGURE 8-5. Difficulties in choosing a potential estimator.\n\n\nUltimately, one may prefer an estimation rule with a small bias but with a substantially smaller MSE over an alternative unbiased estimation rule with a high variance. \\(\\Rightarrow\\) Such an estimation rule will on average be closer to the true population parameter than the unbiased rule.\n\nTo express the variability in original units, one can use the root mean square error:\n\n\\[RMSE = \\sqrt{MSE} = \\sqrt{Bias^2 + Variance}\\]\nNote that we cannot decompose this expression into independent summands\n[a] of the bias and\n[b] of the square-root of the variance,\nbecause both terms are jointly under the square root.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#a-properties-of-the-mean-estimation-rule-barx",
    "href": "w13_Chapter07Estimation.html#a-properties-of-the-mean-estimation-rule-barx",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "7.1 [a] Properties of the Mean Estimation Rule \\(\\bar{X}\\)",
    "text": "7.1 [a] Properties of the Mean Estimation Rule \\(\\bar{X}\\)\n\n\n\nTABLE 8-1. Point Estimators of μ, π, and σ²\n\n\n\nFor an underlying normal population distribution the arithmetic mean is unbiased and its variance is only 56% of that of the median’s variance. It is therefore most efficient.\nSince the mean minimizes the sum of the squared deviations from the central value, it is highly sensitive to extreme value (leverages).\nFor symmetric distributions with heavy tails the mean may become less efficient than the median.\nFor highly asymmetric distributions the mean is even less efficient.\nTrimmed means, which disregard the most extreme observations in both tails, are more robust because are not affected by these high leverage values. Their properties lie in-between those of the mean and median.\nThe advantage of working with the mean is that we know from the central limit theorem its underlying sampling distribution (the normal distribution) as the sample size increases.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#b-population-proportion",
    "href": "w13_Chapter07Estimation.html#b-population-proportion",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "7.2 [b] Population Proportion",
    "text": "7.2 [b] Population Proportion\n\nAssuming the random variables is coded as \\(X_i = \\begin{cases} 1 & \\text{for success} \\\\ 0 & \\text{for failure} \\end{cases}\\).\nThen the population proportion estimation rule is structurally equivalent to the arithmetic mean:\n\n\\[\\hat{\\pi} = \\frac{\\sum_{i=1}^{n} X_i}{n} = \\frac{\\# \\text{ of successes}}{\\# \\text{ of trials}}\\]\n\nTherefore, the properties of the mean apply to the proportion estimator \\(\\hat{\\pi}\\) for large sample sizes.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#c-population-variance",
    "href": "w13_Chapter07Estimation.html#c-population-variance",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "7.3 [c] Population Variance",
    "text": "7.3 [c] Population Variance\n\nThe estimator for the sample variance is \\(S^2 = \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n}(X_i - \\bar{X})^2\\).\nIt is an unbiased estimator (over all potential samples of size \\(n\\) the estimator \\(S^2\\) will average to the population variance \\(\\sigma^2\\)).\nFor a normal distributed underlying population \\(S^2\\) is the most efficient estimator for \\(\\sigma^2\\).\nDivision by \\(n\\) would lead to a biased estimator, which systematically would underestimate the variance. However, for increasing sample sizes it is a consistent estimator.\nReview of the reasons:\n\nUse of \\(\\bar{X}\\) minimizes the sum of the squared deviations.\nWe lose one degree for freedom because, once the mean is known, only \\(n - 1\\) observations need to be available (recall zero sum property \\(\\sum_{i=1}^{n} x_i - n \\cdot \\bar{x} = 0\\)).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#review-standard-normal-distribution",
    "href": "w13_Chapter07Estimation.html#review-standard-normal-distribution",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "8.1 Review: Standard Normal Distribution",
    "text": "8.1 Review: Standard Normal Distribution\n\n\n\nFIGURE 8-7. Sampling distribution of X̄.\n\n\n\nFor a standard normal distributed variable \\((1-\\alpha) \\times 100\\%\\) of the observations are within the interval \\(\\left[z_{\\alpha/2}, z_{1-\\alpha/2}\\right]\\). That is,\n\n\\[\\Pr\\left(z \\in \\left[z_{\\alpha/2}, z_{1-\\alpha/2}\\right]\\right) = 1 - \\alpha\\]\nThe quantiles:\n\n\\(z_{\\alpha/2}\\) is in the left tail of the standard normal distribution and therefore is negative. It has a small cumulative probability.\n\\(z_{1-\\alpha/2}\\) is in the right tail of the standard normal distribution and therefore is positive. It has a large cumulative probability.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#derivation-of-confidence-interval",
    "href": "w13_Chapter07Estimation.html#derivation-of-confidence-interval",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "8.2 Derivation of Confidence Interval",
    "text": "8.2 Derivation of Confidence Interval\n\nThe estimator for the arithmetic mean is distributed as \\(\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\) (recall the central limit theorem).\nTherefore, the confidence interval around the population expectation \\(\\mu\\) is for a given mean estimation rule \\(\\bar{x} = T(X_1 = x_1, \\ldots, X_n = x_n)\\):\n\n\\[\\Pr\\left(z_{\\alpha/2} \\leq \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\leq z_{1-\\alpha/2}\\right) = 1 - \\alpha\\]\n\\[\\Pr\\left(\\sigma/\\sqrt{n} \\cdot z_{\\alpha/2} \\leq \\bar{X} - \\mu \\leq \\sigma/\\sqrt{n} \\cdot z_{1-\\alpha/2}\\right) = 1 - \\alpha\\]\n\\[\\Pr\\left(\\underbrace{-\\bar{X} + \\sigma/\\sqrt{n} \\cdot z_{\\alpha/2}}_{negative} \\leq -\\mu \\leq \\underbrace{-\\bar{X} + \\sigma/\\sqrt{n} \\cdot z_{1-\\alpha/2}}_{positive}\\right) = 1 - \\alpha \\quad \\text{Multiplying by -1 re-orients the inequality}\\]\n\\[\\Pr\\left(\\bar{X} - \\sigma/\\sqrt{n} \\cdot z_{\\alpha/2} \\geq \\mu \\geq \\bar{X} - \\sigma/\\sqrt{n} \\cdot z_{1-\\alpha/2}\\right) = 1 - \\alpha \\quad \\text{making use of } z_{\\alpha/2} = -z_{1-\\alpha/2} \\text{ and } -z_{\\alpha/2} = z_{1-\\alpha/2}\\]\n\\[\\Pr\\left(\\bar{X} + \\sigma/\\sqrt{n} \\cdot z_{1-\\alpha/2} \\geq \\mu \\geq \\bar{X} + \\sigma/\\sqrt{n} \\cdot z_{\\alpha/2}\\right) = 1 - \\alpha \\quad \\text{and rearranging the order}\\]\n\\[\\Pr\\left(\\bar{X} + \\sigma/\\sqrt{n} \\cdot z_{\\alpha/2} \\leq \\mu \\leq \\bar{X} + \\sigma/\\sqrt{n} \\cdot z_{1-\\alpha/2}\\right) = 1 - \\alpha\\]\n\nSince \\(z_{0.025} = -1.96\\) and \\(z_{0.975} = 1.96\\) at \\(\\alpha = 0.05\\) the confidence interval can be calculated by \\(\\bar{x} \\pm 1.96 \\cdot \\sigma_{\\bar{x}}\\):",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#interpretation-of-confidence-interval",
    "href": "w13_Chapter07Estimation.html#interpretation-of-confidence-interval",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "8.3 Interpretation of Confidence Interval",
    "text": "8.3 Interpretation of Confidence Interval\n\n\n\nFIGURE 8-8. Interval estimates constructed from repeated samples of size n.\n\n\n\nThe probability \\(1 - \\alpha\\) can be interpreted as:\n\\((1 - \\alpha) \\times 100\\%\\) of the possible samples lead to confidence intervals that will cover the true but unknown population expectation \\(\\mu\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#general-rules",
    "href": "w13_Chapter07Estimation.html#general-rules",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "8.4 General Rules",
    "text": "8.4 General Rules\n\n\n\nFIGURE 8-10. Effect of confidence level on interval width.\n\n\n\nThe smaller the error probability \\(\\alpha\\) is (i.e., the confidence level \\(1 - \\alpha\\) increases) the wider the confidence interval becomes and vice versa. See example to the right.\n\n\n\n\nFIGURE 8-11. Effect of sample size on confidence interval width.\n\n\n\nFurthermore, beside the error probability the width of the confidence interval also depends on the sample size through the standard error \\(\\sigma/\\sqrt{n}\\) of the mean.\nAs \\(n\\) increases, the confidence interval will shrink.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#assuming-a-normal-distribution",
    "href": "w13_Chapter07Estimation.html#assuming-a-normal-distribution",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "13.1 Assuming a Normal Distribution",
    "text": "13.1 Assuming a Normal Distribution\n\nFor the population expectation from a normal distribution and a confidence level \\(1 - \\alpha\\) we get:\nThe confidence interval bounds are:\n\\[\\bar{x} \\pm z_{\\alpha/2} \\cdot \\sigma/\\sqrt{n}\\]\ntherefore the error \\(E\\) becomes:\n\\[E = |z_{\\alpha/2}| \\cdot \\sigma/\\sqrt{n}\\]\nSolving this expression for sample size leads to:\n\\[n = \\left(\\frac{|z_{\\alpha/2}| \\cdot \\sigma}{E}\\right)^2\\]\nTo explore the effect of the error on the required sample size let us assume that error becomes twice as large: \\(E^* = 2 \\cdot |z_{\\alpha/2}| \\cdot \\sigma/\\sqrt{n}\\) we get as required sample size:\n\\[n = \\left(\\frac{|z_{\\alpha/2}| \\cdot \\sigma}{2 \\cdot E}\\right)^2 = \\frac{1}{4} \\cdot \\left(\\frac{|z_{\\alpha/2}| \\cdot \\sigma}{E}\\right)^2\\]\nThus just a quarter of sample data is needed if we are willing to make the error twice as large.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#general-rules-1",
    "href": "w13_Chapter07Estimation.html#general-rules-1",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "13.2 General Rules",
    "text": "13.2 General Rules\n\nAs the error \\(E\\) decreases the sample size \\(n\\) increases.\nAs the error probability \\(\\alpha\\) decreases, the critical tail value \\(|z_{\\alpha/2}|\\) becomes larger and, therefore, the required sample size \\(n\\) increases.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#assuming-a-binomial-distribution",
    "href": "w13_Chapter07Estimation.html#assuming-a-binomial-distribution",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "13.3 Assuming a Binomial Distribution",
    "text": "13.3 Assuming a Binomial Distribution\n\n\n\nStandard deviation of binary distribution\n\n\n\nFor the proportion estimator \\(\\hat{\\pi}\\) of a binomial distribution we get:\n\\[n = \\left[\\frac{|z_{\\alpha/2}| \\cdot \\sqrt{\\hat{\\pi} \\cdot (1 - \\hat{\\pi})}}{E}\\right]^2\\]\nSince \\(\\hat{\\pi}\\) is unknown before the sample is drawn, it must be determined exogenously (e.g., through experience).\nAlternatively, the worst-case scenario of \\(\\pi = 0.5\\) can be used, for which the variance \\(\\pi \\cdot (1 - \\pi)\\) becomes the largest. This provides a conservative upper bound for \\(n\\).\nNote, the error \\(E\\) must be substantially smaller than 0.5 for the interval covering the support \\(\\pi \\in ]0, 1[\\).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#lay-persons-interpretation-no-statistical-rigor",
    "href": "w13_Chapter07Estimation.html#lay-persons-interpretation-no-statistical-rigor",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "14.1 Lay-person’s Interpretation (no statistical rigor)",
    "text": "14.1 Lay-person’s Interpretation (no statistical rigor)\n\nRecall: If the true population parameter is \\(\\beta_j = 0\\) then the independent variable \\(X_j\\) does not influence the variation in the dependent variable.\nTherefore, if the value 0 is within the estimated confidence interval \\(0 \\in [\\beta_j^{lower, \\alpha}, \\beta_j^{upper, \\alpha}]\\), then this implies that that the true population parameter \\(\\beta_j\\) is not different from zero, i.e., \\(1 - \\alpha \\times 100\\%\\) of the possible intervals will cover the value zero.\nConsequently, the associated independent variable \\(X_j\\) has no influence on the variability of \\(Y\\).\nExplore the R script StateSchoolConfint.r.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#working-with-normal-distribution-quantiles",
    "href": "w13_Chapter07Estimation.html#working-with-normal-distribution-quantiles",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "15.1 Working with Normal Distribution Quantiles",
    "text": "15.1 Working with Normal Distribution Quantiles\n\n# z-values for common confidence levels\nalpha &lt;- 0.05\nz_lower &lt;- qnorm(alpha/2)\nz_upper &lt;- qnorm(1 - alpha/2)\ncat(\"For alpha =\", alpha, \"\\n\")\n\nFor alpha = 0.05 \n\ncat(\"z_alpha/2 =\", z_lower, \"\\n\")\n\nz_alpha/2 = -1.959964 \n\ncat(\"z_1-alpha/2 =\", z_upper, \"\\n\")\n\nz_1-alpha/2 = 1.959964",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#confidence-interval-for-mean-known-variance",
    "href": "w13_Chapter07Estimation.html#confidence-interval-for-mean-known-variance",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "15.2 Confidence Interval for Mean (Known Variance)",
    "text": "15.2 Confidence Interval for Mean (Known Variance)\n\n# Example: Confidence interval for population mean\n# when population standard deviation is known\nset.seed(123)\nsample_data &lt;- rnorm(50, mean = 100, sd = 15)\nx_bar &lt;- mean(sample_data)\nsigma &lt;- 15  # Known population SD\nn &lt;- length(sample_data)\nalpha &lt;- 0.05\n\n# Calculate confidence interval\nz_crit &lt;- qnorm(1 - alpha/2)\nmargin_error &lt;- z_crit * sigma / sqrt(n)\nci_lower &lt;- x_bar - margin_error\nci_upper &lt;- x_bar + margin_error\n\ncat(\"Sample mean:\", round(x_bar, 2), \"\\n\")\n\nSample mean: 100.52 \n\ncat(\"95% Confidence Interval: [\", round(ci_lower, 2), \",\", round(ci_upper, 2), \"]\\n\")\n\n95% Confidence Interval: [ 96.36 , 104.67 ]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#confidence-interval-for-mean-unknown-variance",
    "href": "w13_Chapter07Estimation.html#confidence-interval-for-mean-unknown-variance",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "15.3 Confidence Interval for Mean (Unknown Variance)",
    "text": "15.3 Confidence Interval for Mean (Unknown Variance)\n\n# When population standard deviation is unknown, use t-distribution\ns &lt;- sd(sample_data)\nt_crit &lt;- qt(1 - alpha/2, df = n - 1)\nmargin_error_t &lt;- t_crit * s / sqrt(n)\nci_lower_t &lt;- x_bar - margin_error_t\nci_upper_t &lt;- x_bar + margin_error_t\n\ncat(\"Using t-distribution:\\n\")\n\nUsing t-distribution:\n\ncat(\"Sample SD:\", round(s, 2), \"\\n\")\n\nSample SD: 13.89 \n\ncat(\"95% Confidence Interval: [\", round(ci_lower_t, 2), \",\", round(ci_upper_t, 2), \"]\\n\")\n\n95% Confidence Interval: [ 96.57 , 104.46 ]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#sample-size-determination-1",
    "href": "w13_Chapter07Estimation.html#sample-size-determination-1",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "15.4 Sample Size Determination",
    "text": "15.4 Sample Size Determination\n\n# Determine required sample size for estimating population mean\nsigma &lt;- 15  # Assumed population SD\nE &lt;- 2       # Desired margin of error\nalpha &lt;- 0.05\nz_crit &lt;- qnorm(1 - alpha/2)\n\nn_required &lt;- (z_crit * sigma / E)^2\ncat(\"Required sample size:\", ceiling(n_required), \"\\n\")\n\nRequired sample size: 217",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#confidence-interval-for-proportion",
    "href": "w13_Chapter07Estimation.html#confidence-interval-for-proportion",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "15.5 Confidence Interval for Proportion",
    "text": "15.5 Confidence Interval for Proportion\n\n# Example: Estimating population proportion\nsuccesses &lt;- 45\nn &lt;- 100\np_hat &lt;- successes / n\n\n# Calculate confidence interval using normal approximation\nse_p &lt;- sqrt(p_hat * (1 - p_hat) / n)\nci_lower_p &lt;- p_hat + qnorm(alpha/2) * se_p\nci_upper_p &lt;- p_hat + qnorm(1 - alpha/2) * se_p\n\ncat(\"Sample proportion:\", p_hat, \"\\n\")\n\nSample proportion: 0.45 \n\ncat(\"95% Confidence Interval: [\", round(ci_lower_p, 3), \",\", round(ci_upper_p, 3), \"]\\n\")\n\n95% Confidence Interval: [ 0.352 , 0.548 ]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w13_Chapter07Estimation.html#visualizing-confidence-intervals",
    "href": "w13_Chapter07Estimation.html#visualizing-confidence-intervals",
    "title": "Chapter 07: Point and Interval Estimation",
    "section": "15.6 Visualizing Confidence Intervals",
    "text": "15.6 Visualizing Confidence Intervals\n\n# Simulate repeated sampling and confidence intervals\nset.seed(42)\ntrue_mu &lt;- 100\nsigma &lt;- 15\nn &lt;- 30\nn_samples &lt;- 20\nalpha &lt;- 0.05\nz_crit &lt;- qnorm(1 - alpha/2)\n\n# Generate samples and calculate CIs\nresults &lt;- data.frame(\n  sample_id = 1:n_samples,\n  mean = numeric(n_samples),\n  lower = numeric(n_samples),\n  upper = numeric(n_samples),\n  covers_mu = logical(n_samples)\n)\n\nfor (i in 1:n_samples) {\n  sample &lt;- rnorm(n, mean = true_mu, sd = sigma)\n  x_bar &lt;- mean(sample)\n  margin &lt;- z_crit * sigma / sqrt(n)\n  results$mean[i] &lt;- x_bar\n  results$lower[i] &lt;- x_bar - margin\n  results$upper[i] &lt;- x_bar + margin\n  results$covers_mu[i] &lt;- (results$lower[i] &lt;= true_mu) & (results$upper[i] &gt;= true_mu)\n}\n\n# Plot\nlibrary(ggplot2)\nggplot(results, aes(x = sample_id, y = mean, color = covers_mu)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.3) +\n  geom_hline(yintercept = true_mu, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  scale_color_manual(values = c(\"FALSE\" = \"red\", \"TRUE\" = \"blue\"),\n                     labels = c(\"Does not cover μ\", \"Covers μ\")) +\n  labs(title = \"95% Confidence Intervals from Repeated Sampling\",\n       subtitle = paste(\"Red dashed line = true μ =\", true_mu),\n       x = \"Sample Number\",\n       y = \"Mean with 95% CI\",\n       color = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nNote: This document was converted from lecture slides for GISC6301 Geo-Spatial Data Analysis Fundamentals (Fall 2025) by Tiefelsdorf.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk13-ch7.Point and Interval Estimation"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html",
    "href": "w15_Chapter09TwoSampleMeanTest.html",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "",
    "text": "Objective: Test whether the population means \\(\\mu_A\\) and \\(\\mu_B\\) of either dependent or independent samples differ by a hypothesized quantity \\(D_0 = \\mu_B - \\mu_A\\).\nFrequently, the difference is assumed zero under the null hypothesis \\(D_0 = 0\\).\nThe null and the alternative hypotheses are:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(D_0 = 0\\)\n\n\\(D_0 \\neq 0\\)\n\n\n\n\n\n\n\\(H_0\\)\n\\(H_1\\)\n\\(H_0\\)\n\\(H_1\\)\n\n\ntwo sided\n\\(H_0: \\mu_A = \\mu_B\\)\n\\(H_1: \\mu_A \\neq \\mu_B\\)\n\\(H_0: \\mu_A - \\mu_B = D_0\\)\n\\(H_1: \\mu_A - \\mu_B \\neq D_0\\)\n\n\none-sided\n\\(H_0: \\mu_A \\leq \\mu_B\\)\n\\(H_1: \\mu_A &gt; \\mu_B\\)\n\\(H_0: \\mu_A - \\mu_B \\leq D_0\\)\n\\(H_1: \\mu_A - \\mu_B &gt; D_0\\)\n\n\n\n\\(H_0: \\mu_A \\geq \\mu_B\\)\n\\(H_1: \\mu_A &lt; \\mu_B\\)\n\\(H_0: \\mu_A - \\mu_B \\geq D_0\\)\n\\(H_1: \\mu_A - \\mu_B &lt; D_0\\)\n\n\n\n\n\n\n\n\n\nStudy design for comparing means: Treatment/Case vs Control groups\n\n\n\nIn the related/matched sample design the means within cases before (time \\(t_A\\)) and after the treatment (time \\(t_B\\)) are compared (green arrows).\nIn the independent sample design, the cases which received treatment are compared (blue arrow) against the controls, which have not received the treatment, at time \\(t_B\\).\nIn either scenario, sampling is performed from the same baseline population which has not been exposed to a treatment.\nThus, the variances of the cases and controls should be identical because both come from the same population.\n\n\n\n\n\n\n\nDecision rules for two-sample tests\n\n\nThe decision tree shows:\n\nStudy Design splits into:\n\nMatched Pairs:\n\nNormal Differences → One-Sample t-test\nNon-normal Differences → Sign-Test for Median or Wilcoxon\n\nTwo Sample:\n\nNon-normal Samples → Mann-Whitney test\nNormal Samples:\n\nEqual Variance → Two-sample t-Test\nUnequal Variance → Adjusted Two-sample t-Test",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#introduction",
    "href": "w15_Chapter09TwoSampleMeanTest.html#introduction",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "",
    "text": "Objective: Test whether the population means \\(\\mu_A\\) and \\(\\mu_B\\) of either dependent or independent samples differ by a hypothesized quantity \\(D_0 = \\mu_B - \\mu_A\\).\nFrequently, the difference is assumed zero under the null hypothesis \\(D_0 = 0\\).\nThe null and the alternative hypotheses are:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(D_0 = 0\\)\n\n\\(D_0 \\neq 0\\)\n\n\n\n\n\n\n\\(H_0\\)\n\\(H_1\\)\n\\(H_0\\)\n\\(H_1\\)\n\n\ntwo sided\n\\(H_0: \\mu_A = \\mu_B\\)\n\\(H_1: \\mu_A \\neq \\mu_B\\)\n\\(H_0: \\mu_A - \\mu_B = D_0\\)\n\\(H_1: \\mu_A - \\mu_B \\neq D_0\\)\n\n\none-sided\n\\(H_0: \\mu_A \\leq \\mu_B\\)\n\\(H_1: \\mu_A &gt; \\mu_B\\)\n\\(H_0: \\mu_A - \\mu_B \\leq D_0\\)\n\\(H_1: \\mu_A - \\mu_B &gt; D_0\\)\n\n\n\n\\(H_0: \\mu_A \\geq \\mu_B\\)\n\\(H_1: \\mu_A &lt; \\mu_B\\)\n\\(H_0: \\mu_A - \\mu_B \\geq D_0\\)\n\\(H_1: \\mu_A - \\mu_B &lt; D_0\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#study-designs-to-compare-means",
    "href": "w15_Chapter09TwoSampleMeanTest.html#study-designs-to-compare-means",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "",
    "text": "Study design for comparing means: Treatment/Case vs Control groups\n\n\n\nIn the related/matched sample design the means within cases before (time \\(t_A\\)) and after the treatment (time \\(t_B\\)) are compared (green arrows).\nIn the independent sample design, the cases which received treatment are compared (blue arrow) against the controls, which have not received the treatment, at time \\(t_B\\).\nIn either scenario, sampling is performed from the same baseline population which has not been exposed to a treatment.\nThus, the variances of the cases and controls should be identical because both come from the same population.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#two-sample-test-decision-rules",
    "href": "w15_Chapter09TwoSampleMeanTest.html#two-sample-test-decision-rules",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "",
    "text": "Decision rules for two-sample tests\n\n\nThe decision tree shows:\n\nStudy Design splits into:\n\nMatched Pairs:\n\nNormal Differences → One-Sample t-test\nNon-normal Differences → Sign-Test for Median or Wilcoxon\n\nTwo Sample:\n\nNon-normal Samples → Mann-Whitney test\nNormal Samples:\n\nEqual Variance → Two-sample t-Test\nUnequal Variance → Adjusted Two-sample t-Test",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#when-to-use-related-samples",
    "href": "w15_Chapter09TwoSampleMeanTest.html#when-to-use-related-samples",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "2.1 When to Use Related Samples",
    "text": "2.1 When to Use Related Samples\nTwo measurements may be related either because:\n\nThey are performed repeatedly at the same object before and after the treatment, or\nCases are matched to untreated members (controls), which are closely related with the cases. For example, treated and untreated siblings sharing similar genetics, or pairs of members from the same neighborhood sharing similar environments, etc.\n\n\nIn either case, one can expect a high degree of correlation between both measurements.\n\n\\(\\Rightarrow\\) This implies for the underlying data structure that each observation (or matched pair) has two measurements. Therefore, the number of treated and untreated measurements are identical.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#the-test-statistic-for-matched-pairs",
    "href": "w15_Chapter09TwoSampleMeanTest.html#the-test-statistic-for-matched-pairs",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "2.2 The Test Statistic for Matched Pairs",
    "text": "2.2 The Test Statistic for Matched Pairs\n\n2.2.1 Gain Score\nThe two measurements at \\(t_A\\) and \\(t_B\\) for the \\(i^{th}\\) observation can be expressed by the gain score \\(D_i\\) as difference:\n\\[D_i = X_{iA} - X_{iB}\\]\n\n\n2.2.2 Mean of Differences\nThe test statistic becomes:\n\\[\\bar{D} = \\frac{\\sum_{i=1}^{n} D_i}{n}\\]\n\n\n2.2.3 Standard Error\nThe standard error of the average gain \\(\\bar{D}\\) is \\(s_D / \\sqrt{n}\\) with the estimator of the variance being:\n\\[s_D^2 = \\frac{\\sum_{i=1}^{n} (D_i - \\bar{D})^2}{n-1}\\]\nWhy is the division of \\(s_D\\) by \\(\\sqrt{n}\\) performed?\n\n\n2.2.4 t-Statistic\nThe test statistic becomes:\n\\[t = \\frac{\\bar{D} - D_0}{s_D / \\sqrt{n}}\\]\nand for \\(D_0 = 0\\) it reduces to:\n\\[t = \\frac{\\bar{D}}{s_D / \\sqrt{n}}\\]\n\nAssuming the sample measurements come from a population that satisfies the null hypothesis, then the test statistic will follow a t-distribution with \\(n - 1\\) degrees of freedom.\nThe test is equivalent to the one-sample test of the mean \\(H_0: \\mu_D = D_0\\).\n\n\n\n2.2.5 Definition: Sampling Distribution of Paired-Observation Mean \\(\\bar{D}\\)\nAssume \\(X_1\\) and \\(X_2\\) are normal with a difference in means \\(\\mu_1 - \\mu_2 = D_0\\). Given a random sample of \\(n\\) paired observations, the following has an approximate \\(t\\)-distribution:\n\\[T = \\frac{\\bar{D} - D_0}{S_d / \\sqrt{n}}\\]\nwith \\(n - 1\\) degrees of freedom.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#advantages-and-disadvantages-of-using-related-samples",
    "href": "w15_Chapter09TwoSampleMeanTest.html#advantages-and-disadvantages-of-using-related-samples",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "2.3 Advantages and Disadvantages of Using Related Samples",
    "text": "2.3 Advantages and Disadvantages of Using Related Samples\n\n2.3.1 Advantages\n\nBase variability \\(c_i\\) among the sample objects becomes irrelevant because it cancels out using the differences at \\(t_A\\) and \\(t_B\\), therefore just leaving the effect size:\nLet \\(X_{iA} = c_i + x_{iA}\\) and \\(X_{iB} = c_i + x_{iB}\\) then the gain becomes:\n\\[D_i = (c_i + x_{iA}) - (c_i + x_{iB}) = x_{iA} - x_{iB}\\]\nExternal factors \\(c_i\\) are controlled because they are constant within each measurement object and therefore cancel out (this is the underlying concept of panel data analysis).\nLess objects/individuals are needed to be recruited into the study sample because we repeat measurements at the same object/individual. That is, we achieve higher power of rejecting \\(H_0\\) when, in fact, it is false with a smaller sample size.\n\n\n\n2.3.2 Disadvantages\n\nThere may be a carry-over effect that arises when the measurement at \\(t_A\\) influences the outcome at \\(t_B\\). E.g., a first test may train a student for the second test.\nPotential loss of observations because a second measurement cannot be performed on specific observations that have dropped out of the study.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#example-everitts-anorexia-data",
    "href": "w15_Chapter09TwoSampleMeanTest.html#example-everitts-anorexia-data",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "2.4 Example: Everitt’s Anorexia Data",
    "text": "2.4 Example: Everitt’s Anorexia Data\n\n\n\nEffect of Intervention on 17 Girls\n\n\nInvestigate the data organization of the data-frames DEPSAMPLE and INDEPSAMPLE\nNote: Both measurements on the same girl are positively correlated (positive slope of the green line).\nNote: There is an apparent weight gain (above red dashed identity line).\nNote: The 4 observations in the lower left corner may be special cases, because these low weight girls (&lt; 83 pounds) lose weight (red dashed line) even after treatment.\n\n2.4.1 Performing the Paired t-test in R\n\nt.test(DepSample$AFTER, DepSample$BEFORE, \n       alternative='two.sided', \n       conf.level=.95, \n       paired=TRUE)\n\nOutput:\nPaired t-test\n\ndata: DepSample$AFTER and DepSample$BEFORE \nt = 4.1849, df = 16, p-value = 0.0007003\nalternative hypothesis: true difference in means is not equal to 0 \n95 percent confidence interval:\n 3.58470 10.94471 \nsample estimates:\nmean of the differences \n             7.264706\nImportant: The order of both variables matters!",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#overview",
    "href": "w15_Chapter09TwoSampleMeanTest.html#overview",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "3.1 Overview",
    "text": "3.1 Overview\n\nIn the two-independent sample design, we distinguish between a case group that has been exposed to a particular treatment and the control group remains unexposed to the treatment. We want to test for the effect of the treatment on the case group compared to the control group.\nThe members of the case and the control groups were randomly sampled from a common population and randomly assigned to either group. Thus, we can assume a common variance.\nBoth sample groups can be of different sizes.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#the-test-statistic-for-independent-samples",
    "href": "w15_Chapter09TwoSampleMeanTest.html#the-test-statistic-for-independent-samples",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "3.2 The Test Statistic for Independent Samples",
    "text": "3.2 The Test Statistic for Independent Samples\n\n3.2.1 The Standard Error Problem\n\nThe key problem of developing the test statistic lies in the evaluation of its standard error.\nWhile both samples, under the null hypothesis, come from the same population and, therefore, should have identical variances:\n\nThe different sample sizes of the cases and controls complicate the estimation of the variance from the sample observations\nThe treatment of the cases compared to the missing treatment of controls may also induce a change in the variance of the case and control group.\n\n\n\n\n3.2.2 Variance of Difference of Means\nTheoretically, for subtraction or addition of independent means, their joint variance is equal to their sums:\n\\[\\sigma_{\\bar{X}_A \\pm \\bar{X}_B}^2 = \\sigma_{\\bar{X}_A}^2 + \\sigma_{\\bar{X}_B}^2 = \\frac{\\sigma_A^2}{n_A} + \\frac{\\sigma_B^2}{n_B}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#pooling-variances",
    "href": "w15_Chapter09TwoSampleMeanTest.html#pooling-variances",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "3.3 Pooling Variances",
    "text": "3.3 Pooling Variances\n\n3.3.1 The Problem with Simple Averaging\nThe expression for the standard error of the difference of means:\n\\[s_{\\bar{X}_A - \\bar{X}_B} = \\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}\\]\nweights each sample variance equally, which is misleading if the sample sizes differ substantially, e.g., \\(n_A \\neq n_B\\).\n\n\n3.3.2 The Pooled Variance Estimator\nA better estimate is the pooled variance estimator which employs differential weights:\n\\[s_{pooled}^2 = \\frac{\\sum_{i=1}^{n_A}(X_{iA} - \\bar{X}_A)^2 + \\sum_{j=1}^{n_B}(X_{jB} - \\bar{X}_B)^2}{n_A + n_B - 2}\\]\nThis can also be written as:\n\\[s_{pooled}^2 = \\frac{(n_A - 1) \\cdot s_A^2 + (n_B - 1) \\cdot s_B^2}{n_A + n_B - 2}\\]\n\n\n3.3.3 The Two-Sample t-Test Statistic\nThe test statistic becomes:\n\\[t = \\frac{(\\bar{X}_A - \\bar{X}_B) - D_0}{\\sqrt{s_{pooled}^2} \\cdot \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}\\]\nwhere \\(\\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}\\) accounts for the shrinkage of the standard error based on the sample size.\n\n\n3.3.4 Definition: Sampling Distribution of \\(\\bar{X}_1 - \\bar{X}_2\\), Equal Population Variances\nAssume \\(X_1\\) and \\(X_2\\) are normal with a difference in means \\(\\mu_1 - \\mu_2 = D_0\\). If the variance \\(\\sigma^2\\) is the same for both populations, then the following has a \\(t\\)-distribution:\n\\[T = \\frac{\\bar{X}_1 - \\bar{X}_2 - D_0}{\\hat{\\sigma}_{\\bar{X}_1 - \\bar{X}_2}} = \\frac{\\bar{X}_1 - \\bar{X}_2 - D_0}{S_p\\sqrt{1/n_1 + 1/n_2}}\\]\nwith degrees of freedom:\n\\[df = n_1 + n_2 - 2\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#when-variances-are-equal",
    "href": "w15_Chapter09TwoSampleMeanTest.html#when-variances-are-equal",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "4.1 When Variances Are Equal",
    "text": "4.1 When Variances Are Equal\n\nWe can usually assume that the treatment just shifts the mean level without affecting the variance.\nTherefore, the test statistic \\(t\\) follows a t-distribution with \\(n_A + n_B - 2\\) degrees of freedom.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#when-variances-are-unequal-fisher-behrens-problem",
    "href": "w15_Chapter09TwoSampleMeanTest.html#when-variances-are-unequal-fisher-behrens-problem",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "4.2 When Variances Are Unequal (Fisher-Behrens Problem)",
    "text": "4.2 When Variances Are Unequal (Fisher-Behrens Problem)\n\nHowever, if this assumption is incorrect then difficulties arise (see the Fisher-Behrens problem).\nSeveral conservative adjustments are proposed in the literature.\nConservative means: the actual error probability of mistakenly rejecting a true null hypothesis is smaller than the nominal level \\(\\alpha\\).\nThis implies that we are on the safe side and if the null hypothesis is rejected, the true error probability is smaller than the nominal error probability \\(\\alpha\\).\n\n\n4.2.1 Adjustments for Degrees of Freedom\nTwo proposed adjustments are used for the degrees of freedom of the \\(t\\)-test:\n\nThe smaller of the two numbers \\((n_A - 1)\\) or \\((n_B - 1)\\), or\nWelch-Satterthwaite approximation:\n\n\\[df \\approx \\frac{(S_A^2/n_A + S_B^2/n_B)^2}{(S_A^2/n_A)^2/(n_A-1) + (S_B^2/n_B)^2/(n_B-1)}\\]\n\n\n4.2.2 Definition: Sampling Distribution of \\(\\bar{X}_1 - \\bar{X}_2\\), Population Variances Unequal\nAssume \\(X_1\\) and \\(X_2\\) are normal with a difference in means \\(\\mu_1 - \\mu_2 = D_0\\) and variances \\(\\sigma_1^2 \\neq \\sigma_2^2\\). Then the following has an approximate \\(t\\)-distribution:\n\\[T = \\frac{\\bar{X}_1 - \\bar{X}_2 - D_0}{\\hat{\\sigma}_{\\bar{X}_1 - \\bar{X}_1}} = \\frac{\\bar{X}_1 - \\bar{X}_2 - D_0}{\\sqrt{S_1^2/n_1 + S_2^2/n_2}}\\]\nwith degrees of freedom given by:\n\\[df = \\frac{(S_1^2/n_1 + S_2^2/n_2)^2}{(S_1^2/n_1)^2/(n_1-1) + (S_2^2/n_2)^2/(n_2-1)}\\]\nAlternatively, the (approximate) degrees of freedom can be found from:\n\\[df = \\min(n_1 - 1, n_2 - 1)\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#impact-of-the-deviation-from-the-underlying-assumptions",
    "href": "w15_Chapter09TwoSampleMeanTest.html#impact-of-the-deviation-from-the-underlying-assumptions",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "4.3 Impact of the Deviation from the Underlying Assumptions",
    "text": "4.3 Impact of the Deviation from the Underlying Assumptions\nSee the script tTestSimulation.r for an investigation on how violations of the standard assumptions [a] normality and [b] equality of variances affect the significance levels of the t-test.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#formulating-the-hypothesis",
    "href": "w15_Chapter09TwoSampleMeanTest.html#formulating-the-hypothesis",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "5.1 Formulating the Hypothesis",
    "text": "5.1 Formulating the Hypothesis\nThe specification of the two-sample difference-of-means test depends on whether \\(\\sigma_1^2 = \\sigma_2^2\\) or \\(\\sigma_1^2 \\neq \\sigma_2^2\\).\nThe two-sided hypothesis is formulated in terms of ratios:\n\\[H_0: \\frac{\\sigma_1^2}{\\sigma_2^2} = 1 \\quad \\text{against} \\quad H_A: \\frac{\\sigma_1^2}{\\sigma_2^2} \\neq 1\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#the-f-distribution",
    "href": "w15_Chapter09TwoSampleMeanTest.html#the-f-distribution",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "5.2 The F-Distribution",
    "text": "5.2 The F-Distribution\n\n5.2.1 Definition: Sampling Distribution of the Ratio of Variances\nAssume \\(X_1\\) and \\(X_2\\) are both normally distributed, with variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\). Given independent random samples of size \\(n_1\\) and \\(n_2\\), then the statistic:\n\\[F = \\frac{S_1^2/\\sigma_1^2}{S_2^2/\\sigma_2^2}\\]\nwill follow an F distribution with \\(n_1 - 1\\) and \\(n_2 - 1\\) degrees of freedom.\n\n\n5.2.2 Test Statistic Under Null Hypothesis\nBecause under the null hypothesis \\(H_0\\) the population variance is identical, the test statistic becomes:\n\\[F = \\frac{S_1^2}{S_2^2}\\]",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#comparison-of-sample-variances-f-test",
    "href": "w15_Chapter09TwoSampleMeanTest.html#comparison-of-sample-variances-f-test",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "6.1 Comparison of Sample Variances (F-test)",
    "text": "6.1 Comparison of Sample Variances (F-test)\n\nvar.test(GAIN ~ Treatment, alternative='two.sided', conf.level=.95, \n         data=IndepSample)\n\nOutput:\nF test to compare two variances\n\ndata: GAIN by Treatment \nF = 0.8027, num df = 16, denom df = 25, p-value = 0.6587\nalternative hypothesis: true ratio of variances is not equal to 1 \n95 percent confidence interval:\n 0.3367083 2.0981634 \nsample estimates:\nratio of variances \n         0.8027132\nConclusion: We cannot reject \\(H_0: \\frac{\\sigma_A^2}{\\sigma_B^2} = 1\\) and therefore tentatively assume equal variances.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#two-sample-t-test-assuming-equal-variances",
    "href": "w15_Chapter09TwoSampleMeanTest.html#two-sample-t-test-assuming-equal-variances",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "6.2 Two-Sample t-Test (Assuming Equal Variances)",
    "text": "6.2 Two-Sample t-Test (Assuming Equal Variances)\nThe t-test assuming identical variances (see option var.equal=TRUE) of a two-sided null hypothesis \\(H_0: \\mu_{case} = \\mu_{control}\\) is accomplished by:\n\nt.test(GAIN ~ Treatment, alternative='two.sided', conf.level=.95, \n       var.equal=TRUE, data=IndepSample)\n\nOutput:\nTwo Sample t-test\n\ndata: GAIN by Treatment \nt = 3.2227, df = 41, p-value = 0.002491\nalternative hypothesis: true difference in means is not equal to 0 \n95 percent confidence interval:\n  2.880164 12.549248 \nsample estimates:\n mean in group Case mean in group Control \n           7.264706            -0.450000\nNote: Use var.equal=FALSE if the F-test indicates heteroscedasticity (i.e., reject the null hypothesis).",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#equivalent-regression-approach",
    "href": "w15_Chapter09TwoSampleMeanTest.html#equivalent-regression-approach",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "6.3 Equivalent Regression Approach",
    "text": "6.3 Equivalent Regression Approach\nAlternatively, we could have achieved the same results with a regression model that uses TREATMENT as a dummy variable:\n\nlm(formula = GAIN ~ Treatment, data = IndepSample)\n\nOutput:\nCall: lm(formula = GAIN ~ Treatment, data = IndepSample)\n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            7.265      1.861   3.903 0.000347 ***\nTreatment[T.Control]  -7.715      2.394  -3.223 0.002491 ** \n\nResidual standard error: 7.675 on 41 degrees of freedom\nMultiple R-squared:  0.2021,    Adjusted R-squared:  0.1827 \nF-statistic: 10.39 on 1 and 41 DF,  p-value: 0.002491\n\n6.3.1 Interpretation\nThis model implies for:\n\nCases: \\(GAIN = 7.265 + \\underbrace{0}_{Treatment=0} \\cdot (-7.715) = 7.265\\)\nControls: \\(GAIN = 7.265 + \\underbrace{1}_{Treatment=1} \\cdot (-7.715) = -0.45\\)",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#paired-t-test",
    "href": "w15_Chapter09TwoSampleMeanTest.html#paired-t-test",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "7.1 Paired t-Test",
    "text": "7.1 Paired t-Test\n\n# Simulated paired data (before/after treatment)\nset.seed(123)\nn &lt;- 20\nbefore &lt;- rnorm(n, mean = 80, sd = 10)\neffect &lt;- rnorm(n, mean = 5, sd = 3)  # Treatment effect\nafter &lt;- before + effect\n\n# Calculate gain scores\nD &lt;- after - before\nD_bar &lt;- mean(D)\ns_D &lt;- sd(D)\n\ncat(\"Mean difference:\", round(D_bar, 3), \"\\n\")\n\nMean difference: 4.846 \n\ncat(\"SD of differences:\", round(s_D, 3), \"\\n\")\n\nSD of differences: 2.49 \n\n# Paired t-test\nt_stat &lt;- D_bar / (s_D / sqrt(n))\np_value &lt;- 2 * pt(-abs(t_stat), df = n - 1)\n\ncat(\"t-statistic:\", round(t_stat, 3), \"\\n\")\n\nt-statistic: 8.705 \n\ncat(\"p-value:\", round(p_value, 4), \"\\n\")\n\np-value: 0 \n\n# Using t.test function\nt.test(after, before, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  after and before\nt = 8.7047, df = 19, p-value = 4.679e-08\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 3.680959 6.011498\nsample estimates:\nmean difference \n       4.846229",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#independent-two-sample-t-test",
    "href": "w15_Chapter09TwoSampleMeanTest.html#independent-two-sample-t-test",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "7.2 Independent Two-Sample t-Test",
    "text": "7.2 Independent Two-Sample t-Test\n\n# Simulated independent samples\nset.seed(456)\nn_A &lt;- 25\nn_B &lt;- 30\n\n# Case group (received treatment)\ngroup_A &lt;- rnorm(n_A, mean = 75, sd = 8)\n# Control group\ngroup_B &lt;- rnorm(n_B, mean = 70, sd = 8)\n\n# Calculate sample statistics\nx_bar_A &lt;- mean(group_A)\nx_bar_B &lt;- mean(group_B)\ns_A &lt;- sd(group_A)\ns_B &lt;- sd(group_B)\n\ncat(\"Group A: mean =\", round(x_bar_A, 2), \", sd =\", round(s_A, 2), \"\\n\")\n\nGroup A: mean = 76.99 , sd = 9.5 \n\ncat(\"Group B: mean =\", round(x_bar_B, 2), \", sd =\", round(s_B, 2), \"\\n\")\n\nGroup B: mean = 70.65 , sd = 7.02 \n\n# Pooled variance\ns_pooled_sq &lt;- ((n_A - 1) * s_A^2 + (n_B - 1) * s_B^2) / (n_A + n_B - 2)\ns_pooled &lt;- sqrt(s_pooled_sq)\ncat(\"Pooled SD:\", round(s_pooled, 3), \"\\n\")\n\nPooled SD: 8.238 \n\n# t-statistic\nse &lt;- s_pooled * sqrt(1/n_A + 1/n_B)\nt_stat &lt;- (x_bar_A - x_bar_B) / se\ndf &lt;- n_A + n_B - 2\np_value &lt;- 2 * pt(-abs(t_stat), df = df)\n\ncat(\"t-statistic:\", round(t_stat, 3), \"\\n\")\n\nt-statistic: 2.843 \n\ncat(\"Degrees of freedom:\", df, \"\\n\")\n\nDegrees of freedom: 53 \n\ncat(\"p-value:\", round(p_value, 4), \"\\n\")\n\np-value: 0.0063 \n\n# Using t.test function (equal variances)\nt.test(group_A, group_B, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  group_A and group_B\nt = 2.8426, df = 53, p-value = 0.00634\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  1.866869 10.815440\nsample estimates:\nmean of x mean of y \n 76.98851  70.64735",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#f-test-for-equality-of-variances",
    "href": "w15_Chapter09TwoSampleMeanTest.html#f-test-for-equality-of-variances",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "7.3 F-Test for Equality of Variances",
    "text": "7.3 F-Test for Equality of Variances\n\n# Test for equality of variances\nvar.test(group_A, group_B)\n\n\n    F test to compare two variances\n\ndata:  group_A and group_B\nF = 1.8293, num df = 24, denom df = 29, p-value = 0.1216\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.8492353 4.0562745\nsample estimates:\nratio of variances \n          1.829258 \n\n# Manual calculation\nF_stat &lt;- var(group_A) / var(group_B)\ndf1 &lt;- n_A - 1\ndf2 &lt;- n_B - 1\np_value_F &lt;- 2 * min(pf(F_stat, df1, df2), 1 - pf(F_stat, df1, df2))\n\ncat(\"\\nManual F-test:\\n\")\n\n\nManual F-test:\n\ncat(\"F-statistic:\", round(F_stat, 3), \"\\n\")\n\nF-statistic: 1.829 \n\ncat(\"p-value:\", round(p_value_F, 4), \"\\n\")\n\np-value: 0.1216",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#welchs-t-test-unequal-variances",
    "href": "w15_Chapter09TwoSampleMeanTest.html#welchs-t-test-unequal-variances",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "7.4 Welch’s t-Test (Unequal Variances)",
    "text": "7.4 Welch’s t-Test (Unequal Variances)\n\n# When variances are unequal\nset.seed(789)\ngroup_C &lt;- rnorm(20, mean = 50, sd = 5)   # Small variance\ngroup_D &lt;- rnorm(25, mean = 55, sd = 15)  # Large variance\n\n# F-test shows unequal variances\nvar.test(group_C, group_D)\n\n\n    F test to compare two variances\n\ndata:  group_C and group_D\nF = 0.072619, num df = 19, denom df = 24, p-value = 2.725e-07\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.03096559 0.17808527\nsample estimates:\nratio of variances \n        0.07261907 \n\n# Welch's t-test (default in R)\nt.test(group_C, group_D, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  group_C and group_D\nt = -2.2132, df = 28.261, p-value = 0.03513\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -11.9794693  -0.4656552\nsample estimates:\nmean of x mean of y \n 48.44991  54.67247 \n\n# Compare with equal variance assumption (incorrect)\nt.test(group_C, group_D, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  group_C and group_D\nt = -2.0058, df = 43, p-value = 0.0512\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12.47899089   0.03386638\nsample estimates:\nmean of x mean of y \n 48.44991  54.67247",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  },
  {
    "objectID": "w15_Chapter09TwoSampleMeanTest.html#visualization-comparing-groups",
    "href": "w15_Chapter09TwoSampleMeanTest.html#visualization-comparing-groups",
    "title": "Chapter 09: Two-Sample Hypothesis Tests",
    "section": "7.5 Visualization: Comparing Groups",
    "text": "7.5 Visualization: Comparing Groups\n\nlibrary(ggplot2)\n\n# Create data frame for visualization\ndf &lt;- data.frame(\n  value = c(group_A, group_B),\n  group = factor(c(rep(\"Treatment\", n_A), rep(\"Control\", n_B)))\n)\n\n# Side-by-side boxplots\nggplot(df, aes(x = group, y = value, fill = group)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 4, color = \"red\") +\n  labs(title = \"Comparison of Treatment and Control Groups\",\n       subtitle = paste0(\"t = \", round(t_stat, 2), \", p = \", round(p_value, 4)),\n       x = \"Group\",\n       y = \"Response Variable\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\nNote: This document was converted from lecture slides for GISC6301 Geo-spatial Data Fundamentals (Fall 2025) by Tiefelsdorf.",
    "crumbs": [
      "Home",
      "GISC6301",
      "Wk15-ch9.Two-Sample Hypothesis Tests"
    ]
  }
]